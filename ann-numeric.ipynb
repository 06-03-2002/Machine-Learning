{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7298901,"sourceType":"datasetVersion","datasetId":4234079},{"sourceId":7797672,"sourceType":"datasetVersion","datasetId":4565267}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"LSTM\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\ndata = pd.read_csv('AMZN.csv')\n\ndata\ndata = data[['Date', 'Close']]\ndata\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ndevice\n\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\ndata['Date'] = data['Date'].to_numpy()\n\nplt.plot(data['Date'], data['Close'])\nfrom copy import deepcopy as dc\n\ndef prepare_dataframe_for_lstm(df, n_steps):\n    df = dc(df)\n\n    df.set_index('Date', inplace=True)\n\n    for i in range(1, n_steps+1):\n        df[f'Close(t-{i})'] = df['Close'].shift(i)\n\n    df.dropna(inplace=True)\n\n    return df\n\nlookback = 7\nshifted_df = prepare_dataframe_for_lstm(data, lookback)\nshifted_df\nshifted_df_as_np = shifted_df.to_numpy()\n\nshifted_df_as_np\nshifted_df_as_np.shape\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nshifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n\nshifted_df_as_np\nX = shifted_df_as_np[:, 1:]\ny = shifted_df_as_np[:, 0]\n\nX.shape, y.shape\nX = dc(np.flip(X, axis=1))\nX\nsplit_index = int(len(X) * 0.95)\n\nsplit_index\nX_train = X[:split_index]\nX_test = X[split_index:]\n\ny_train = y[:split_index]\ny_test = y[split_index:]\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nX_train = X_train.reshape((-1, lookback, 1))\nX_test = X_test.reshape((-1, lookback, 1))\n\ny_train = y_train.reshape((-1, 1))\ny_test = y_test.reshape((-1, 1))\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nX_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train).float()\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test).float()\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nfrom torch.utils.data import Dataset\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n\ntrain_dataset = TimeSeriesDataset(X_train, y_train)\ntest_dataset = TimeSeriesDataset(X_test, y_test)\ntrain_dataset\nfrom torch.utils.data import DataLoader\n\nbatch_size = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nfor _, batch in enumerate(train_loader):\n    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n    print(x_batch.shape, y_batch.shape)\n    break\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_stacked_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_stacked_layers = num_stacked_layers\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n                            batch_first=True)\n\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\nmodel = LSTM(1, 4, 1)\nmodel.to(device)\nmodel\ndef train_one_epoch():\n    model.train(True)\n    print(f'Epoch: {epoch + 1}')\n    running_loss = 0.0\n\n    for batch_index, batch in enumerate(train_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n\n        output = model(x_batch)\n        loss = loss_function(output, y_batch)\n        running_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_index % 100 == 99:  # print every 100 batches\n            avg_loss_across_batches = running_loss / 100\n            print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n                                                    avg_loss_across_batches))\n            running_loss = 0.0\n    print()\ndef validate_one_epoch():\n    model.train(False)\n    running_loss = 0.0\n\n    for batch_index, batch in enumerate(test_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n\n        with torch.no_grad():\n            output = model(x_batch)\n            loss = loss_function(output, y_batch)\n            running_loss += loss.item()\n\n    avg_loss_across_batches = running_loss / len(test_loader)\n\n    print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n    print('***************************************************')\n    print()\nlearning_rate = 0.001\nnum_epochs = 10\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    train_one_epoch()\n    validate_one_epoch()\nwith torch.no_grad():\n    predicted = model(X_train.to(device)).to('cpu').numpy()\n\nplt.plot(y_train, label='Actual Close')\nplt.plot(predicted, label='Predicted Close')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.legend()\nplt.show()\n\ntrain_predictions = predicted.flatten()\n\ndummies = np.zeros((X_train.shape[0], lookback+1))\ndummies[:, 0] = train_predictions\ndummies = scaler.inverse_transform(dummies)\n\ntrain_predictions = dc(dummies[:, 0])\ntrain_predictions\ndummies = np.zeros((X_train.shape[0], lookback+1))\ndummies[:, 0] = y_train.flatten()\ndummies = scaler.inverse_transform(dummies)\n\nnew_y_train = dc(dummies[:, 0])\nnew_y_train\nplt.plot(new_y_train, label='Actual Close')\nplt.plot(train_predictions, label='Predicted Close')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.legend()\nplt.show()\ntest_predictions = model(X_test.to(device)).detach().cpu().numpy().flatten()\n\ndummies = np.zeros((X_test.shape[0], lookback+1))\ndummies[:, 0] = test_predictions\ndummies = scaler.inverse_transform(dummies)\n\ntest_predictions = dc(dummies[:, 0])\ntest_predictions\ndummies = np.zeros((X_test.shape[0], lookback+1))\ndummies[:, 0] = y_test.flatten()\ndummies = scaler.inverse_transform(dummies)\n\nnew_y_test = dc(dummies[:, 0])\nnew_y_test\nplt.plot(new_y_test, label='Actual Close')\nplt.plot(test_predictions, label='Predicted Close')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.legend()\nplt.show()\n#@title pytorch-r^2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom torch.utils.data import Dataset, DataLoader\nfrom copy import deepcopy as dc\n\n# Load the data\ndata = pd.read_csv('E:/AMZN.csv', encoding='latin1')\n\n# Preprocess the data\ndata = data[['Date', 'Close']]\n\n# Convert non-numeric columns to categorical type and encode them\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\ndata['Date'] = data['Date'].to_numpy()\n\n# Plotting the data\nplt.plot(data['Date'], data['Close'])\nplt.xlabel('Date')\nplt.ylabel('Close')\nplt.show()\n\n# Prepare data for LSTM\ndef prepare_dataframe_for_lstm(df, n_steps):\n    df = dc(df)\n    df.set_index('Date', inplace=True)\n    for i in range(1, n_steps+1):\n        df[f'Close(t-{i})'] = df['Close'].shift(i)\n    df.dropna(inplace=True)\n    return df\n\nlookback = 7\nshifted_df = prepare_dataframe_for_lstm(data, lookback)\n\n# Normalize the data\nshifted_df_as_np = shifted_df.to_numpy()\nscaler = MinMaxScaler(feature_range=(-1, 1))\nshifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n\nX = shifted_df_as_np[:, 1:]\ny = shifted_df_as_np[:, 0]\n\n# Reverse the input data for LSTM\nX = dc(np.flip(X, axis=1))\n\n# Train-test split\nsplit_index = int(len(X) * 0.95)\nX_train = X[:split_index]\nX_test = X[split_index:]\ny_train = y[:split_index]\ny_test = y[split_index:]\n\n# Reshaping data for LSTM\nX_train = X_train.reshape((-1, lookback, 1))\nX_test = X_test.reshape((-1, lookback, 1))\ny_train = y_train.reshape((-1, 1))\ny_test = y_test.reshape((-1, 1))\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train).float()\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test).float()\n\n# Create custom Dataset\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n\ntrain_dataset = TimeSeriesDataset(X_train, y_train)\ntest_dataset = TimeSeriesDataset(X_test, y_test)\n\n# DataLoader for batching\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# LSTM model\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_stacked_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_stacked_layers = num_stacked_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Initialize model\nmodel = LSTM(1, 4, 1)\nmodel.to(device)\n\n# Accuracy calculation metrics\ndef calculate_accuracy(y_true, y_pred):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    r2 = r2_score(y_true, y_pred)\n    return mae, rmse, r2\n\n# Training and validation\nlearning_rate = 0.001\nnum_epochs = 10\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\ndef train_one_epoch():\n    model.train(True)\n    print(f'Epoch: {epoch + 1}')\n    running_loss = 0.0\n\n    for batch_index, batch in enumerate(train_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n        output = model(x_batch)\n        loss = loss_function(output, y_batch)\n        running_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_index % 100 == 99:\n            avg_loss_across_batches = running_loss / 100\n            print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1, avg_loss_across_batches))\n            running_loss = 0.0\n\ndef validate_one_epoch():\n    model.train(False)\n    running_loss = 0.0\n\n    for batch_index, batch in enumerate(test_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n        with torch.no_grad():\n            output = model(x_batch)\n            loss = loss_function(output, y_batch)\n            running_loss += loss.item()\n\n    avg_loss_across_batches = running_loss / len(test_loader)\n    print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n    print('***************************************************')\n    print()\n\n# Training loop\nfor epoch in range(num_epochs):\n    train_one_epoch()\n    validate_one_epoch()\n\n# Prediction on training data\nwith torch.no_grad():\n    predicted = model(X_train.to(device)).to('cpu').numpy()\n\ntrain_predictions = predicted.flatten()\n\n# Rescale the predictions\ndummies = np.zeros((X_train.shape[0], lookback+1))\ndummies[:, 0] = train_predictions\ndummies = scaler.inverse_transform(dummies)\ntrain_predictions_rescaled = dc(dummies[:, 0])\n\n# Rescale actual values\ndummies[:, 0] = y_train.flatten()\ndummies = scaler.inverse_transform(dummies)\nnew_y_train_rescaled = dc(dummies[:, 0])\n\n# Calculate accuracy for training\nmae_train, rmse_train, r2_train = calculate_accuracy(new_y_train_rescaled, train_predictions_rescaled)\nprint(f'Training Accuracy - MAE: {mae_train:.3f}, RMSE: {rmse_train:.3f}, R²: {r2_train:.3f}')\n\n# Prediction on test data\ntest_predictions = model(X_test.to(device)).detach().cpu().numpy().flatten()\n\n# Rescale the test predictions\ndummies = np.zeros((X_test.shape[0], lookback+1))\ndummies[:, 0] = test_predictions\ndummies = scaler.inverse_transform(dummies)\ntest_predictions_rescaled = dc(dummies[:, 0])\n\n# Rescale actual test values\ndummies[:, 0] = y_test.flatten()\ndummies = scaler.inverse_transform(dummies)\nnew_y_test_rescaled = dc(dummies[:, 0])\n\n# Calculate accuracy for testing\nmae_test, rmse_test, r2_test = calculate_accuracy(new_y_test_rescaled, test_predictions_rescaled)\nprint(f'Test Accuracy - MAE: {mae_test:.3f}, RMSE: {rmse_test:.3f}, R²: {r2_test:.3f}')\n\n# Plot the results\nplt.plot(new_y_train_rescaled, label='Actual Close (Train)')\nplt.plot(train_predictions_rescaled, label='Predicted Close (Train)')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.legend()\nplt.show()\n\nplt.plot(new_y_test_rescaled, label='Actual Close (Test)')\nplt.plot(test_predictions_rescaled, label='Predicted Close (Test)')\nplt.xlabel('Day')\nplt.ylabel('Close')\nplt.legend()\nplt.show()\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom torch.utils.data import Dataset, DataLoader\nfrom copy import deepcopy as dc\n\n# Load the data\ndata = pd.read_csv('E:/AMZN.csv', encoding='latin1')\n\n# Preprocess the data\ndata = data[['Date', 'Close']]\n\n# Convert non-numeric columns to categorical type and encode them\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\ndata['Date'] = data['Date'].to_numpy()\n\n# Plotting the data\nplt.plot(data['Date'], data['Close'])\nplt.xlabel('Date')\nplt.ylabel('Close')\nplt.show()\n\n# Prepare data for LSTM\ndef prepare_dataframe_for_lstm(df, n_steps):\n    df = dc(df)\n    df.set_index('Date', inplace=True)\n    for i in range(1, n_steps+1):\n        df[f'Close(t-{i})'] = df['Close'].shift(i)\n    df.dropna(inplace=True)\n    return df\n\nlookback = 7\nshifted_df = prepare_dataframe_for_lstm(data, lookback)\n\n# Normalize the data\nshifted_df_as_np = shifted_df.to_numpy()\nscaler = MinMaxScaler(feature_range=(-1, 1))\nshifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n\nX = shifted_df_as_np[:, 1:]\ny = shifted_df_as_np[:, 0]\n\n# Reverse the input data for LSTM\nX = dc(np.flip(X, axis=1))\n\n# Train-test split\nsplit_index = int(len(X) * 0.95)\nX_train = X[:split_index]\nX_test = X[split_index:]\ny_train = y[:split_index]\ny_test = y[split_index:]\n\n# Reshaping data for LSTM\nX_train = X_train.reshape((-1, lookback, 1))\nX_test = X_test.reshape((-1, lookback, 1))\ny_train = y_train.reshape((-1, 1))\ny_test = y_test.reshape((-1, 1))\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train).float()\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test).float()\n\n# Create custom Dataset\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n\ntrain_dataset = TimeSeriesDataset(X_train, y_train)\ntest_dataset = TimeSeriesDataset(X_test, y_test)\n\n# DataLoader for batching\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# LSTM model\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_stacked_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_stacked_layers = num_stacked_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Initialize model\nmodel = LSTM(1, 4, 1)\nmodel.to(device)\n\n# Training loop\nlearning_rate = 0.001\nnum_epochs = 10\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\ndef train_one_epoch():\n    model.train(True)\n    running_loss = 0.0\n\n    for batch_index, batch in enumerate(train_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n        output = model(x_batch)\n        loss = loss_function(output, y_batch)\n        running_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ndef validate_one_epoch():\n    model.train(False)\n    running_loss = 0.0\n\n    for batch_index, batch in enumerate(test_loader):\n        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n        with torch.no_grad():\n            output = model(x_batch)\n            loss = loss_function(output, y_batch)\n            running_loss += loss.item()\n\n    avg_loss_across_batches = running_loss / len(test_loader)\n    print(f'Val Loss: {avg_loss_across_batches:.3f}')\n\n# Training loop\nfor epoch in range(num_epochs):\n    train_one_epoch()\n    validate_one_epoch()\n\n# Prediction on test data\nwith torch.no_grad():\n    test_predictions = model(X_test.to(device)).detach().cpu().numpy().flatten()\n\n# Rescale the test predictions\ndummies = np.zeros((X_test.shape[0], lookback+1))\ndummies[:, 0] = test_predictions\ndummies = scaler.inverse_transform(dummies)\ntest_predictions_rescaled = dc(dummies[:, 0])\n\n# Rescale actual test values\ndummies[:, 0] = y_test.flatten()\ndummies = scaler.inverse_transform(dummies)\nnew_y_test_rescaled = dc(dummies[:, 0])\n\n# Convert predictions and actual values into binary classes (Up/Down)\n# If predicted value is higher than the actual, label it as 1 (Up), else 0 (Down)\ny_pred_class = (test_predictions_rescaled > new_y_test_rescaled).astype(int)\ny_true_class = (new_y_test_rescaled > np.roll(new_y_test_rescaled, 1)).astype(int)  # Shift to get the direction of movement\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_true_class[1:], y_pred_class[1:])  # Exclude the first point to avoid index error\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate other metrics\nmae_test = mean_absolute_error(new_y_test_rescaled, test_predictions_rescaled)\nrmse_test = np.sqrt(mean_squared_error(new_y_test_rescaled, test_predictions_rescaled))\nr2_test = r2_score(new_y_test_rescaled, test_predictions_rescaled)\nprint(f'Test Accuracy - MAE: {mae_test:.3f}, RMSE: {rmse_test:.3f}, R²: {r2_test:.3f}')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SYMBOLIC-REGRESSION-2(MORE ACCURATE)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Load dataset\ndata_path = r\"E:/URP/thesis/train file-kuznet curve.csv\"\ndf = pd.read_csv(data_path, encoding='latin1')\n\n# Ensure target column exists and drop rows where target is NaN\ntarget_column = \"gdp\"\ndf = df.dropna(subset=[target_column]).reset_index(drop=True)\n\n# Encode categorical features\nlabel_encoder = LabelEncoder()\nfor column in df.select_dtypes(include=['object']):\n    df[column] = df[column].astype(str)\n    df[column] = label_encoder.fit_transform(df[column])\n\n# Automatically detect feature columns (all except target)\nfeature_columns = [col for col in df.columns if col != target_column]\n\n# Handle missing values in features using mean imputation\nimputer = SimpleImputer(strategy='mean')\nX = imputer.fit_transform(df[feature_columns])\ny = df[target_column].values\n\n# Conditional logarithmic transformation for features\ndef signed_log(x):\n    \"\"\"If x > 0: ln(x), if x < 0: -ln(|x|), if x=0: 0\"\"\"\n    x = np.array(x)\n    result = np.zeros_like(x, dtype=float)\n    pos_mask = x > 0\n    neg_mask = x < 0\n    result[pos_mask] = np.log(x[pos_mask] + epsilon)\n    result[neg_mask] = -np.log(np.abs(x[neg_mask]) + epsilon)\n    return result\n\nln_X = signed_log(X)\nln_y = signed_log(y).reshape(-1, 1)\n\n# Create polynomial features\ndegree = 2  # Adjust degree as needed\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(ln_X)\n\n# Fit linear regression on transformed data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y.ravel())\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y)\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Generate dynamic equation\nfeature_names = poly.get_feature_names_out([f\"sln({col})\" for col in feature_columns])\nequation = f\"sln(gdp) = {intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\n\n# Print results\nprint(\"Equation in transformed space (signed-log with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.5f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load test data\ntest_df = pd.read_csv('/kaggle/input/russian-car-train-s1/russian_car_train.csv')\n\n# Load encoded mappings\nencoded_letters_dict = pd.read_csv('/kaggle/input/encoded-letters-for-russian-car/encoded_letters.csv') \\\n    .set_index('original')['encoded'].to_dict()\nencoded_home_dict = pd.read_csv('/kaggle/input/unique-region-for-russian-cars/unique_region_name_encodings.csv') \\\n    .set_index('region_name')['region_name_encoded'].to_dict()\n\n# Map categorical columns to encoded values\ntest_df['letters'] = test_df['letters'].apply(lambda x: encoded_letters_dict.get(x, 0))\ntest_df['region_name'] = test_df['region_name'].apply(lambda x: encoded_home_dict.get(x, 0))\n\n# Convert boolean/int flags to int\nbool_cols = ['is_repeating_letters', 'is_repeating_digits', 'is_palindrome_digits',\n             'contains_lucky_digit_7', 'contains_lucky_digit_8', 'is_government_plate',\n             'has_road_advantage', 'is_weekend']\nfor col in bool_cols:\n    test_df[col] = test_df[col].astype(int)\n\n# Replace zeros with epsilon for log safety\nepsilon = 1e-9\ntest_df = test_df.replace(0, epsilon).astype(float)\n\n\ndef safe_ln(x, epsilon=1e-10):\n    return np.log(np.abs(x) + epsilon)\n\ndef calculate_plate_price(df):\n  # Compute log-transformed features\n  ln_digits = safe_ln(df['digits'])\n  ln_region = safe_ln(df['region'])\n  ln_letters = safe_ln(df['letters'])\n  ln_region_name = safe_ln(df['region_name'])\n  ln_is_government_plate = safe_ln(df['is_government_plate'])\n  ln_has_road_advantage = safe_ln(df['has_road_advantage'])\n  ln_gov_significance_score = safe_ln(df['gov_significance_score'])\n  ln_is_repeating_letters = safe_ln(df['is_repeating_letters'])\n  ln_is_repeating_digits = safe_ln(df['is_repeating_digits'])\n  ln_is_palindrome_digits = safe_ln(df['is_palindrome_digits'])\n  ln_contains_lucky_digit_7 = safe_ln(df['contains_lucky_digit_7'])\n  ln_contains_lucky_digit_8 = safe_ln(df['contains_lucky_digit_8'])\n  ln_year = safe_ln(df['year'])\n  ln_month = safe_ln(df['month'])\n  ln_day = safe_ln(df['day'])\n  ln_weekday = safe_ln(df['weekday'])\n  ln_is_weekend = safe_ln(df['is_weekend'])\n  ln_hour = safe_ln(df['hour'])\n  ln_minute = safe_ln(df['minute'])\n  ln_second = safe_ln(df['second'])\n  ln_plate_listing_count = safe_ln(df['plate_listing_count'])\n    \n  # Apply symbolic equation to compute predictions\n  price = (9910345.44054 +\n             445.22990 * ln_digits +\n             1223.03444 * ln_region +\n             8.39760 * ln_letters +\n             -44.25892 * ln_region_name +\n             21959.09750 * ln_is_government_plate +\n             22112.47253 * ln_has_road_advantage +\n             -2054.66321 * ln_gov_significance_score +\n             7526.61748 * ln_is_repeating_letters +\n             1493.58991 * ln_is_repeating_digits +\n             82.20587 * ln_is_palindrome_digits +\n             1595.16359 * ln_contains_lucky_digit_7 +\n             1570.42135 * ln_contains_lucky_digit_8 +\n             -2605741.06766 * ln_year +\n             -390.45876 * ln_month +\n             -86.99772 * ln_day +\n             -8.20003 * ln_weekday +\n             1648.11501 * ln_is_weekend +\n             -38.43041 * ln_hour +\n             -19.95811 * ln_minute +\n             4.49435 * ln_second +\n             -145.28560 * ln_plate_listing_count +\n             0.07441 * ln_digits**2 +\n             -0.01274 * ln_digits * ln_region +\n             -0.00379 * ln_digits * ln_letters +\n             -0.00053 * ln_digits * ln_region_name +\n             0.05214 * ln_digits * ln_is_government_plate +\n             -0.00534 * ln_digits * ln_has_road_advantage +\n             -0.04696 * ln_digits * ln_gov_significance_score +\n             -0.00900 * ln_digits * ln_is_repeating_letters +\n             0.00301 * ln_digits * ln_is_repeating_digits +\n             -0.00455 * ln_digits * ln_is_palindrome_digits +\n             -0.00591 * ln_digits * ln_contains_lucky_digit_7 +\n             -0.00280 * ln_digits * ln_contains_lucky_digit_8 +\n             -58.62074 * ln_digits * ln_year +\n             -0.00101 * ln_digits * ln_month +\n             -0.00500 * ln_digits * ln_day +\n             0.00004 * ln_digits * ln_weekday +\n             0.00014 * ln_digits * ln_is_weekend +\n             -0.00063 * ln_digits * ln_hour +\n             0.00074 * ln_digits * ln_minute +\n             0.00086 * ln_digits * ln_second +\n             0.03658 * ln_digits * ln_plate_listing_count +\n             -0.02610 * ln_region**2 +\n             0.00203 * ln_region * ln_letters +\n             -0.00453 * ln_region * ln_region_name +\n             0.24540 * ln_region * ln_is_government_plate +\n             0.03635 * ln_region * ln_has_road_advantage +\n             -0.24897 * ln_region * ln_gov_significance_score +\n             0.00853 * ln_region * ln_is_repeating_letters +\n             0.00420 * ln_region * ln_is_repeating_digits +\n             0.00383 * ln_region * ln_is_palindrome_digits +\n             0.00218 * ln_region * ln_contains_lucky_digit_7 +\n             -0.00060 * ln_region * ln_contains_lucky_digit_8 +\n             -160.49811 * ln_region * ln_year +\n             0.00178 * ln_region * ln_month +\n             0.00317 * ln_region * ln_day +\n             0.00016 * ln_region * ln_weekday +\n             -0.00045 * ln_region * ln_is_weekend +\n             0.00242 * ln_region * ln_hour +\n             -0.00010 * ln_region * ln_minute +\n             -0.00540 * ln_region * ln_second +\n             -0.01395 * ln_region * ln_plate_listing_count +\n             0.00162 * ln_letters**2 +\n             0.00021 * ln_letters * ln_region_name +\n             0.07941 * ln_letters * ln_is_government_plate +\n             -0.03561 * ln_letters * ln_has_road_advantage +\n             -0.03972 * ln_letters * ln_gov_significance_score +\n             0.00407 * ln_letters * ln_is_repeating_letters +\n             0.00160 * ln_letters * ln_is_repeating_digits +\n             0.00019 * ln_letters * ln_is_palindrome_digits +\n             -0.00001 * ln_letters * ln_contains_lucky_digit_7 +\n             -0.00002 * ln_letters * ln_contains_lucky_digit_8 +\n             -1.08391 * ln_letters * ln_year +\n             0.00049 * ln_letters * ln_month +\n             -0.00051 * ln_letters * ln_day +\n             0.00011 * ln_letters * ln_weekday +\n             -0.00023 * ln_letters * ln_is_weekend +\n             -0.00026 * ln_letters * ln_hour +\n             -0.00000 * ln_letters * ln_minute +\n             0.00031 * ln_letters * ln_second +\n             -0.00788 * ln_letters * ln_plate_listing_count +\n             -0.00233 * ln_region_name**2 +\n             0.16600 * ln_region_name * ln_is_government_plate +\n             -0.03346 * ln_region_name * ln_has_road_advantage +\n             -0.14930 * ln_region_name * ln_gov_significance_score +\n             0.00044 * ln_region_name * ln_is_repeating_letters +\n             -0.00062 * ln_region_name * ln_is_repeating_digits +\n             -0.00001 * ln_region_name * ln_is_palindrome_digits +\n             -0.00006 * ln_region_name * ln_contains_lucky_digit_7 +\n             -0.00009 * ln_region_name * ln_contains_lucky_digit_8 +\n             5.75453 * ln_region_name * ln_year +\n             0.00164 * ln_region_name * ln_month +\n             -0.00173 * ln_region_name * ln_day +\n             0.00001 * ln_region_name * ln_weekday +\n             -0.00002 * ln_region_name * ln_is_weekend +\n             -0.01463 * ln_region_name * ln_hour +\n             -0.00700 * ln_region_name * ln_minute +\n             0.01981 * ln_region_name * ln_second +\n             -0.00040 * ln_region_name * ln_plate_listing_count +\n             -623.79252 * ln_is_government_plate**2 +\n             93.11939 * ln_is_government_plate * ln_has_road_advantage +\n             1392.28005 * ln_is_government_plate * ln_gov_significance_score +\n             0.00097 * ln_is_government_plate * ln_is_repeating_letters +\n             -0.03138 * ln_is_government_plate * ln_is_repeating_digits +\n             0.00738 * ln_is_government_plate * ln_is_palindrome_digits +\n             0.00400 * ln_is_government_plate * ln_contains_lucky_digit_7 +\n             0.01188 * ln_is_government_plate * ln_contains_lucky_digit_8 +\n             -272.01339 * ln_is_government_plate * ln_year +\n             0.08776 * ln_is_government_plate * ln_month +\n             -0.06306 * ln_is_government_plate * ln_day +\n             0.00184 * ln_is_government_plate * ln_weekday +\n             0.00368 * ln_is_government_plate * ln_is_weekend +\n             -0.01281 * ln_is_government_plate * ln_hour +\n             0.00070 * ln_is_government_plate * ln_minute +\n             -0.00325 * ln_is_government_plate * ln_second +\n             -0.26698 * ln_is_government_plate * ln_plate_listing_count +\n             958.23524 * ln_has_road_advantage**2 +\n             -0.10460 * ln_has_road_advantage * ln_gov_significance_score +\n             -0.00099 * ln_has_road_advantage * ln_is_repeating_letters +\n             -0.00117 * ln_has_road_advantage * ln_is_repeating_digits +\n             0.00071 * ln_has_road_advantage * ln_is_palindrome_digits +\n             -0.00023 * ln_has_road_advantage * ln_contains_lucky_digit_7 +\n             0.00138 * ln_has_road_advantage * ln_contains_lucky_digit_8 +\n             -6.29474 * ln_has_road_advantage * ln_year +\n             -0.00390 * ln_has_road_advantage * ln_month +\n             -0.00253 * ln_has_road_advantage * ln_day +\n             -0.00098 * ln_has_road_advantage * ln_weekday +\n             0.00103 * ln_has_road_advantage * ln_is_weekend +\n             -0.00961 * ln_has_road_advantage * ln_hour +\n             0.01802 * ln_has_road_advantage * ln_minute +\n             -0.00964 * ln_has_road_advantage * ln_second +\n             -0.05788 * ln_has_road_advantage * ln_plate_listing_count +\n             2.14539 * ln_gov_significance_score**2 +\n             -0.00177 * ln_gov_significance_score * ln_is_repeating_letters +\n             0.02980 * ln_gov_significance_score * ln_is_repeating_digits +\n             -0.00703 * ln_gov_significance_score * ln_is_palindrome_digits +\n             -0.00385 * ln_gov_significance_score * ln_contains_lucky_digit_7 +\n             -0.01227 * ln_gov_significance_score * ln_contains_lucky_digit_8 +\n             269.57368 * ln_gov_significance_score * ln_year +\n             -0.08170 * ln_gov_significance_score * ln_month +\n             0.06395 * ln_gov_significance_score * ln_day +\n             -0.00101 * ln_gov_significance_score * ln_weekday +\n             -0.00425 * ln_gov_significance_score * ln_is_weekend +\n             0.02166 * ln_gov_significance_score * ln_hour +\n             -0.01772 * ln_gov_significance_score * ln_minute +\n             0.01140 * ln_gov_significance_score * ln_second +\n             0.29529 * ln_gov_significance_score * ln_plate_listing_count +\n             331.83945 * ln_is_repeating_letters**2 +\n             0.00119 * ln_is_repeating_letters * ln_is_repeating_digits +\n             0.00145 * ln_is_repeating_letters * ln_is_palindrome_digits +\n             0.00005 * ln_is_repeating_letters * ln_contains_lucky_digit_7 +\n             -0.00002 * ln_is_repeating_letters * ln_contains_lucky_digit_8 +\n             15.01581 * ln_is_repeating_letters * ln_year +\n             0.00019 * ln_is_repeating_letters * ln_month +\n             0.00037 * ln_is_repeating_letters * ln_day +\n             -0.00012 * ln_is_repeating_letters * ln_weekday +\n             -0.00010 * ln_is_repeating_letters * ln_is_weekend +\n             -0.00013 * ln_is_repeating_letters * ln_hour +\n             -0.00006 * ln_is_repeating_letters * ln_minute +\n             -0.00005 * ln_is_repeating_letters * ln_second +\n             -0.00959 * ln_is_repeating_letters * ln_plate_listing_count +\n             70.24220 * ln_is_repeating_digits**2 +\n             -60301.93418 * ln_is_repeating_digits * ln_is_palindrome_digits +\n             0.00112 * ln_is_repeating_digits * ln_contains_lucky_digit_7 +\n             0.00030 * ln_is_repeating_digits * ln_contains_lucky_digit_8 +\n             16.26346 * ln_is_repeating_digits * ln_year +\n             0.00205 * ln_is_repeating_digits * ln_month +\n             -0.00033 * ln_is_repeating_digits * ln_day +\n             -0.00004 * ln_is_repeating_digits * ln_weekday +\n             0.00001 * ln_is_repeating_digits * ln_is_weekend +\n             0.00027 * ln_is_repeating_digits * ln_hour +\n             -0.00009 * ln_is_repeating_digits * ln_minute +\n             -0.00004 * ln_is_repeating_digits * ln_second +\n             -0.01162 * ln_is_repeating_digits * ln_plate_listing_count +\n             60307.11675 * ln_is_palindrome_digits**2 +\n             0.00045 * ln_is_palindrome_digits * ln_contains_lucky_digit_7 +\n             0.00032 * ln_is_palindrome_digits * ln_contains_lucky_digit_8 +\n             4.88521 * ln_is_palindrome_digits * ln_year +\n             -0.00046 * ln_is_palindrome_digits * ln_month +\n             -0.00033 * ln_is_palindrome_digits * ln_day +\n             0.00007 * ln_is_palindrome_digits * ln_weekday +\n             -0.00005 * ln_is_palindrome_digits * ln_is_weekend +\n             -0.00009 * ln_is_palindrome_digits * ln_hour +\n             -0.00013 * ln_is_palindrome_digits * ln_minute +\n             -0.00015 * ln_is_palindrome_digits * ln_second +\n             0.00269 * ln_is_palindrome_digits * ln_plate_listing_count +\n             69.40251 * ln_contains_lucky_digit_7**2 +\n             -0.00007 * ln_contains_lucky_digit_7 * ln_contains_lucky_digit_8 +\n             0.38824 * ln_contains_lucky_digit_7 * ln_year +\n             0.00007 * ln_contains_lucky_digit_7 * ln_month +\n             -0.00081 * ln_contains_lucky_digit_7 * ln_day +\n             0.00007 * ln_contains_lucky_digit_7 * ln_weekday +\n             0.00001 * ln_contains_lucky_digit_7 * ln_is_weekend +\n             -0.00004 * ln_contains_lucky_digit_7 * ln_hour +\n             0.00002 * ln_contains_lucky_digit_7 * ln_minute +\n             0.00024 * ln_contains_lucky_digit_7 * ln_second +\n             -0.00396 * ln_contains_lucky_digit_7 * ln_plate_listing_count +\n             67.56364 * ln_contains_lucky_digit_8**2 +\n             -1.92583 * ln_contains_lucky_digit_8 * ln_year +\n             -0.00036 * ln_contains_lucky_digit_8 * ln_month +\n             0.00027 * ln_contains_lucky_digit_8 * ln_day +\n             -0.00008 * ln_contains_lucky_digit_8 * ln_weekday +\n             0.00002 * ln_contains_lucky_digit_8 * ln_is_weekend +\n             -0.00051 * ln_contains_lucky_digit_8 * ln_hour +\n             0.00051 * ln_contains_lucky_digit_8 * ln_minute +\n             0.00006 * ln_contains_lucky_digit_8 * ln_second +\n             -0.00328 * ln_contains_lucky_digit_8 * ln_plate_listing_count +\n             171283.00356 * ln_year**2 +\n             51.34567 * ln_year * ln_month +\n             11.43708 * ln_year * ln_day +\n             1.04852 * ln_year * ln_weekday +\n             -1.54772 * ln_year * ln_is_weekend +\n             5.05070 * ln_year * ln_hour +\n             2.62844 * ln_year * ln_minute +\n             -0.59840 * ln_year * ln_second +\n             18.92864 * ln_year * ln_plate_listing_count +\n             -0.05463 * ln_month**2 +\n             -0.04369 * ln_month * ln_day +\n             0.00043 * ln_month * ln_weekday +\n             -0.00144 * ln_month * ln_is_weekend +\n             0.00306 * ln_month * ln_hour +\n             -0.00013 * ln_month * ln_minute +\n             -0.00208 * ln_month * ln_second +\n             -0.00143 * ln_month * ln_plate_listing_count +\n             0.00044 * ln_day**2 +\n             0.00117 * ln_day * ln_weekday +\n             -0.00010 * ln_day * ln_is_weekend +\n             -0.00070 * ln_day * ln_hour +\n             0.00119 * ln_day * ln_minute +\n             -0.00005 * ln_day * ln_second +\n             -0.00430 * ln_day * ln_plate_listing_count +\n             -0.00107 * ln_weekday**2 +\n             -0.00782 * ln_weekday * ln_is_weekend +\n             -0.00037 * ln_weekday * ln_hour +\n             -0.00013 * ln_weekday * ln_minute +\n             0.00036 * ln_weekday * ln_second +\n             0.00124 * ln_weekday * ln_plate_listing_count +\n             71.06380 * ln_is_weekend**2 +\n             0.00013 * ln_is_weekend * ln_hour +\n             0.00004 * ln_is_weekend * ln_minute +\n             -0.00019 * ln_is_weekend * ln_second +\n             0.00046 * ln_is_weekend * ln_plate_listing_count +\n             -0.00020 * ln_hour**2 +\n             0.00105 * ln_hour * ln_minute +\n             -0.00101 * ln_hour * ln_second +\n             -0.00597 * ln_hour * ln_plate_listing_count +\n             -0.00002 * ln_minute**2 +\n             0.00049 * ln_minute * ln_second +\n             -0.00165 * ln_minute * ln_plate_listing_count +\n             0.00068 * ln_second**2 +\n             -0.00192 * ln_second * ln_plate_listing_count +\n             0.02578 * ln_plate_listing_count**2)\n  return np.abs(price)\n  \ntest_df['price'] = calculate_plate_price(test_df)\n# Prepare submission DataFrame\nsubmission_df = pd.DataFrame()\nsubmission_df['id'] = range(51636, 51636 + len(test_df))\nsubmission_df['price'] = test_df['price']\n\nsubmission_df.to_csv(\"submission.csv1\", index=False)\nprint(submission_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"inference","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Take absolute values of y and input features\ny_positive = np.abs(y)\nx1_positive = np.abs(x1)\nx2_positive = np.abs(x2)\nx3_positive = np.abs(x3)\n\n# Logarithmic transformations\nln_y = np.log(y_positive)\nln_x1 = np.log(x1_positive)\nln_x2 = np.log(x2_positive)\nln_x3 = np.log(x3_positive)\n\n# Combine x1, x2, x3 into a multivariate feature matrix\nX_log = np.vstack((ln_x1, ln_x2, ln_x3)).T\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this value for higher-degree nurturing\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_log)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Display the equation in transformed space\nfeature_names = poly.get_feature_names_out([\"ln(|x1|)\", \"ln(|x2|)\", \"ln(|x3|)\"])\nequation = f\"ln([y]) = {intercept:.2f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.2f}*{name}\"\n\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.2f}\")\n#COULMN-STACK:\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Take absolute values of y and input features\ny_positive = np.abs(y)\nx1_positive = np.abs(x1)\nx2_positive = np.abs(x2)\nx3_positive = np.abs(x3)\n\n# Logarithmic transformations\nln_y = np.log(y_positive)\nln_x1 = np.log(x1_positive)\nln_x2 = np.log(x2_positive)\nln_x3 = np.log(x3_positive)\n\n# Combine x1, x2, x3 into a multivariate feature matrix\nX_log = np.column_stack((ln_x1, ln_x2, ln_x3))\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this value for higher-degree nurturing\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_log)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Display the equation in transformed space\nfeature_names = poly.get_feature_names_out([\"ln(|x1|)\", \"ln(|x2|)\", \"ln(|x3|)\"])\nequation = f\"ln([y]) = {intercept:.2f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.2f}*{name}\"\n\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.2f}\")\n#2D Array\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Take absolute values of y and input features\ny_positive = np.abs(y) + 1e-10\nx1_positive = np.abs(x1) + 1e-10\nx2_positive = np.abs(x2) + 1e-10\nx3_positive = np.abs(x3) + 1e-10\n\n# Logarithmic transformations with reshaping\nln_y = np.log(y_positive).reshape(-1, 1)  # Target variable, reshaped to 2D\nln_x1 = np.log(x1_positive).reshape(-1, 1)\nln_x2 = np.log(x2_positive).reshape(-1, 1)\nln_x3 = np.log(x3_positive).reshape(-1, 1)\n\n# Combine x1, x2, x3 into a multivariate feature matrix\nX_log = np.hstack((ln_x1, ln_x2, ln_x3))  # Combine features horizontally into a matrix\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this value for higher-degree nurturing\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_log)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y.ravel())  # Use .ravel() to flatten ln_y for regression\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Display the equation in transformed space\nfeature_names = poly.get_feature_names_out([\"ln(|x1|)\", \"ln(|x2|)\", \"ln(|x3|)\"])\nequation = f\"ln([y]) = {intercept:.2f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.2f}*{name}\"\n\n# Print the results\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.2f}\")\n#1 feature\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Given data\nx1 = np.array([30, 45, 60])\ny = np.array([0.5, 0.71, 0.867])\n\n# Take absolute values of y and input features\ny_positive = np.abs(y) + 1e-10\nx1_positive = np.abs(x1) + 1e-10\n\n# Logarithmic transformations\nln_y = np.log(y_positive)\nln_x1 = np.log(x1_positive)\n\n# Reshape ln_x1 into a 2D array (required for PolynomialFeatures)\nX_log = ln_x1.reshape(-1, 1)\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this value for higher-degree nurturing\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_log)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Display the equation in transformed space\nfeature_names = poly.get_feature_names_out([\"ln(|x1|)\"])\nequation = f\"ln([y]) = {intercept:.2f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.2f}*{name}\"\n\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.2f}\")\n#Linear-Transformation\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import linregress\n\nx = np.array([3.4, 3.81,4.1])\n\ny = np.array([-0.69,-0.34,-0.143])\n\n\n\n# Perform linear regression\nslope, intercept, r_value, p_value, std_err = linregress(x, y)\ny_fit = slope * x + intercept\n\n# Plot the graph\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, marker='o', linestyle='-', color='blue', label='Original Data')\nplt.plot(x, y_fit, linestyle='--', color='red', label=f'Linear Fit: y={slope:.2f}x+{intercept:.2f}')\n\n# Add title and labels\nplt.title('Plot of Given x and y Values with Linear Fit', fontsize=14)\nplt.xlabel('x', fontsize=12)\nplt.ylabel('y', fontsize=12)\n\n# Add grid and legend\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend(fontsize=12)\n\n# Show the plot\nplt.show()\n\n# Print the equation and R-squared\nprint(f\"Linear Equation: y = {slope:.2f}x + {intercept:.2f}\")\nprint(f\"R-squared: {r_value**2:.2f}\")\n#Multi-feature fraction symbolic Regression\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Take absolute values of y and input features\ny_positive = np.abs(y) + 1e-10\nx1_positive = np.abs(x1) + 1e-10\nx2_positive = np.abs(x2) + 1e-10\nx3_positive = np.abs(x3) + 1e-10\n\n# Logarithmic transformations with reshaping\nln_y = np.log(y_positive).reshape(-1, 1)\nln_x1 = np.log(x1_positive).reshape(-1, 1)\nln_x2 = np.log(x2_positive).reshape(-1, 1)\nln_x3 = np.log(x3_positive).reshape(-1, 1)\n\n# Generate custom polynomial features for degrees 1 to 3 and fractional degree 3.5\nfeature_names = []\nX_custom = np.empty((ln_x1.shape[0], 0))  # Initialize empty feature matrix\n\n# Add features for degrees 1 to 3\nfor degree in [1, 2, 3]:\n    X_custom = np.hstack((X_custom, ln_x1**degree, ln_x2**degree, ln_x3**degree))\n    feature_names.extend([\n        f\"ln(|x1|)^{degree}\",\n        f\"ln(|x2|)^{degree}\",\n        f\"ln(|x3|)^{degree}\"\n    ])\n\n# Add fractional degree 3.5\nfractional_degree = 3.5\nX_custom = np.hstack((X_custom, ln_x1**fractional_degree, ln_x2**fractional_degree, ln_x3**fractional_degree))\nfeature_names.extend([\n    f\"ln(|x1|)^{fractional_degree}\",\n    f\"ln(|x2|)^{fractional_degree}\",\n    f\"ln(|x3|)^{fractional_degree}\"\n])\n\n# Perform multivariate linear regression on transformed data\nmodel = LinearRegression()\nmodel.fit(X_custom, ln_y.ravel())\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_custom)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Display the equation in transformed space\nequation = f\"ln([y]) = {intercept:.2f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.2f}*{name}\"\n\n# Print the results\nprint(\"Equation in transformed space (with fractional polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.2f}\")\n#single feature fraction symbolic regression\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Given data\nx1 = np.array([1, 5, 7,11,20])\ny = np.array([3, 37, 135,2059,1048596])\n\n# Take absolute values of y and input features\ny_positive = np.abs(y) + 1e-10\nx1_positive = np.abs(x1) + 1e-10\n\n# Logarithmic transformations\nln_y = np.log(y_positive)\nln_x1 = np.log(x1_positive)\n\n# Manually applying polynomial features, including fractional degree 3.5\nfractional_degree = 4.3\n\n# Create a custom feature matrix including ln(x1)^1, ln(x1)^2, ln(x1)^3, and ln(x1)^3.5\nX_custom = np.column_stack((\n    ln_x1**1,    # ln(x1)^1\n    ln_x1**2,    # ln(x1)^2\n    ln_x1**3,    # ln(x1)^3\n    ln_x1**4,    # ln(x1)^3\n    ln_x1**fractional_degree  # ln(x1)^3.5\n))\n\n# Perform multivariate linear regression on the transformed data\nmodel = LinearRegression()\nmodel.fit(X_custom, ln_y)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_custom)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Feature names for the custom features\nfeature_names = [\n    \"ln(|x1|)^1\", \"ln(|x1|)^2\", \"ln(|x1|)^3\",\"ln(|x1|)^4\", f\"ln(|x1|)^{fractional_degree}\"\n]\n\n# Display the equation in transformed space\nequation = f\"ln([y]) = {intercept:.2f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.2f}*{name}\"\n\n# Print the results\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.2f}\")\n#Number of symbolic regression\nimport numpy as np\nfrom scipy.optimize import fsolve\n\n# Function to calculate ln(y) for each pair of x1 and y\ndef equation(N, x1, y):\n    results = []\n    for xi, yi in zip(x1, y):\n        ln_y = np.log(yi)  # Calculate ln(y)\n        # Calculate the terms based on the given equation\n        term1 = 8.32541478\n        term2 = -23.19830324 * np.log(np.abs(xi)) ** 1\n        term3 = 23.86326923 * np.log(np.abs(xi)) ** 2\n        term4 = -9.14175787 * np.log(np.abs(xi)) ** N[0]  # Ensure N is treated as a scalar\n        # Append the difference for this pair\n        results.append(ln_y - (term1 + term2 + term3 + term4))\n    return np.array(results)  # Return a NumPy array of results\n\n# Example usage\nx1 = [69]  # Two values for x1\ny = [325188814]  # Corresponding values for y\n\n# Use fsolve to solve for N\nN_initial_guess = [1.0]  # Provide an initial guess for N\nN_solution = fsolve(equation, x0=N_initial_guess, args=(x1, y))\n\n# Output the solution for N\nprint(f\"The value of N that satisfies both pairs is approximately: {N_solution[0]}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Symbolic-Regression\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\n\n# Enable inline plotting for Spyder\n\n\n# Define the file path\nfile_path = os.path.join(\"E:/LHC-CERN/Z_boson.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(file_path)\n\n# Print basic information\nprint(df)\nprint(df.dtypes)\nprint(df.shape)\nprint(df[df['class'] == 'Zee'])\nprint(df[df['class'] == 'Zmumu'])\n\n# Extract pt1 values for both classes\npt1_class_zmumu = df[df['class'] == 'Zmumu']['pt1']\npt1_class_zee = df[df['class'] == 'Zee']['pt1']\n\n# Plot settings\nplt.figure(figsize=(8,6))  # Set figure size\nplt.xlabel(\"pt1\")\nplt.ylabel(\"Number Of Class\")\nplt.title(\"Class Prediction Visualization\")\n\n# Histogram\nplt.hist([pt1_class_zmumu, pt1_class_zee], bins=20, rwidth=0.95, color=['green','red'], label=['Class=Zmumu', 'Class=Zee'])\n\n# Add legend and show the plot\nplt.legend()\nplt.show()\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Define the file path\nfile_path = os.path.join(\"E:/LHC-CERN/Z_boson.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(file_path)\n\n# Extract pt1 values for both classes\npt1_class_zmumu = df[df['class'] == 'Zmumu']['pt1']\npt1_class_zee = df[df['class'] == 'Zee']['pt1']\n\n# Plot settings\nplt.figure(figsize=(8, 6))  # Set figure size\nplt.xlabel(\"pt1\")\nplt.ylabel(\"Number of Class\")\nplt.title(\"Class Prediction Visualization\")\n\n# Histogram with returned values\nn_zmumu, bins, _ = plt.hist(pt1_class_zmumu, bins=20, rwidth=0.95, color='green', alpha=0.7, label='Class=Zmumu', density=False)\nn_zee, _, _ = plt.hist(pt1_class_zee, bins=bins, rwidth=0.95, color='red', alpha=0.7, label='Class=Zee', density=False)\n\n# Add legend and show the plot\nplt.legend()\nplt.show()\n\n# Compute bin centers\nbin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of bins\n\n# Create a DataFrame with all bins\ndata = pd.DataFrame({\n    'pt1_bin_center': bin_centers, \n    'Zmumu_counts': n_zmumu, \n    'Zee_counts': n_zee\n})\n\n# **Remove rows where either class count is zero**\nfiltered_data = data[(data['Zmumu_counts'] > 0) & (data['Zee_counts'] > 0)]\n\n# Print the filtered lists horizontally\nprint(\"\\nFiltered pt1_bin_center:\", list(filtered_data['pt1_bin_center']))\nprint(\"Filtered Zmumu_counts:   \", list(filtered_data['Zmumu_counts']))\nprint(\"Filtered Zee_counts:     \", list(filtered_data['Zee_counts']))\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Small epsilon to avoid log(0)\nepsilon = 1e-2  \n\n# Sample bin centers for Tenure and Monthly Charges\ntenure_bins = np.array([10.1041075, 23.3849225, 36.6657375, 49.946552499999996, 63.2273675, 76.5081825, 89.7889975, \n                     103.06981249999998, 116.3506275, 129.6314425, 142.91225749999995, 156.19307249999997, 169.4738875, \n                     182.75470249999995, 196.03551749999997, 209.3163325, 222.59714749999995, 235.87796249999997, \n                     249.15877749999999, 262.4395925])\n\n# Churn=Yes and Churn=No counts\nchurn_yes_counts = np.array([592, 1870, 4286, 2632, 387, 112, 69, 26, 8, 6, 4, 2, 2, 1, 2, 0, 0, 0, 0, 1])\nchurn_no_counts = np.array([0, 1877, 5033, 2455, 421, 133, 45, 20, 8, 1, 3, 1, 0, 2, 0, 0, 0, 0, 1, 0])\n\n# Apply natural logarithm (ln) with epsilon to prevent log(0)\nln_churn_yes = np.log(churn_yes_counts + epsilon)\nln_churn_no = np.log(churn_no_counts + epsilon)\n\n# Compute log-odds\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to tenure bins\nln_tenure = np.log(tenure_bins + epsilon)\n\n# Stack transformed features\nX_data = ln_tenure.reshape(-1, 1)  # Ensure it's 2D for sklearn\n\n# Fit a polynomial model\ndegree = 301  # Adjust the degree (too high can overfit)\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100).reshape(-1, 1)\n\n# Transform fitted features\nX_fit_poly = poly.transform(tenure_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds, color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob, color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability\")\nplt.legend()\n\nplt.show()\n\n# Print logistic equation\nequation_terms = \" + \".join([\n    f\"{coef:.6f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_tenure']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.6f} + {equation_terms})))\")\n\n# Compute R² Score\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Sample bin centers for Tenure and Monthly Charges (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\nmonthly_charges_bins = np.array([20, 40, 60, 80, 100, 110, 120])  # 7 monthly charge bins\n\n# Churn=Yes and Churn=No counts (must match bin lengths)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Apply natural logarithm (ln) to absolute values of churn counts\nln_churn_yes = np.log(np.abs(churn_yes_counts) + epsilon)\nln_churn_no = np.log(np.abs(churn_no_counts) + epsilon)\n\n# Compute log-odds using log-transformed churn values\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to absolute feature values\nln_tenure = np.log(np.abs(tenure_bins) + epsilon)\nln_charges = np.log(np.abs(monthly_charges_bins) + epsilon)\n\n# Stack transformed features\nX_data = np.vstack((ln_tenure, ln_charges)).T\n\n# Fit a polynomial model to the transformed features\ndegree = 2  # Set polynomial degree\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit a linear regression model to the polynomial-transformed features\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100)\ncharges_fit = np.linspace(ln_charges.min(), ln_charges.max(), 100)\nX_fit = np.array(np.meshgrid(tenure_fit, charges_fit)).T.reshape(-1, 2)\n\n# Transform fitted features\nX_fit_poly = poly.transform(X_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds[:100], color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn (Log-Transformed Features & Churn)\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob[:100], color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability (Log-Transformed Features & Churn)\")\nplt.legend()\n\nplt.show()\n\n# Print the final logistic equation with explicit multiplication signs\nequation_terms = \" + \".join([\n    f\"{coef:.5f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_[tenure]', 'ln_[charges]']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.5f} + {equation_terms})))\")\n\n# Compute R² Score for the polynomial fit\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Main Logistic","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Load data from CSV\ndf = pd.read_csv(\"E:/AIRCRAFT/Aircraft Engines.csv\",encoding='latin1')  # Change \"data.csv\" to your actual filename\nfor column in df.select_dtypes(include=['object']):\n    df[column] = df[column].astype(str)\n\nlabel_encoder = LabelEncoder()\nfor column in df.select_dtypes(include=['object']):\n    df[column] = label_encoder.fit_transform(df[column])\n\n# Automatically detect feature columns (all except the last column)\n#feature_columns = df.columns[:-1]  # Assumes last column is the target (y)\n#target_column = df.columns[-1]  # Assumes last column is y\ntarget_column = \"power\"  # Set your target column explicitly\nfeature_columns = [col for col in df.columns if col != target_column] \n# Extract features (X) and target (y)\nX = df[feature_columns].values  # Feature matrix\ny = df[target_column].values  # Target variable\n\n# Take absolute values and add epsilon for stability\ny_positive = np.abs(y) + epsilon\nX_positive = np.abs(X) + epsilon  # Apply to all feature columns\n\n# Logarithmic transformations\nln_y = np.log(y_positive).reshape(-1, 1)  # Log-transformed target\nln_X = np.log(X_positive)  # Log-transformed features\n\n# Create polynomial features (degree can be adjusted)\ndegree = 7  # Adjust this value for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(ln_X)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y.ravel())  # Use .ravel() to flatten ln_y\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Generate equation dynamically based on input features\nfeature_names = poly.get_feature_names_out([f\"ln(|{col}|)\" for col in feature_columns])\nequation = f\"ln(y) = {intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\n\n# Print the results\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.5f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CSV SYMBOLIC","metadata":{}},{"cell_type":"code","source":"#@title polynomial to logistic \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import r2_score\n\n# Define a cubic polynomial function\ndef poly_func(x, a, b, c, d):\n    return a*x**3 + b*x**2 + c*x + d\n\n# Define the logistic function using polynomial output\ndef logistic_func(x, a, b, c, d):\n    return 1 / (1 + np.exp(-(a*x**3 + b*x**2 + c*x + d)))\n\n# Sample bin centers (adjusted to ensure matching counts)\nbin_centers = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 bins\n\n# Churn=Yes and Churn=No counts (must match bin_centers length)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Compute log-odds: log(Churn Yes / Churn No)\nlog_odds = np.log(churn_yes_counts / churn_no_counts)\n\n# Fit the polynomial to the log-odds data\nparams, _ = curve_fit(poly_func, bin_centers, log_odds)\n\n# Generate fitted curve\nx_fit = np.linspace(0, 70, 100)\ny_fit_log_odds = poly_func(x_fit, *params)\n\n# Convert to probability using the logistic function\ny_fit_prob = logistic_func(x_fit, *params)\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(bin_centers, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(x_fit, y_fit_log_odds, color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(bin_centers, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(x_fit, y_fit_prob, color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability\")\nplt.legend()\n\nplt.show()\n\n# Print the final logistic equation\na, b, c, d = params\nprint(f\"Final Logistic Equation:\")\nprint(f\"P(Churn=Yes) = 1 / (1 + e^(-({a:.5f}x³ + {b:.5f}x² + {c:.5f}x + {d:.5f})))\")\n\n# Compute R² Score for the polynomial fit\nlog_odds_pred = poly_func(bin_centers, *params)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"polynomial to logistic","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import r2_score\n\n# Define a polynomial function with two input features (tenure, monthly charges)\ndef poly_func(X, a, b, c, d, e, f, g):\n    tenure, charges = X\n    return a * tenure**2 + b * tenure + c * charges**2 + d * charges + e * tenure * charges + f * tenure + g\n\n# Define the logistic function using polynomial output\ndef logistic_func(X, a, b, c, d, e, f, g):\n    return 1 / (1 + np.exp(-(poly_func(X, a, b, c, d, e, f, g))))\n\n# Sample bin centers for Tenure and Monthly Charges (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\nmonthly_charges_bins = np.array([20, 40, 60, 80, 100, 110, 120])  # 7 monthly charge bins\n\n# Churn=Yes and Churn=No counts (must match bin lengths)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Compute log-odds: log(Churn Yes / Churn No)\nlog_odds = np.log(churn_yes_counts / churn_no_counts)\n\n# Fit the polynomial to the log-odds data\nX_data = np.vstack((tenure_bins, monthly_charges_bins))  # Stacking both features as input\nparams, _ = curve_fit(poly_func, X_data, log_odds)\n\n# Generate fitted curve\ntenure_fit = np.linspace(0, 70, 100)\ncharges_fit = np.linspace(20, 120, 100)\nX_fit = np.meshgrid(tenure_fit, charges_fit)\ny_fit_log_odds = poly_func((X_fit[0], X_fit[1]), *params)\n\n# Convert to probability using logistic function\ny_fit_prob = logistic_func((X_fit[0], X_fit[1]), *params)\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(tenure_bins, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds.mean(axis=0), color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(tenure_bins, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob.mean(axis=0), color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability\")\nplt.legend()\n\nplt.show()\n\n# Print the final logistic equation\na, b, c, d, e, f, g = params\nprint(f\"Final Logistic Equation:\")\nprint(f\"P(Churn=Yes) = 1 / (1 + e^(-({a:.5f} * tenure² + {b:.5f} * tenure + {c:.5f} * charges² + {d:.5f} * charges + {e:.5f} * tenure * charges + {f:.5f} * tenure + {g:.5f})))\")\n\n# Compute R² Score for the polynomial fit\nlog_odds_pred = poly_func((tenure_bins, monthly_charges_bins), *params)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"polynomial to logistic multifeature","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\n\n# 🔹 Set the maximum polynomial degree (adjustable)\nmax_poly_degree = 2  # Set to 1 or adjust based on available data\n\n# Sample bin centers for Tenure and Monthly Charges (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\nmonthly_charges_bins = np.array([20, 40, 60, 80, 100, 110, 120])  # 7 monthly charge bins\n\n# Churn=Yes and Churn=No counts (must match bin lengths)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Compute log-odds: log(Churn Yes / Churn No)\nlog_odds = np.log(churn_yes_counts / churn_no_counts)\n\n# 🔹 Generate polynomial features\nX_data = np.vstack((tenure_bins, monthly_charges_bins)).T  # Shape (7,2)\npoly = PolynomialFeatures(degree=max_poly_degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)  # Expand features\n\n# 🔹 Ensure the number of parameters does not exceed available data points\nnum_params = X_poly.shape[1]  # Number of polynomial terms\nnum_data_points = len(log_odds)\n\nif num_params > num_data_points:\n    print(f\"⚠️ Too many parameters ({num_params}) for {num_data_points} data points! Using Ridge regression instead.\")\n    model = Ridge(alpha=0.1)  # Regularization\nelse:\n    model = np.linalg.lstsq(X_poly, log_odds, rcond=None)[0]  # Least squares fit\n\n# 🔹 Fit polynomial model\nif isinstance(model, Ridge):\n    model.fit(X_poly, log_odds)\n    params = model.coef_\nelse:\n    params = model\n\n# 🔹 Generate fitted curve\ntenure_fit = np.linspace(0, 70, 100)\ncharges_fit = np.linspace(20, 120, 100)\nX_fit = np.array(np.meshgrid(tenure_fit, charges_fit)).T.reshape(-1, 2)\nX_fit_poly = poly.transform(X_fit)\ny_fit_log_odds = X_fit_poly @ params  # Polynomial transformation\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# 🔹 Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(tenure_bins, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds.reshape(100, -1).mean(axis=1), color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn\")\nplt.legend()\n\n# 🔹 Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(tenure_bins, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob.reshape(100, -1).mean(axis=1), color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability\")\nplt.legend()\n\nplt.show()\n\n\n# Corrected equation formatting\nequation_terms = \" + \".join([\n    f\"{coef:.5f} * {feature.replace(' ', ' * ')}\"  # Ensure multiplication signs\n    for coef, feature in zip(params, poly.get_feature_names_out(['tenure', 'charges']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({equation_terms})))\")\n\n# 🔹 Compute R² Score\nlog_odds_pred = X_poly @ params\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Sample bin centers for Tenure and Monthly Charges (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\nmonthly_charges_bins = np.array([20, 40, 60, 80, 100, 110, 120])  # 7 monthly charge bins\n\n# Churn=Yes and Churn=No counts (must match bin lengths)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Apply natural logarithm (ln) to absolute values of churn counts\nln_churn_yes = np.log(np.abs(churn_yes_counts))\nln_churn_no = np.log(np.abs(churn_no_counts))\n\n# Compute log-odds using log-transformed churn values\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to absolute feature values\nln_tenure = np.log(np.abs(tenure_bins))\nln_charges = np.log(np.abs(monthly_charges_bins))\n\n# Stack transformed features\nX_data = np.vstack((ln_tenure, ln_charges)).T\n\n# Fit a polynomial model to the transformed features\ndegree = 2  # Set polynomial degree\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit a linear regression model to the polynomial-transformed features\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100)\ncharges_fit = np.linspace(ln_charges.min(), ln_charges.max(), 100)\nX_fit = np.array(np.meshgrid(tenure_fit, charges_fit)).T.reshape(-1, 2)\n\n# Transform fitted features\nX_fit_poly = poly.transform(X_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds[:100], color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn (Log-Transformed Features & Churn)\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob[:100], color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability (Log-Transformed Features & Churn)\")\nplt.legend()\n\nplt.show()\n\n# Print the final logistic equation with explicit multiplication signs\nequation_terms = \" + \".join([\n    f\"{coef:.5f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_[tenure]', 'ln_[charges]']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.5f} + {equation_terms})))\")\n\n# Compute R² Score for the polynomial fit\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"symbolic logistic multifeature","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\n\n# 🔹 Set the maximum polynomial degree (adjustable)\nmax_poly_degree = 2  # Adjust based on available data\n\n# Sample bin centers for Tenure (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\n\n# Churn=Yes and Churn=No counts\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Compute log-odds: log(Churn Yes / Churn No)\nlog_odds = np.log(churn_yes_counts / churn_no_counts).reshape(-1, 1)  # Reshape to (7,1)\n\n# 🔹 Generate polynomial features\nX_data = tenure_bins.reshape(-1, 1)  # Ensure (7,1) shape for sklearn\npoly = PolynomialFeatures(degree=max_poly_degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)  # Expand features\n\n# 🔹 Ensure the number of parameters does not exceed available data points\nnum_params = X_poly.shape[1]  # Number of polynomial terms\nnum_data_points = len(log_odds)\n\nif num_params > num_data_points:\n    print(f\"⚠️ Too many parameters ({num_params}) for {num_data_points} data points! Using Ridge regression instead.\")\n    model = Ridge(alpha=0.1)  # Regularization\n    model.fit(X_poly, log_odds.ravel())  # Fit Ridge model\n    params = model.coef_\nelse:\n    params = np.linalg.lstsq(X_poly, log_odds, rcond=None)[0]  # Least squares fit\n\n# 🔹 Generate fitted curve\ntenure_fit = np.linspace(0, 70, 100).reshape(-1, 1)  # Ensure shape (100,1)\nX_fit_poly = poly.transform(tenure_fit)\ny_fit_log_odds = X_fit_poly @ params  # Polynomial transformation\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# 🔹 Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(tenure_bins, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds, color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn\")\nplt.legend()\n\n# 🔹 Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(tenure_bins, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob, color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability\")\nplt.legend()\n\nplt.show()\n\n# 🔹 Generate final logistic equation\nequation_terms = \" + \".join([\n    f\"{coef:.5f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params.flatten(), poly.get_feature_names_out(['tenure']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({equation_terms})))\")\n\n# 🔹 Compute R² Score\nlog_odds_pred = X_poly @ params\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Sample bin centers for Tenure and Monthly Charges (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\n\n# Churn=Yes and Churn=No counts (must match bin lengths)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Apply natural logarithm (ln) to absolute values of churn counts\nln_churn_yes = np.log(np.abs(churn_yes_counts))\nln_churn_no = np.log(np.abs(churn_no_counts))\n\n# Compute log-odds using log-transformed churn values\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to absolute feature values\nln_tenure = np.log(np.abs(tenure_bins))\n\n# Stack transformed features\nX_data = ln_tenure.reshape(-1, 1)  # Ensure it's a 2D array\n\n# Fit a polynomial model to the transformed features\ndegree = 1  # Set polynomial degree\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit a linear regression model to the polynomial-transformed features\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100).reshape(-1, 1)\n\n# Transform fitted features\nX_fit_poly = poly.transform(tenure_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds, color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn (Log-Transformed Features & Churn)\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob, color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability (Log-Transformed Features & Churn)\")\nplt.legend()\n\nplt.show()\n\n# Print the final logistic equation with explicit multiplication signs\nequation_terms = \" + \".join([\n    f\"{coef:.5f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_tenure']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.5f} + {equation_terms})))\")\n\n# Compute R² Score for the polynomial fit\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"symbolic logistic single feature","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Small epsilon to avoid log(0)\nepsilon = 1e-2  \n\n# Sample bin centers for Tenure and Monthly Charges\ntenure_bins = np.array([10.1041075, 23.3849225, 36.6657375, 49.946552499999996, 63.2273675, 76.5081825, 89.7889975, \n                     103.06981249999998, 116.3506275, 129.6314425, 142.91225749999995, 156.19307249999997, 169.4738875, \n                     182.75470249999995, 196.03551749999997, 209.3163325, 222.59714749999995, 235.87796249999997, \n                     249.15877749999999, 262.4395925])\n\n# Churn=Yes and Churn=No counts\nchurn_yes_counts = np.array([592, 1870, 4286, 2632, 387, 112, 69, 26, 8, 6, 4, 2, 2, 1, 2, 0, 0, 0, 0, 1])\nchurn_no_counts = np.array([0, 1877, 5033, 2455, 421, 133, 45, 20, 8, 1, 3, 1, 0, 2, 0, 0, 0, 0, 1, 0])\n\n# Apply natural logarithm (ln) with epsilon to prevent log(0)\nln_churn_yes = np.log(churn_yes_counts + epsilon)\nln_churn_no = np.log(churn_no_counts + epsilon)\n\n# Compute log-odds\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to tenure bins\nln_tenure = np.log(tenure_bins + epsilon)\n\n# Stack transformed features\nX_data = ln_tenure.reshape(-1, 1)  # Ensure it's 2D for sklearn\n\n# Fit a polynomial model\ndegree = 301  # Adjust the degree (too high can overfit)\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100).reshape(-1, 1)\n\n# Transform fitted features\nX_fit_poly = poly.transform(tenure_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds, color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob, color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability\")\nplt.legend()\n\nplt.show()\n\n# Print logistic equation\nequation_terms = \" + \".join([\n    f\"{coef:.6f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_tenure']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.6f} + {equation_terms})))\")\n\n# Compute R² Score\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Sample bin centers for Tenure and Monthly Charges (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\nmonthly_charges_bins = np.array([20, 40, 60, 80, 100, 110, 120])  # 7 monthly charge bins\n\n# Churn=Yes and Churn=No counts (must match bin lengths)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Apply natural logarithm (ln) to absolute values of churn counts\nln_churn_yes = np.log(np.abs(churn_yes_counts) + epsilon)\nln_churn_no = np.log(np.abs(churn_no_counts) + epsilon)\n\n# Compute log-odds using log-transformed churn values\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to absolute feature values\nln_tenure = np.log(np.abs(tenure_bins) + epsilon)\nln_charges = np.log(np.abs(monthly_charges_bins) + epsilon)\n\n# Stack transformed features\nX_data = np.vstack((ln_tenure, ln_charges)).T\n\n# Fit a polynomial model to the transformed features\ndegree = 2  # Set polynomial degree\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit a linear regression model to the polynomial-transformed features\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100)\ncharges_fit = np.linspace(ln_charges.min(), ln_charges.max(), 100)\nX_fit = np.array(np.meshgrid(tenure_fit, charges_fit)).T.reshape(-1, 2)\n\n# Transform fitted features\nX_fit_poly = poly.transform(X_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds[:100], color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn (Log-Transformed Features & Churn)\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob[:100], color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability (Log-Transformed Features & Churn)\")\nplt.legend()\n\nplt.show()\n\n# Print the final logistic equation with explicit multiplication signs\nequation_terms = \" + \".join([\n    f\"{coef:.5f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_[tenure]', 'ln_[charges]']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.5f} + {equation_terms})))\")\n\n# Compute R² Score for the polynomial fit\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"symbolic logistic multifeature","metadata":{}},{"cell_type":"code","source":"#@title symbolic regression linear transformation-epsilon\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Take absolute values and add epsilon for safety\ny_positive = np.abs(y) + epsilon\nx1_positive = np.abs(x1) + epsilon\nx2_positive = np.abs(x2) + epsilon\nx3_positive = np.abs(x3) + epsilon\n\n# Logarithmic transformations with reshaping\nln_y = np.log(y_positive).reshape(-1, 1)  # Target variable, reshaped to 2D\nln_x1 = np.log(x1_positive).reshape(-1, 1)\nln_x2 = np.log(x2_positive).reshape(-1, 1)\nln_x3 = np.log(x3_positive).reshape(-1, 1)\n\n# Combine x1, x2, x3 into a multivariate feature matrix\nX_log = np.hstack((ln_x1, ln_x2, ln_x3))  # Combine features horizontally into a matrix\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this value for higher-degree nurturing\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_log)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y.ravel())  # Use .ravel() to flatten ln_y for regression\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Display the equation in transformed space\nfeature_names = poly.get_feature_names_out([\"ln(|x1|)\", \"ln(|x2|)\", \"ln(|x3|)\"])\nequation = f\"ln(y) = {intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\n\n# Print the results\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.5f}\")\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Given data\nx1 = np.array([30, 45, 60])\ny = np.array([0.5, 0.71, 0.867])\n\n# Take absolute values and add epsilon for safety\ny_positive = np.abs(y) + epsilon\nx1_positive = np.abs(x1) + epsilon\n\n# Logarithmic transformations\nln_y = np.log(y_positive)\nln_x1 = np.log(x1_positive)\n\n# Reshape ln_x1 into a 2D array (required for PolynomialFeatures)\nX_log = ln_x1.reshape(-1, 1)\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this value for higher-degree nurturing\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_log)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Display the equation in transformed space\nfeature_names = poly.get_feature_names_out([\"ln(|x1|)\"])\nequation = f\"ln(y) = {intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\n\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.5f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"symbolic reg linear transformation epsilon","metadata":{}},{"cell_type":"code","source":"#@title SYMBOLIC REGRESSION\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Load data from CSV\ndf = pd.read_csv(\"E:/AIRCRAFT/Aircraft Engines.csv\",encoding='latin1')  # Change \"data.csv\" to your actual filename\nfor column in df.select_dtypes(include=['object']):\n    df[column] = df[column].astype(str)\n\nlabel_encoder = LabelEncoder()\nfor column in df.select_dtypes(include=['object']):\n    df[column] = label_encoder.fit_transform(df[column])\n\n# Automatically detect feature columns (all except the last column)\n#feature_columns = df.columns[:-1]  # Assumes last column is the target (y)\n#target_column = df.columns[-1]  # Assumes last column is y\ntarget_column = \"power\"  # Set your target column explicitly\nfeature_columns = [col for col in df.columns if col != target_column] \n# Extract features (X) and target (y)\nX = df[feature_columns].values  # Feature matrix\ny = df[target_column].values  # Target variable\n\n# Take absolute values and add epsilon for stability\ny_positive = np.abs(y) + epsilon\nX_positive = np.abs(X) + epsilon  # Apply to all feature columns\n\n# Logarithmic transformations\nln_y = np.log(y_positive).reshape(-1, 1)  # Log-transformed target\nln_X = np.log(X_positive)  # Log-transformed features\n\n# Create polynomial features (degree can be adjusted)\ndegree = 7  # Adjust this value for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(ln_X)\n\n# Perform multivariate linear regression on transformed polynomial data\nmodel = LinearRegression()\nmodel.fit(X_poly, ln_y.ravel())  # Use .ravel() to flatten ln_y\n\n# Extract coefficients and intercept\ncoefficients = model.coef_\nintercept = model.intercept_\n\n# Predict ln(y) values using the model\nln_y_pred = model.predict(X_poly)\n\n# Calculate R-squared score\nr2 = r2_score(ln_y, ln_y_pred)\n\n# Generate equation dynamically based on input features\nfeature_names = poly.get_feature_names_out([f\"ln(|{col}|)\" for col in feature_columns])\nequation = f\"ln(y) = {intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\n\n# Print the results\nprint(\"Equation in transformed space (log-linearized with polynomial features):\")\nprint(equation)\nprint(f\"R-squared (transformed space): {r2:.5f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SYMBOLIC REG-CSV","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\n\n# Enable inline plotting for Spyder\n\n\n# Define the file path\nfile_path = os.path.join(\"E:/LHC-CERN/Z_boson.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(file_path)\n\n# Print basic information\nprint(df)\nprint(df.dtypes)\nprint(df.shape)\nprint(df[df['class'] == 'Zee'])\nprint(df[df['class'] == 'Zmumu'])\n\n# Extract pt1 values for both classes\npt1_class_zmumu = df[df['class'] == 'Zmumu']['pt1']\npt1_class_zee = df[df['class'] == 'Zee']['pt1']\n\n# Plot settings\nplt.figure(figsize=(8,6))  # Set figure size\nplt.xlabel(\"pt1\")\nplt.ylabel(\"Number Of Class\")\nplt.title(\"Class Prediction Visualization\")\n\n# Histogram\nplt.hist([pt1_class_zmumu, pt1_class_zee], bins=20, rwidth=0.95, color=['green','red'], label=['Class=Zmumu', 'Class=Zee'])\n\n# Add legend and show the plot\nplt.legend()\nplt.show()\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Define the file path\nfile_path = os.path.join(\"E:/LHC-CERN/Z_boson.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(file_path)\n\n# Extract pt1 values for both classes\npt1_class_zmumu = df[df['class'] == 'Zmumu']['pt1']\npt1_class_zee = df[df['class'] == 'Zee']['pt1']\n\n# Plot settings\nplt.figure(figsize=(8, 6))  # Set figure size\nplt.xlabel(\"pt1\")\nplt.ylabel(\"Number of Class\")\nplt.title(\"Class Prediction Visualization\")\n\n# Histogram with returned values\nn_zmumu, bins, _ = plt.hist(pt1_class_zmumu, bins=20, rwidth=0.95, color='green', alpha=0.7, label='Class=Zmumu', density=False)\nn_zee, _, _ = plt.hist(pt1_class_zee, bins=bins, rwidth=0.95, color='red', alpha=0.7, label='Class=Zee', density=False)\n\n# Add legend and show the plot\nplt.legend()\nplt.show()\n\n# Compute bin centers\nbin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of bins\n\n# Create a DataFrame with all bins\ndata = pd.DataFrame({\n    'pt1_bin_center': bin_centers, \n    'Zmumu_counts': n_zmumu, \n    'Zee_counts': n_zee\n})\n\n# **Remove rows where either class count is zero**\nfiltered_data = data[(data['Zmumu_counts'] > 0) & (data['Zee_counts'] > 0)]\n\n# Print the filtered lists horizontally\nprint(\"\\nFiltered pt1_bin_center:\", list(filtered_data['pt1_bin_center']))\nprint(\"Filtered Zmumu_counts:   \", list(filtered_data['Zmumu_counts']))\nprint(\"Filtered Zee_counts:     \", list(filtered_data['Zee_counts']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Logistic curve value extraction","metadata":{}},{"cell_type":"code","source":"#@title MainLogistic\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\n\n# Enable inline plotting for Spyder\n\n\n# Define the file path\nfile_path = os.path.join(\"E:/LHC-CERN/Z_boson.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(file_path)\n\n# Print basic information\nprint(df)\nprint(df.dtypes)\nprint(df.shape)\nprint(df[df['class'] == 'Zee'])\nprint(df[df['class'] == 'Zmumu'])\n\n# Extract pt1 values for both classes\npt1_class_zmumu = df[df['class'] == 'Zmumu']['pt1']\npt1_class_zee = df[df['class'] == 'Zee']['pt1']\n\n# Plot settings\nplt.figure(figsize=(8,6))  # Set figure size\nplt.xlabel(\"pt1\")\nplt.ylabel(\"Number Of Class\")\nplt.title(\"Class Prediction Visualization\")\n\n# Histogram\nplt.hist([pt1_class_zmumu, pt1_class_zee], bins=20, rwidth=0.95, color=['green','red'], label=['Class=Zmumu', 'Class=Zee'])\n\n# Add legend and show the plot\nplt.legend()\nplt.show()\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Define the file path\nfile_path = os.path.join(\"E:/LHC-CERN/Z_boson.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(file_path)\n\n# Extract pt1 values for both classes\npt1_class_zmumu = df[df['class'] == 'Zmumu']['pt1']\npt1_class_zee = df[df['class'] == 'Zee']['pt1']\n\n# Plot settings\nplt.figure(figsize=(8, 6))  # Set figure size\nplt.xlabel(\"pt1\")\nplt.ylabel(\"Number of Class\")\nplt.title(\"Class Prediction Visualization\")\n\n# Histogram with returned values\nn_zmumu, bins, _ = plt.hist(pt1_class_zmumu, bins=20, rwidth=0.95, color='green', alpha=0.7, label='Class=Zmumu', density=False)\nn_zee, _, _ = plt.hist(pt1_class_zee, bins=bins, rwidth=0.95, color='red', alpha=0.7, label='Class=Zee', density=False)\n\n# Add legend and show the plot\nplt.legend()\nplt.show()\n\n# Compute bin centers\nbin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of bins\n\n# Create a DataFrame with all bins\ndata = pd.DataFrame({\n    'pt1_bin_center': bin_centers, \n    'Zmumu_counts': n_zmumu, \n    'Zee_counts': n_zee\n})\n\n# **Remove rows where either class count is zero**\nfiltered_data = data[(data['Zmumu_counts'] > 0) & (data['Zee_counts'] > 0)]\n\n# Print the filtered lists horizontally\nprint(\"\\nFiltered pt1_bin_center:\", list(filtered_data['pt1_bin_center']))\nprint(\"Filtered Zmumu_counts:   \", list(filtered_data['Zmumu_counts']))\nprint(\"Filtered Zee_counts:     \", list(filtered_data['Zee_counts']))\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Small epsilon to avoid log(0)\nepsilon = 1e-2  \n\n# Sample bin centers for Tenure and Monthly Charges\ntenure_bins = np.array([10.1041075, 23.3849225, 36.6657375, 49.946552499999996, 63.2273675, 76.5081825, 89.7889975, \n                     103.06981249999998, 116.3506275, 129.6314425, 142.91225749999995, 156.19307249999997, 169.4738875, \n                     182.75470249999995, 196.03551749999997, 209.3163325, 222.59714749999995, 235.87796249999997, \n                     249.15877749999999, 262.4395925])\n\n# Churn=Yes and Churn=No counts\nchurn_yes_counts = np.array([592, 1870, 4286, 2632, 387, 112, 69, 26, 8, 6, 4, 2, 2, 1, 2, 0, 0, 0, 0, 1])\nchurn_no_counts = np.array([0, 1877, 5033, 2455, 421, 133, 45, 20, 8, 1, 3, 1, 0, 2, 0, 0, 0, 0, 1, 0])\n\n# Apply natural logarithm (ln) with epsilon to prevent log(0)\nln_churn_yes = np.log(churn_yes_counts + epsilon)\nln_churn_no = np.log(churn_no_counts + epsilon)\n\n# Compute log-odds\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to tenure bins\nln_tenure = np.log(tenure_bins + epsilon)\n\n# Stack transformed features\nX_data = ln_tenure.reshape(-1, 1)  # Ensure it's 2D for sklearn\n\n# Fit a polynomial model\ndegree = 301  # Adjust the degree (too high can overfit)\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100).reshape(-1, 1)\n\n# Transform fitted features\nX_fit_poly = poly.transform(tenure_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds, color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob, color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability\")\nplt.legend()\n\nplt.show()\n\n# Print logistic equation\nequation_terms = \" + \".join([\n    f\"{coef:.6f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_tenure']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.6f} + {equation_terms})))\")\n\n# Compute R² Score\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Sample bin centers for Tenure and Monthly Charges (from image)\ntenure_bins = np.array([5, 15, 25, 35, 45, 55, 65])  # 7 tenure bins\nmonthly_charges_bins = np.array([20, 40, 60, 80, 100, 110, 120])  # 7 monthly charge bins\n\n# Churn=Yes and Churn=No counts (must match bin lengths)\nchurn_yes_counts = np.array([900, 250, 150, 120, 100, 90, 80])  # 7 values\nchurn_no_counts = np.array([850, 450, 400, 380, 350, 400, 450])  # 7 values\n\n# Apply natural logarithm (ln) to absolute values of churn counts\nln_churn_yes = np.log(np.abs(churn_yes_counts) + epsilon)\nln_churn_no = np.log(np.abs(churn_no_counts) + epsilon)\n\n# Compute log-odds using log-transformed churn values\nlog_odds = ln_churn_yes - ln_churn_no\n\n# Apply natural logarithm to absolute feature values\nln_tenure = np.log(np.abs(tenure_bins) + epsilon)\nln_charges = np.log(np.abs(monthly_charges_bins) + epsilon)\n\n# Stack transformed features\nX_data = np.vstack((ln_tenure, ln_charges)).T\n\n# Fit a polynomial model to the transformed features\ndegree = 2  # Set polynomial degree\npoly = PolynomialFeatures(degree, include_bias=False)\nX_poly = poly.fit_transform(X_data)\n\n# Fit a linear regression model to the polynomial-transformed features\nmodel = LinearRegression()\nmodel.fit(X_poly, log_odds)\n\n# Get polynomial coefficients\nparams = model.coef_\nintercept = model.intercept_\n\n# Generate fitted curve\ntenure_fit = np.linspace(ln_tenure.min(), ln_tenure.max(), 100)\ncharges_fit = np.linspace(ln_charges.min(), ln_charges.max(), 100)\nX_fit = np.array(np.meshgrid(tenure_fit, charges_fit)).T.reshape(-1, 2)\n\n# Transform fitted features\nX_fit_poly = poly.transform(X_fit)\ny_fit_log_odds = model.predict(X_fit_poly)\n\n# Convert to probability using logistic function\ny_fit_prob = 1 / (1 + np.exp(-y_fit_log_odds))\n\n# Plot log-odds fit\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(ln_tenure, log_odds, color='blue', label=\"Log-Odds (Actual)\")\nplt.plot(tenure_fit, y_fit_log_odds[:100], color='black', linestyle=\"dashed\", label=\"Log-Odds (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Log-Odds of Churn\")\nplt.title(\"Polynomial Fit to Log-Odds of Churn (Log-Transformed Features & Churn)\")\nplt.legend()\n\n# Plot logistic probability fit\nplt.subplot(1, 2, 2)\nplt.scatter(ln_tenure, churn_yes_counts / (churn_yes_counts + churn_no_counts), color='green', label=\"Churn Probability (Actual)\")\nplt.plot(tenure_fit, y_fit_prob[:100], color='red', linestyle=\"dashed\", label=\"Churn Probability (Fitted)\")\nplt.xlabel(\"ln(Tenure)\")\nplt.ylabel(\"Probability of Churn=Yes\")\nplt.title(\"Logistic Curve Fit for Churn Probability (Log-Transformed Features & Churn)\")\nplt.legend()\n\nplt.show()\n\n# Print the final logistic equation with explicit multiplication signs\nequation_terms = \" + \".join([\n    f\"{coef:.5f} * {feature.replace(' ', ' * ')}\"\n    for coef, feature in zip(params, poly.get_feature_names_out(['ln_[tenure]', 'ln_[charges]']))\n])\nprint(f\"\\nFinal Logistic Equation:\\nP(Churn=Yes) = 1 / (1 + e^(-({intercept:.5f} + {equation_terms})))\")\n\n# Compute R² Score for the polynomial fit\nlog_odds_pred = model.predict(X_poly)\nr2 = r2_score(log_odds, log_odds_pred)\nprint(f\"R² Score for Polynomial Fit: {r2:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Main logistic","metadata":{}},{"cell_type":"code","source":"#@title Non-linear-logistic-main\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Small epsilon to prevent numerical instability\nepsilon = 1e-10\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Ensure non-negative values by taking absolute values and adding epsilon\nx1_transformed = np.abs(x1) + epsilon\nx2_transformed = np.abs(x2) + epsilon\nx3_transformed = np.abs(x3) + epsilon\n\n# Convert y into binary classification (Threshold: Choose based on the problem)\ny_binary = (y < -5000).astype(int)  # Example: Classify as 1 if y < -5000, else 0\n\n# Combine features into a single matrix\nX = np.column_stack((x1_transformed, x2_transformed, x3_transformed))\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Print feature names for debugging\nfeature_names = poly.get_feature_names_out([\"x1\", \"x2\", \"x3\"])\nprint(\"Generated polynomial features:\", feature_names)\n\n# Perform logistic regression\nmodel = LogisticRegression(max_iter=1000)  # Increase iterations to ensure convergence\nmodel.fit(X_poly, y_binary)  # Fit the logistic model\n\n# Predict class labels\ny_pred = model.predict(X_poly)\n\n# Compute accuracy\naccuracy = accuracy_score(y_binary, y_pred)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Display the equation\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name.replace(' ', ' * ')}\"\nequation += \")))\"\n\n# Print results\nprint(\"\\nLogistic Regression Equation (with polynomial features):\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy:.5f}\")\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import accuracy_score\n\n# Small epsilon to prevent numerical instability\nepsilon = 1e-10\n\n# Given data\nx1 = np.array([30, 45, 60])\ny = np.array([0.5, 0.71, 0.867])\n\n# Convert y into binary classification (Threshold: Choose based on the problem)\ny_binary = (y > 0.7).astype(int)  # Classify as 1 if y > 0.7, else 0\n\n# Take absolute values and add epsilon for safety\nx1_transformed = np.abs(x1) + epsilon\n\n# Reshape x1 into a 2D array (required for PolynomialFeatures)\nX = x1_transformed.reshape(-1, 1)\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Perform logistic regression\nmodel = LogisticRegression(max_iter=1000)  # Increased iterations for stability\nmodel.fit(X_poly, y_binary)\n\n# Predict class labels\ny_pred = model.predict(X_poly)\n\n# Compute accuracy\naccuracy = accuracy_score(y_binary, y_pred)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Display the logistic regression equation\nfeature_names = poly.get_feature_names_out([\"x1\"])\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\nequation += \")))\"\n\n# Print results\nprint(\"\\nLogistic Regression Equation (with polynomial features):\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy:.5f}\")\n\n\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Apply ln transformation to features\nx1_ln = np.log(np.abs(x1) + epsilon)\nx2_ln = np.log(np.abs(x2) + epsilon)\nx3_ln = np.log(np.abs(x3) + epsilon)\n\n# Combine transformed features into a single matrix\nX_ln = np.column_stack((x1_ln, x2_ln, x3_ln))\n\n# Create polynomial features\ndegree = 2  \npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_ln)\n\n# Correct feature name formatting (replace spaces with *)\nfeature_names = poly.get_feature_names_out([\"ln(x1)\", \"ln(x2)\", \"ln(x3)\"])\nfeature_names = [name.replace(\" \", \" * \") for name in feature_names]  \n\n# Train logistic regression\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_poly, (y < -5000).astype(int))  # Binary classification\n\n# Get model coefficients\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Construct the equation properly\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\nequation += \")))\"\n\n# Print results\nprint(\"\\nCorrected Logistic Regression Equation:\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy_score((y < -5000).astype(int), model.predict(X_poly)):.5f}\")\n\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import accuracy_score\n\n# Small epsilon to prevent log(0)\nepsilon = 1e-10\n\n# Given data\nx1 = np.array([30, 45, 60])\ny = np.array([0.5, 0.71, 0.867])\n\n# Convert y into binary classification (Threshold: Choose based on the problem)\ny_binary = (y > 0.7).astype(int)  # Classify as 1 if y > 0.7, else 0\n\n# Apply ln transformation to x1 (ensure non-negative input)\nx1_ln = np.log(np.abs(x1) + epsilon)\n\n# Reshape x1_ln into a 2D array (required for PolynomialFeatures)\nX_ln = x1_ln.reshape(-1, 1)\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X_ln)\n\n# Perform logistic regression\nmodel = LogisticRegression(max_iter=1000)  # Increased iterations for stability\nmodel.fit(X_poly, y_binary)\n\n# Predict class labels\ny_pred = model.predict(X_poly)\n\n# Compute accuracy\naccuracy = accuracy_score(y_binary, y_pred)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Display the logistic regression equation\nfeature_names = poly.get_feature_names_out([\"ln(x1)\"])\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\nequation += \")))\"\n\n# Print results\nprint(\"\\nLogistic Regression Equation (with ln-transformed polynomial features):\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy:.5f}\")\n\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Given data\nx1 = np.array([1, 2, 3, 4])\nx2 = np.array([1, 2, 3, 4])\nx3 = np.array([1, 3, 2, 4])\ny = np.array([-9, -8836, -1079, -67776])\n\n# Convert y into binary classification (Threshold: Choose based on the problem)\ny_binary = (y < -5000).astype(int)  # Example: Classify as 1 if y < -5000, else 0\n\n# Combine features into a single matrix (no ln transformation)\nX = np.column_stack((x1, x2, x3))\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Print feature names for debugging\nfeature_names = poly.get_feature_names_out([\"x1\", \"x2\", \"x3\"])\nfeature_names = [name.replace(\" \", \" * \") for name in feature_names]  # Fix formatting\n\n# Perform logistic regression\nmodel = LogisticRegression(max_iter=1000)  # Increase iterations to ensure convergence\nmodel.fit(X_poly, y_binary)  # Fit the logistic model\n\n# Predict class labels\ny_pred = model.predict(X_poly)\n\n# Compute accuracy\naccuracy = accuracy_score(y_binary, y_pred)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Display the equation\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name}\"\nequation += \")))\"\n\n# Print results\nprint(\"\\nLogistic Regression Equation (with polynomial features):\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy:.5f}\")\n\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import accuracy_score\n\n# Given data\nx1 = np.array([30, 45, 60])\ny = np.array([0.5, 0.71, 0.867])\n\n# Convert y into binary classification (Threshold: Choose based on the problem)\ny_binary = (y > 0.7).astype(int)  # Classify as 1 if y > 0.7, else 0\n\n# Reshape x1 into a 2D array (required for PolynomialFeatures)\nX = x1.reshape(-1, 1)  # Removed log transformation\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust this for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Perform logistic regression\nmodel = LogisticRegression(max_iter=1000)  # Increased iterations for stability\nmodel.fit(X_poly, y_binary)\n\n# Predict class labels\ny_pred = model.predict(X_poly)\n\n# Compute accuracy\naccuracy = accuracy_score(y_binary, y_pred)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Display the logistic regression equation\nfeature_names = poly.get_feature_names_out([\"x1\"])  # Removed ln(x1)\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.5f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.5f} * {name.replace(' ', ' * ')}\"  # Ensure correct formatting\nequation += \")))\"\n\n# Print results\nprint(\"\\nLogistic Regression Equation (with polynomial features):\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy:.5f}\")\n\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Small epsilon to prevent numerical instability\nepsilon = 1e-10\n#phi2_bins = np.array([-3.0, -3.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -0.0, -0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0])\n#Q2_bins = np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n#class_bins = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n# Given data\nx1 = np.array([-3.0, -3.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -0.0, -0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0])\nx2 = np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n\ny = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n\n# Ensure non-negative values by taking absolute values and adding epsilon\n#x1_transformed = np.abs(x1) + epsilon\n#x2_transformed = np.abs(x2) + epsilon\n\n# Convert y into binary classification (Threshold: Choose based on the problem)\n#y_binary = (y < -5000).astype(int)  # Example: Classify as 1 if y < -5000, else 0\ny_binary=(y==1)\n# Combine features into a single matrix\n\nX = np.column_stack((x1, x2))\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Print feature names for debugging\nfeature_names = poly.get_feature_names_out([\"x1\", \"x2\"])\nprint(\"Generated polynomial features:\", feature_names)\n\n# Perform logistic regression\nmodel = LogisticRegression(max_iter=1000)  # Increase iterations to ensure convergence\nmodel.fit(X_poly, y_binary)  # Fit the logistic model\n\n# Predict class labels\ny_pred = model.predict(X_poly)\n\n# Compute accuracy\naccuracy = accuracy_score(y_binary, y_pred)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Display the equation\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.15f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.15f} * {name.replace(' ', ' * ')}\"\nequation += \")))\"\n\n# Print results\nprint(\"\\nLogistic Regression Equation (with polynomial features):\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy:.5f}\")\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Small epsilon to prevent numerical instability\nepsilon = 1e-10\n#phi2_bins = np.array([-3.0, -3.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -0.0, -0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0])\n#Q2_bins = np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n#class_bins = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n# Given data\nx1 = np.array([-3.0, -3.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -0.0, -0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0])\n#x2 = np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n\ny = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n\n# Ensure non-negative values by taking absolute values and adding epsilon\n#x1_transformed = np.abs(x1) + epsilon\n#x2_transformed = np.abs(x2) + epsilon\n\n# Convert y into binary classification (Threshold: Choose based on the problem)\n#y_binary = (y < -5000).astype(int)  # Example: Classify as 1 if y < -5000, else 0\ny_binary=(y==1)\n# Combine features into a single matrix\n\n#X = np.column_stack((x1))\nX = x1.reshape(-1, 1)\n\n# Create polynomial features (degree can be adjusted)\ndegree = 2  # Adjust for higher-degree polynomial regression\npoly = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Print feature names for debugging\nfeature_names = poly.get_feature_names_out([\"x1\"])\nprint(\"Generated polynomial features:\", feature_names)\n\n# Perform logistic regression\nmodel = LogisticRegression(max_iter=1000)  # Increase iterations to ensure convergence\nmodel.fit(X_poly, y_binary)  # Fit the logistic model\n\n# Predict class labels\ny_pred = model.predict(X_poly)\n\n# Compute accuracy\naccuracy = accuracy_score(y_binary, y_pred)\n\n# Extract coefficients and intercept\ncoefficients = model.coef_[0]\nintercept = model.intercept_[0]\n\n# Display the equation\nequation = f\"sigmoid(y) = 1 / (1 + exp(-({intercept:.15f}\"\nfor coef, name in zip(coefficients, feature_names):\n    equation += f\" + {coef:.15f} * {name.replace(' ', ' * ')}\"\nequation += \")))\"\n\n# Print results\nprint(\"\\nLogistic Regression Equation (with polynomial features):\")\nprint(equation)\nprint(f\"Model Accuracy: {accuracy:.5f}\")\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\ndata = pd.read_csv('E:/LHC-CERN/Z_boson.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"class\", axis=1))\n\n# Combine features and target into a DataFrame for plotting\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable\ndf_plot['class'] = data['class']\n\n# Select a subset of features for binning\nselected_features = df_plot.columns  # Use all features\n\n# Number of bins (adjust as needed)\nnum_bins = 7\n\n# Store bins for each feature\nfeature_bins = {}\n\nfor feature in selected_features:\n    min_val, max_val = df_plot[feature].min(), df_plot[feature].max()\n    \n    # Create bins evenly spaced between min and max\n    bins = np.linspace(min_val, max_val, num_bins)\n    \n    feature_bins[feature] = np.round(bins)  # Round for cleaner output\n\n    # Plot histogram with bin markers\n    plt.figure(figsize=(8, 5))\n    sns.histplot(df_plot[feature], bins=num_bins, kde=True, color=\"blue\", alpha=0.6)\n    \n    # Overlay bin edges as vertical lines\n    for bin_edge in bins:\n        plt.axvline(bin_edge, color='red', linestyle='dashed', alpha=0.7)\n    \n    plt.title(f\"Histogram of {feature} with Bins\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.grid(True)\n    plt.show()\n\n# Print extracted bins in requested format\nfor feature, bins in feature_bins.items():\n    print(f\"{feature}_bins = np.array({bins.tolist()})\")  # Convert to np.array format\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\ndata = pd.read_csv('E:/LHC-CERN/Z_boson.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"class\", axis=1))\n\n# Combine features and target into a DataFrame for plotting\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable\ndf_plot['class'] = data['class']\n\n# Function to calculate Freedman-Diaconis bin width\ndef freedman_diaconis_bins(data):\n    q25, q75 = np.percentile(data, [25, 75])\n    iqr = q75 - q25  # Interquartile range\n    bin_width = 2 * iqr / (len(data) ** (1 / 3))  # Freedman-Diaconis rule\n    if bin_width == 0:  # Prevent division by zero\n        return 5  # Default minimum bin count\n    return max(5, int((data.max() - data.min()) / bin_width))  # At least 5 bins\n\n# Determine max bin count across all features\nnum_bins = max(freedman_diaconis_bins(df_plot[col]) for col in df_plot.columns if col != \"class\")\n\n# Store uniform bins for each feature\nfeature_bins = {}\n\n# Apply consistent binning for each feature\nfor feature in df_plot.columns:\n    if feature == \"class\":\n        continue  # Skip the target variable\n\n    min_val, max_val = df_plot[feature].min(), df_plot[feature].max()\n    bins = np.linspace(min_val, max_val, num_bins)  # Use same bin count for all features\n    feature_bins[feature] = np.round(bins, decimals=4)  # Round for cleaner output\n\n    # Plot histogram with uniform bin markers\n    plt.figure(figsize=(8, 5))\n    sns.histplot(df_plot[feature], bins=bins, kde=True, color=\"blue\", alpha=0.6)\n    \n    # Overlay bin edges as vertical lines\n    for bin_edge in bins:\n        plt.axvline(bin_edge, color='red', linestyle='dashed', alpha=0.7)\n    \n    plt.title(f\"Histogram of {feature} with Uniform Bins ({num_bins} bins)\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.grid(True)\n    plt.show()\n\n# Print extracted bins in requested format\nfor feature, bins in feature_bins.items():\n    print(f\"{feature}_bins = np.array({bins.tolist()})\")  # Convert to np.array format\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\ndata = pd.read_csv('E:/LHC-CERN/Z_boson.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"class\", axis=1))\n\n# Combine features and target into a DataFrame for plotting\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable\ndf_plot['class'] = data['class']\n\n# Function to calculate Freedman-Diaconis bin width\ndef freedman_diaconis_bins(data):\n    q25, q75 = np.percentile(data, [25, 75])\n    iqr = q75 - q25  # Interquartile range\n    bin_width = 2 * iqr / (len(data) ** (1 / 3))  # Freedman-Diaconis rule\n    if bin_width == 0:  # Prevent division by zero\n        return 5  # Default minimum bin count\n    return max(5, int((data.max() - data.min()) / bin_width))  # At least 5 bins\n\n# Determine max bin count across all features (including class)\nnum_bins = max(freedman_diaconis_bins(df_plot[col]) for col in df_plot.columns)\n\n# Store uniform bins for each feature\nfeature_bins = {}\n\n# Apply consistent binning for each feature\nfor feature in df_plot.columns:\n    min_val, max_val = df_plot[feature].min(), df_plot[feature].max()\n    bins = np.linspace(min_val, max_val, num_bins)  # Use same bin count for all features\n    feature_bins[feature] = np.round(bins, decimals=4)  # Round for cleaner output\n\n    # Plot histogram with uniform bin markers\n    plt.figure(figsize=(8, 5))\n    sns.histplot(df_plot[feature], bins=bins, kde=True, color=\"blue\", alpha=0.6)\n    \n    # Overlay bin edges as vertical lines\n    for bin_edge in bins:\n        plt.axvline(bin_edge, color='red', linestyle='dashed', alpha=0.7)\n    \n    plt.title(f\"Histogram of {feature} with Uniform Bins ({num_bins} bins)\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.grid(True)\n    plt.show()\n\n# Print extracted bins in NumPy array format\nfor feature, bins in feature_bins.items():\n    print(f\"{feature}_bins = np.array({bins.tolist()})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Non linear logistic main","metadata":{}},{"cell_type":"code","source":"#@title Curve+value extraction\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\ndata = pd.read_csv('E:/LHC-CERN/Z_boson.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"class\", axis=1))\n\n# Combine features and target into a DataFrame for plotting\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable\ndf_plot['class'] = data['class']\n\n# Select a subset of features for binning\nselected_features = df_plot.columns  # Use all features\n\n# Number of bins (adjust as needed)\nnum_bins = 7\n\n# Store bins for each feature\nfeature_bins = {}\n\nfor feature in selected_features:\n    min_val, max_val = df_plot[feature].min(), df_plot[feature].max()\n    \n    # Create bins evenly spaced between min and max\n    bins = np.linspace(min_val, max_val, num_bins)\n    \n    feature_bins[feature] = np.round(bins)  # Round for cleaner output\n\n    # Plot histogram with bin markers\n    plt.figure(figsize=(8, 5))\n    sns.histplot(df_plot[feature], bins=num_bins, kde=True, color=\"blue\", alpha=0.6)\n    \n    # Overlay bin edges as vertical lines\n    for bin_edge in bins:\n        plt.axvline(bin_edge, color='red', linestyle='dashed', alpha=0.7)\n    \n    plt.title(f\"Histogram of {feature} with Bins\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.grid(True)\n    plt.show()\n\n# Print extracted bins in requested format\nfor feature, bins in feature_bins.items():\n    print(f\"{feature}_bins = np.array({bins.tolist()})\")  # Convert to np.array format\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\ndata = pd.read_csv('E:/LHC-CERN/Z_boson.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"class\", axis=1))\n\n# Combine features and target into a DataFrame for plotting\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable\ndf_plot['class'] = data['class']\n\n# Function to calculate Freedman-Diaconis bin width\ndef freedman_diaconis_bins(data):\n    q25, q75 = np.percentile(data, [25, 75])\n    iqr = q75 - q25  # Interquartile range\n    bin_width = 2 * iqr / (len(data) ** (1 / 3))  # Freedman-Diaconis rule\n    if bin_width == 0:  # Prevent division by zero\n        return 5  # Default minimum bin count\n    return max(5, int((data.max() - data.min()) / bin_width))  # At least 5 bins\n\n# Determine max bin count across all features\nnum_bins = max(freedman_diaconis_bins(df_plot[col]) for col in df_plot.columns if col != \"class\")\n\n# Store uniform bins for each feature\nfeature_bins = {}\n\n# Apply consistent binning for each feature\nfor feature in df_plot.columns:\n    if feature == \"class\":\n        continue  # Skip the target variable\n\n    min_val, max_val = df_plot[feature].min(), df_plot[feature].max()\n    bins = np.linspace(min_val, max_val, num_bins)  # Use same bin count for all features\n    feature_bins[feature] = np.round(bins, decimals=4)  # Round for cleaner output\n\n    # Plot histogram with uniform bin markers\n    plt.figure(figsize=(8, 5))\n    sns.histplot(df_plot[feature], bins=bins, kde=True, color=\"blue\", alpha=0.6)\n    \n    # Overlay bin edges as vertical lines\n    for bin_edge in bins:\n        plt.axvline(bin_edge, color='red', linestyle='dashed', alpha=0.7)\n    \n    plt.title(f\"Histogram of {feature} with Uniform Bins ({num_bins} bins)\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.grid(True)\n    plt.show()\n\n# Print extracted bins in requested format\nfor feature, bins in feature_bins.items():\n    print(f\"{feature}_bins = np.array({bins.tolist()})\")  # Convert to np.array format\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"curve+value extraction","metadata":{}},{"cell_type":"code","source":"# @title Sea-sns-curve\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Load the dataset\ndata = pd.read_csv('E:/LHC-CERN/Z_boson.csv', encoding='latin1')\n\n# Convert categorical columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables if any\ncolumns_to_encode = []\nencoder = LabelEncoder()\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Handle missing values\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"class\", axis=1))\n\n# Convert back to DataFrame\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])\ndf_plot['class'] = data['class']\n\n# Get all features\nselected_features = df_plot.columns[:-1]  # Exclude 'class'\n\n# Downsample dataset for visualization (keep original data for modeling)\ndf_sample = df_plot.sample(2000, random_state=42)  # Adjust sample size for speed\n\n# Plot each feature separately on a new page\nfor feature in selected_features:\n    plt.figure(figsize=(8, 6))  # Create a new figure for each feature\n    \n    # Scatter plot of feature vs. class\n    sns.scatterplot(x=df_sample[feature], y=df_sample[\"class\"], alpha=0.5)\n    \n    plt.xlabel(feature)\n    plt.ylabel(\"Class\")\n    plt.title(f\"{feature} vs Class\")\n    \n    plt.show()  # Show one figure at a time (separate page)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = pd.read_csv('E:/LHC-CERN/Z_boson.csv', encoding='latin1')\n\n# Convert categorical columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables if any\ncolumns_to_encode = []\nencoder = LabelEncoder()\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Handle missing values\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"class\", axis=1))\n\n# Convert back to DataFrame\ndf = pd.DataFrame(features_imputed, columns=data.columns[:-1])\ndf['class'] = data['class']\n\n# Get all features\nselected_features = df.columns[:-1]  # Exclude 'class'\n\n# Train a separate logistic regression model for each feature and show each plot separately\nfor feature in selected_features:\n    plt.figure(figsize=(8, 6))  # Create a new figure for each feature\n    \n    # Extract single feature\n    X = df[[feature]]  # Keep it as DataFrame\n    y = df[\"class\"]\n    \n    # Train Logistic Regression Model\n    model = LogisticRegression()\n    model.fit(X, y)\n    \n    # Generate predictions (probabilities)\n    X_test = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)  # Test data for smooth curve\n    y_prob = model.predict_proba(X_test)[:, 1]  # Get probability for class 1\n    \n    # Scatter plot of actual data\n    sns.scatterplot(x=X.squeeze(), y=y, alpha=0.5, label=\"Actual Data\")\n    \n    # Plot predicted probability curve\n    plt.plot(X_test, y_prob, color='red', linestyle=\"dashed\", label=\"Predicted Probability\")\n    \n    plt.xlabel(feature)\n    plt.ylabel(\"Class\")\n    plt.title(f\"Prediction for {feature}\")\n    plt.legend()\n    \n    plt.show()  # Show one figure at a time (separate page)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"sea-sns-curve","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Read the CSV file\nFILENAME='/kaggle/input/aircrashes-and-fatalities/air_crashes and fatalities-3.csv'\ndata = pd.read_csv(FILENAME, encoding='latin1')\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Track encoded columns and their mappings\nencoded_mappings = {}\n\n# Encode categorical variables\nfor column in data.select_dtypes(include=['object']):\n    # Display the unique values before encoding\n    print(f\"Original values in '{column}': {data[column].unique()}\")\n    \n    # Fit the encoder and transform the column\n    data[column] = label_encoder.fit_transform(data[column].astype(str))\n    \n    # Store the mapping of original values to encoded labels\n    encoded_mappings[column] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n    \n    # Display the mapping and the encoded values\n    print(f\"Mapping for '{column}': {encoded_mappings[column]}\")\n    print(f\"Encoded values in '{column}': {data[column].unique()}\\n\")\n\n# Define features and target variable\nfeatures = data.drop(\"Fatalities\", axis=1)\ntarget = data[\"Fatalities\"]\n\n# Display the first few rows of the dataframe\nprint(\"First few rows of the encoded data:\")\nprint(data)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Read the CSV file\nFILENAME = '/kaggle/input/wikipedia-molecule-sorted/wikipedia molecules-sorted.csv'\ndata = pd.read_csv(FILENAME, encoding='latin1')\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Track encoded columns and their mappings\nencoded_mappings = {}\n\n# Encode categorical variables\nfor column in data.select_dtypes(include=['object']):\n    # Display the unique values before encoding\n    print(f\"Original values in '{column}': {data[column].unique()}\")\n    \n    # Fit the encoder and transform the column, adding 1 to each encoded value\n    data[column] = label_encoder.fit_transform(data[column].astype(str)) + 1\n    \n    # Store the mapping of original values to encoded labels, adjusted by +1\n    encoded_mappings[column] = {cls: code + 1 for cls, code in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n    \n    # Display the mapping and the encoded values\n    print(f\"Mapping for '{column}': {encoded_mappings[column]}\")\n    print(f\"Encoded values in '{column}': {data[column].unique()}\\n\")\n\n# Define features and target variable\nfeatures = data.drop(\"Molecule\", axis=1)\ntarget = data[\"Molecule\"]\n\n# Display the first few rows of the dataframe\nprint(\"First few rows of the encoded data:\")\nprint(data)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Read the CSV file\nFILENAME = 'E:/MARS-ROVER-RSR/SCIENCE/structural protien synthesis/wikipedia molecules-sorted.csv'\ndata = pd.read_csv(FILENAME, encoding='latin1')\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Track encoded columns and their mappings\nencoded_mappings = {}\n\n# Encode categorical variables\nfor column in data.select_dtypes(include=['object']):\n    # Display the unique values before encoding\n    print(f\"Original values in '{column}': {data[column].unique()}\")\n    \n    # Fit the encoder and transform the column, adding 1 to each encoded value\n    data[column] = label_encoder.fit_transform(data[column].astype(str)) + 1\n    \n    # Store the mapping of original values to encoded labels, adjusted by +1\n    encoded_mappings[column] = {cls: code + 1 for cls, code in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n    \n    # Display the mapping and the encoded values\n    print(f\"Mapping for '{column}': {encoded_mappings[column]}\")\n    print(f\"Encoded values in '{column}': {data[column].unique()}\\n\")\n\n# Define features and target variable\nfeatures = data.drop(\"Molecule\", axis=1)\ntarget = data[\"Molecule\"]\n\n# Display the first few rows of the dataframe\nprint(\"First few rows of the encoded data:\")\nprint(data)\n\n# Save the encoded data to a new CSV file\noutput_filename = 'encoded_data1.csv'\ndata.to_csv(output_filename, index=False)\nprint(f\"Encoded data saved to {output_filename}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"encoded-every-value\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree, export_text\n\n# Read the CSV file and select desired columns\ntry:\n    data = pd.read_csv('E:/AIRCRAFT/DAMAGE.csv', encoding='latin1')\n    print(\"Data loaded successfully.\")\nexcept FileNotFoundError:\n    print(\"The file was not found. Please check the file path.\")\n    exit()\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft',\n                     'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model',\n                     'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position',\n                     'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility',\n                     'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity',\n                     'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage',\n                     'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage',\n                     'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage',\n                     'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage',\n                     'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike',\n                     'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize OrdinalEncoder\nencoder = OrdinalEncoder()\n\n# Encode categorical variables\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\nprint(\"Categorical columns encoded.\")\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features)\nprint(\"Missing values in features handled.\")\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\nprint(\"Missing values in target handled.\")\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\nprint(\"Data split into training and testing sets.\")\n\n# Create an instance of the DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(random_state=42)\n\n# Fit the model on the training data\ndecision_tree.fit(x_train, y_train)\nprint(\"Model training completed.\")\n\n# Make predictions on the test data\ny_pred = decision_tree.predict(x_test)\n\n# Evaluate the model's accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy:\", accuracy)\n\n# Print the decision tree in text format for better readability\ntree_rules = export_text(decision_tree, feature_names=features.columns.tolist())\nprint(tree_rules)\n\n# Optionally, visualize the tree using plot_tree with adjusted plot parameters\nplt.figure(figsize=(40,20))  # Adjust the size to make the text more readable\nplot_tree(decision_tree, filled=True, feature_names=features.columns, class_names=True, fontsize=10, proportion=True, rounded=True)\nplt.show()\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree, export_text\nimport numpy as np\n\n# Read the CSV file and select desired columns\ntry:\n    data = pd.read_csv('/content/DAMAGE.csv', encoding='latin1')\n    print(\"Data loaded successfully.\")\nexcept FileNotFoundError:\n    print(\"The file was not found. Please check the file path.\")\n    exit()\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', \n                     'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', \n                     'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', \n                     'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', \n                     'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', \n                     'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', \n                     'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', \n                     'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', \n                     'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', \n                     'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', \n                     'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize OrdinalEncoder\nencoder = OrdinalEncoder()\n\n# Encode categorical variables\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\nprint(\"Categorical columns encoded.\")\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features)\nprint(\"Missing values in features handled.\")\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\nprint(\"Missing values in target handled.\")\n\n# Generate limited new features using arithmetic operations\ndef add_limited_arithmetic_features(features):\n    new_features = features.copy()\n    n_cols = features.shape[1]\n    \n    # Focus on a limited set of operations\n    for i in range(n_cols):\n        for j in range(i + 1, min(i + 5, n_cols)):  # Limit to a small number of combinations\n            new_features = np.column_stack((new_features, \n                                            features[:, i] + features[:, j], \n                                            features[:, i] - features[:, j],\n                                            features[:, i] * features[:, j],\n                                            np.divide(features[:, i], features[:, j], out=np.zeros_like(features[:, i]), where=features[:, j] != 0)))\n    return new_features\n\nfeatures_augmented = add_limited_arithmetic_features(features_imputed)\nprint(\"Limited arithmetic features added.\")\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_augmented, target_imputed, test_size=0.1, random_state=42)\nprint(\"Data split into training and testing sets.\")\n\n# Create an instance of the DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(random_state=42)\n\n# Fit the model on the training data\ndecision_tree.fit(x_train, y_train)\nprint(\"Model training completed.\")\n\n# Make predictions on the test data\ny_pred = decision_tree.predict(x_test)\n\n# Evaluate the model's accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy:\", accuracy)\n\n# Print the decision tree in text format for better readability\n# Note: We don't have feature names for the newly created features, so using generic names\nfeature_names = features.columns.tolist()\naugmented_feature_names = feature_names.copy()\nfor i in range(len(feature_names)):\n    for j in range(i + 1, min(i + 5, len(feature_names))):  # Limit feature combinations\n        augmented_feature_names.extend([f\"{feature_names[i]} + {feature_names[j]}\",\n                                        f\"{feature_names[i]} - {feature_names[j]}\",\n                                        f\"{feature_names[i]} * {feature_names[j]}\",\n                                        f\"{feature_names[i]} / {feature_names[j]}\"])\n\ntree_rules = export_text(decision_tree, feature_names=augmented_feature_names)\nprint(tree_rules)\n\n# Optionally, visualize the tree using plot_tree with adjusted plot parameters\nplt.figure(figsize=(40, 20))  # Adjust the size to make the text more readable\nplot_tree(decision_tree, filled=True, feature_names=augmented_feature_names, class_names=True, fontsize=10, proportion=True, rounded=True)\nplt.show()\nLN transformation \nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Read the CSV file\nFILENAME = 'E:/MARS-ROVER-RSR/SCIENCE/structural protien synthesis/wikipedia molecules-sorted.csv'\ndata = pd.read_csv(FILENAME, encoding='latin1')\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Track encoded columns and their mappings\nencoded_mappings = {}\n\n# Encode categorical variables\nfor column in data.select_dtypes(include=['object']):\n    # Display the unique values before encoding\n    print(f\"Original values in '{column}': {data[column].unique()}\")\n    \n    # Fit the encoder and transform the column, adding 1 to each encoded value\n    data[column] = label_encoder.fit_transform(data[column].astype(str)) + 1\n    \n    # Store the mapping of original values to encoded labels, adjusted by +1\n    encoded_mappings[column] = {cls: code + 1 for cls, code in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n    \n    # Display the mapping and the encoded values\n    print(f\"Mapping for '{column}': {encoded_mappings[column]}\")\n    print(f\"Encoded values in '{column}': {data[column].unique()}\\n\")\n\n# Apply natural logarithm transformation to the target feature\ntarget_column = \"Molecule\"  # Update this to match the name of your target feature\ndata[target_column] = np.log(data[target_column])\n\n# Define features and target variable\nfeatures = data.drop(target_column, axis=1)\ntarget = data[target_column]\n\n# Display the first few rows of the dataframe\nprint(\"First few rows of the encoded and transformed data:\")\nprint(data)\n\n# Save the encoded data with transformed target to a new CSV file\noutput_filename = 'encoded_log_transformed_data.csv'\ndata.to_csv(output_filename, index=False)\nprint(f\"Encoded and log-transformed data saved to {output_filename}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TREE","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Read the CSV file\ndata = pd.read_csv('E:/AIRCRAFT/DAMAGE.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"Aircraft Damage\", axis=1))\n\n# Handle missing values in the target variable\ntarget = data[\"Aircraft Damage\"].values\ntarget_imputed = imputer.fit_transform(target.reshape(-1, 1)).flatten()\n\n# Combine features and target into a DataFrame for plotting\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable\n\n# Add the target variable to the DataFrame\ndf_plot['Aircraft Damage'] = target_imputed\n\n# Perform bootstrap sampling to increase sample size\nbootstrap_sample_size = len(df_plot) * 5  # Adjust multiplier to increase sample size\nbootstrap_sample_indices = np.random.choice(df_plot.index, size=bootstrap_sample_size, replace=True)\ndf_bootstrap = df_plot.iloc[bootstrap_sample_indices]\n\n# Plot each feature against the target variable separately\nfor feature in df_bootstrap.columns[:-1]:  # Exclude the target variable itself\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=df_bootstrap[feature], y=df_bootstrap['Aircraft Damage'])\n    plt.title(f'Scatter Plot of {feature} vs Aircraft Damage')\n    plt.xlabel(feature)\n    plt.ylabel('Aircraft Damage')\n    plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GRAPH-SNS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nimport shap\nimport os\n\n# Construct the file path\nfile_path = os.path.join(\"/kaggle/input/air-damage/DAMAGE.csv\")\n\n# Read the CSV file and select desired columns\ndata = pd.read_csv(file_path)\ndata = data[['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']]\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\ndata_encoded = encoder.fit_transform(data)\n\n# Define features and target variable\nfeatures = pd.DataFrame(data_encoded, columns=data.columns).drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Create an instance of the Logistic Regression model\nlogistic = LogisticRegression()\n\n# Fit the model on the training data\nlogistic.fit(x_train, y_train)\n\n# Get predictions on the test set\ny_pred = logistic.predict(x_test)\n\n# Count the predicted values\nvalue_counts = pd.Series(y_pred).value_counts()\n\nprint(\"Count of Predicted Values:\")\nprint(value_counts)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Construct the file path\nfile_path = \"/kaggle/input/air-damage/DAMAGE.csv\"\n\n# Read the CSV file and select desired columns\ndata = pd.read_csv(file_path, low_memory=False)\ndata = data[['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']]\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\ndata_encoded = encoder.fit_transform(data)\n\n# Define features and target variable\nfeatures = pd.DataFrame(data_encoded, columns=data.columns).drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Build a simple neural network\nmodel = tf.keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(features_imputed.shape[1],)),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n\n# Evaluate the model on test data\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint('Test accuracy:', test_acc)\n\n# Get predictions on the test set\ny_pred_proba = model.predict(x_test)\ny_pred = np.round(y_pred_proba).flatten()\n\n# Count the predicted values\nvalue_counts = pd.Series(y_pred).value_counts()\n\n# Calculate percentages\ntotal_predictions = len(y_pred)\npercentage_0 = (value_counts.get(0, 0) / total_predictions) * 100\npercentage_1 = (value_counts.get(1, 0) / total_predictions) * 100\n\nprint(\"Percentage of Predicted Values:\")\nprint(\"Class 0: {:.2f}%\".format(percentage_0))\nprint(\"Class 1: {:.2f}%\".format(percentage_1))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TENSOR-PREDICTION-PROB","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import GaussianMixture\n\n# Read the CSV file\n# Read the CSV file\nfile_path = '/kaggle/input/di-electron/DI-ELECTRON-2.csv'\ndata = pd.read_csv(file_path, encoding='latin1')\n# Assuming 'M' is the target variable, drop it for clustering\nfeatures = data.drop(\"M\", axis=1)\n\n# Perform Gaussian Mixture Model clustering\nk = 20 # Number of clusters\ngmm = GaussianMixture(n_components=k, random_state=42)\nclusters = gmm.fit_predict(features)\n\n# Add cluster labels to the original DataFrame\ndata['Cluster'] = clusters\n\n# Analyze the characteristics of each cluster\ncluster_means = data.groupby('Cluster').mean()\n\n# Print characteristics of each cluster\nfor i, cluster_mean in enumerate(cluster_means.iterrows(), 1):\n    print(f\"Cluster {i}:\\n{cluster_mean[1]}\\n\")\n\n# Plot characteristics of each cluster\nnum_clusters = len(cluster_means)\nnum_features = len(features.columns)\n\nfig, axs = plt.subplots(num_clusters, num_features, figsize=(20, 10))\n\nfor i, (cluster_idx, cluster_mean) in enumerate(cluster_means.iterrows(), 1):\n    for j, feature in enumerate(features.columns):\n        axs[i-1, j].bar(x=feature, height=cluster_mean[feature])\n        axs[i-1, j].set_title(f\"Cluster {i}\")\n\nplt.tight_layout()\nplt.show()\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\nfile_path = '/kaggle/input/di-electron/DI-ELECTRON-2.csv'\ndata = pd.read_csv(file_path, encoding='latin1')\n# Assuming 'M' is the target variable, drop it for clustering\nfeatures = data.drop(\"M\", axis=1)\n\n# Perform k-means clustering\nk = 20 # Number of clusters\nkmeans = KMeans(n_clusters=k, random_state=42)\nclusters = kmeans.fit_predict(features)\n\n# Add cluster labels to the original DataFrame\ndata['Cluster'] = clusters\n\n# Analyze the characteristics of each cluster\ncluster_means = data.groupby('Cluster').mean()\n\n# Print characteristics of each cluster\nfor i, cluster_mean in enumerate(cluster_means.iterrows(), 1):\n    print(f\"Cluster {i}:\\n{cluster_mean[1]}\\n\")\n\n# Plot characteristics of each cluster\nnum_clusters = len(cluster_means)\nnum_features = len(features.columns)\n\nfig, axs = plt.subplots(num_clusters, num_features, figsize=(20, 10))\n\nfor i, (cluster_idx, cluster_mean) in enumerate(cluster_means.iterrows(), 1):\n    for j, feature in enumerate(features.columns):\n        axs[i-1, j].bar(x=feature, height=cluster_mean[feature])\n        axs[i-1, j].set_title(f\"Cluster {i}\")\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CLUSTER","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read the CSV file\nfile_path = \"/kaggle/input/di-electron/DI-ELECTRON-2.csv\"\ndata = pd.read_csv(file_path, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata['Run_encoded'] = label_encoder.fit_transform(data['Run'])\n\n# Display the encoded values of the \"Run\" column\nprint(data[['Run', 'Run_encoded']].head())\n# Specify the row index\nrow_index = 0  # Example row index\n\n# Retrieve the label encoded value of \"Run\" at the specified row index\nencoded_value = data.at[row_index, 'Run_encoded']\n\n# Display the label encoded value\nprint(f\"Label Encoded Value of 'Run' at Row {row_index}: {encoded_value}\")\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Read the CSV file\n# Read the CSV file\nfile_path = \"/kaggle/input/di-electron/DI-ELECTRON-2.csv\"\ndata = pd.read_csv(file_path, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Initialize OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\n\n# Encode categorical variables\ndata['Run_encoded'] = ordinal_encoder.fit_transform(data[['Run']])\n\n# Display the encoded values of the \"Run\" column\nprint(data[['Run', 'Run_encoded']].head())\n\n# Specify the row index\nrow_index = 0  # Example row index\n\n# Retrieve the label encoded value of \"Run\" at the specified row index\nencoded_value = data.at[row_index, 'Run_encoded']\n\n# Display the label encoded value\nprint(f\"Label Encoded Value of 'Run' at Row {row_index}: {encoded_value}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LABEL-ENCODED","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\ndata = pd.read_csv('E:/AIRCRAFT/DAMAGE.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Display the transformed dataframe\nprint(data.head())\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.fit_transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Predict on test data\ny_pred = regressor.predict(x_test_poly)\n\n# Binarize the predicted and actual values\ny_pred_binary = (y_pred > 0.5).astype(int)\ny_test_binary = (y_test > 0.5).astype(int)\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(y_test_binary, y_pred_binary, labels=[0, 1])  # Provide labels for clarity\nconf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\nprint(\"Confusion Matrix:\")\nprint(conf_matrix_df)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_binary, y_pred_binary)\nprint(\"\\nAccuracy:\", accuracy)\n\n# Print detailed analysis for each feature\nprint(\"\\nDetailed Analysis of Confusion Matrix by Feature Impact:\")\n\n# Loop through each feature\nfor feature in features.columns:\n    # Create a temporary dataframe with only the feature and target\n    temp_df = pd.DataFrame(features[feature])\n    temp_df['Aircraft Damage'] = target\n\n    # Train-test split for this feature\n    x_feat_train, x_feat_test, y_feat_train, y_feat_test = train_test_split(temp_df[[feature]].values, target, test_size=0.1, random_state=42)\n\n    # Apply polynomial features transformation\n    x_feat_train_poly = poly.fit_transform(x_feat_train)\n    x_feat_test_poly = poly.fit_transform(x_feat_test)\n\n    # Fit the model on the polynomial features\n    regressor.fit(x_feat_train_poly, y_feat_train)\n\n    # Predict on test data\n    y_feat_pred = regressor.predict(x_feat_test_poly)\n\n    # Binarize the predicted and actual values\n    y_feat_pred_binary = (y_feat_pred > 0.5).astype(int)\n    y_feat_test_binary = (y_feat_test > 0.5).astype(int)\n\n    # Compute confusion matrix for this feature\n    conf_matrix_feat = confusion_matrix(y_feat_test_binary, y_feat_pred_binary, labels=[0, 1])\n\n    # Print feature header\n    print(f\"\\nFeature: {feature}\")\n    print(\"Confusion Matrix:\")\n    print(pd.DataFrame(conf_matrix_feat, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n\n    # Print interpretation of confusion matrix for this feature\n    print(\"Interpretation of Confusion Matrix:\")\n    print(\"True Negative (TN): Predicted 0 (No Damage) and Actual 0 (No Damage)\")\n    print(\"False Positive (FP): Predicted 1 (Damage) but Actual 0 (No Damage)\")\n    print(\"False Negative (FN): Predicted 0 (No Damage) but Actual 1 (Damage)\")\n    print(\"True Positive (TP): Predicted 1 (Damage) and Actual 1 (Damage)\")\n\n    # Plot confusion matrix for this feature\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(conf_matrix_feat, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n    plt.title(f'Confusion Matrix for Feature: {feature}')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CONFUSION-MATRIX","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_text\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\nfile_name='/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Convert target variable to binary (0 and 1)\ntarget_binary = (target > 0).astype(int)  # Example: 0 if no damage, 1 if damage\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target_binary.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n# Build the ANN model for binary classification\nmodel = keras.Sequential([\n    layers.Input(shape=(x_train.shape[1],)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)\n\n# Evaluate the model\ny_pred = (model.predict(x_test) > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"ANN Accuracy: {accuracy}\")\n\n# Fit a decision tree to the same data\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(x_train, y_train)\n\n# Predict with the decision tree\ny_tree_pred = tree.predict(x_test)\naccuracy_tree = accuracy_score(y_test, y_tree_pred)\n\nprint(f\"Decision Tree Accuracy: {accuracy_tree}\")\n\n# Plot the tree\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(20, 10))\nplot_tree(tree, filled=True, feature_names=features.columns)\nplt.show()\n\n# Print the decision tree rules as text\ntree_rules = export_text(tree, feature_names=list(features.columns))\nprint(tree_rules)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"BINARY-SEARCH","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier, _tree\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\ndata = pd.read_csv('/content/DAMAGE.csv', encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Convert target variable to binary (0 and 1)\ntarget_binary = (target > 0).astype(int)  # Example: 0 if no damage, 1 if damage\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target_binary.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n# Build the ANN model for binary classification\nmodel = keras.Sequential([\n    layers.Input(shape=(x_train.shape[1],)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)\n\n# Evaluate the model\ny_pred = (model.predict(x_test) > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"ANN Accuracy: {accuracy}\")\n\n# Fit a decision tree to the same data\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(x_train, y_train)\n\n# DFS function to predict using the decision tree\ndef dfs_predict(tree, sample):\n    node = 0\n    while True:\n        if tree.tree_.feature[node] != _tree.TREE_UNDEFINED:  # If node is not a leaf\n            feature_index = tree.tree_.feature[node]\n            threshold = tree.tree_.threshold[node]\n            if sample[feature_index] <= threshold:\n                node = tree.tree_.children_left[node]\n            else:\n                node = tree.tree_.children_right[node]\n        else:  # If node is a leaf\n            return tree.tree_.value[node].argmax()  # Return the predicted class (0 or 1)\n\n# Predict with DFS for each test sample\ndfs_predictions = [dfs_predict(tree, x) for x in x_test]\n\n# Calculate accuracy for DFS predictions\naccuracy_dfs = accuracy_score(y_test, dfs_predictions)\n\nprint(f\"DFS Decision Tree Accuracy: {accuracy_dfs}\")\n\n# Plot the tree\nplt.figure(figsize=(20, 10))\nplot_tree(tree, filled=True, feature_names=features.columns)\nplt.show()\n\n# Print the decision tree rules as text\ntree_rules = export_text(tree, feature_names=list(features.columns))\nprint(tree_rules)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"DFS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier, _tree, plot_tree, export_text\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\nfile_name='/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Convert target variable to binary (0 and 1)\ntarget_binary = (target > 0).astype(int)  # Example: 0 if no damage, 1 if damage\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target_binary.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n# Build the ANN model for binary classification\nmodel = keras.Sequential([\n    layers.Input(shape=(x_train.shape[1],)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)\n\n# Evaluate the model\ny_pred = (model.predict(x_test) > 0.5).astype(int).flatten()  # Convert probabilities to binary predictions\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"ANN Accuracy: {accuracy}\")\n\n# Fit a decision tree to the same data\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(x_train, y_train)\n\n# BFS function to predict using the decision tree\ndef bfs_predict(tree, sample):\n    queue = [0]  # Start with the root node\n    while queue:\n        node = queue.pop(0)  # Dequeue the first element\n        if tree.tree_.feature[node] != _tree.TREE_UNDEFINED:  # If node is not a leaf\n            feature_index = tree.tree_.feature[node]\n            threshold = tree.tree_.threshold[node]\n            if sample[feature_index] <= threshold:\n                queue.append(tree.tree_.children_left[node])  # Go left\n            else:\n                queue.append(tree.tree_.children_right[node])  # Go right\n        else:  # If node is a leaf\n            return tree.tree_.value[node].argmax()  # Return the predicted class (0 or 1)\n\n# Predict with BFS for each test sample\nbfs_predictions = [bfs_predict(tree, x) for x in x_test]\n\n# Calculate accuracy for BFS predictions\naccuracy_bfs = accuracy_score(y_test, bfs_predictions)\n\nprint(f\"BFS Decision Tree Accuracy: {accuracy_bfs}\")\n\n# Plot the tree\nplt.figure(figsize=(20, 10))\nplot_tree(tree, filled=True, feature_names=features.columns)\nplt.show()\n\n# Print the decision tree rules as text\ntree_rules = export_text(tree, feature_names=list(features.columns))\nprint(tree_rules)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"BFS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\n\n# Read the CSV file\nfile_name='/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator',\n                     'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make',\n                     'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position',\n                     'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase',\n                     'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name',\n                     'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike',\n                     'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage',\n                     'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike',\n                     'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike',\n                     'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike',\n                     'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage',\n                     'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Convert target variable to binary (0 and 1)\ntarget_binary = (target > 0).astype(int)\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_binary, test_size=0.1, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n# Calculate the mean value of each feature for the positive and negative classes\npositive_means = np.mean(x_train[y_train == 1], axis=0)\nnegative_means = np.mean(x_train[y_train == 0], axis=0)\n\n# Create rules based on these mean values\nrules = []\nfor i in range(x_train.shape[1]):\n    if positive_means[i] > negative_means[i]:\n        rules.append((i, '>', (positive_means[i] + negative_means[i]) / 2))\n    else:\n        rules.append((i, '<', (positive_means[i] + negative_means[i]) / 2))\n\n# Define a greedy pattern function to predict based on the rules\ndef greedy_predict(sample, rules):\n    score = 0\n    for rule in rules:\n        feature_index, operator, threshold = rule\n        if operator == '>':\n            if sample[feature_index] > threshold:\n                score += 1\n        else:\n            if sample[feature_index] < threshold:\n                score += 1\n    return int(score > len(rules) / 2)\n\n# Predict with the greedy algorithm for each test sample\ngreedy_predictions = [greedy_predict(x, rules) for x in x_test]\n\n# Calculate accuracy for greedy predictions\naccuracy_greedy = accuracy_score(y_test, greedy_predictions)\n\nprint(f\"Greedy Algorithm Accuracy: {accuracy_greedy}\")\n\n# Print the rules\nfor rule in rules:\n    feature_index, operator, threshold = rule\n    feature_name = data.columns[feature_index]\n    print(f\"Feature {feature_name} {operator} {threshold}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GREEDY","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport shap\nimport tempfile\nimport webbrowser\n\n# Read the CSV file\nfile_path = \"/kaggle/input/air-damage/DAMAGE.csv\"\ndata = pd.read_csv(file_path,encoding='latin1')\ndata = data[['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']]\n\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Convert non-numerical values to numerical using LabelEncoder\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Define features and target variable\nfeatures = data_encoded.drop(\"Aircraft Damage\", axis=1)\ntarget = data_encoded[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation with reduced degree and features\npoly = PolynomialFeatures(degree=1, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Get the polynomial feature names\nfeature_names = poly.get_feature_names_out(features.columns)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Predict values using the trained model\ny_train_pred = regressor.predict(x_train_poly)\ny_test_pred = regressor.predict(x_test_poly)\n\n# Initialize the explainer with the trained model and training data\nexplainer = shap.Explainer(regressor, x_train_poly)\n\n# Calculate SHAP values for the testing data\nshap_values = explainer.shap_values(x_test_poly)\n\n# Check the shape of x_test_poly and length of feature_names\nprint(\"Shape of x_test_poly:\", x_test_poly.shape)\nprint(\"Length of feature_names:\", len(feature_names))\n\n# Print each feature name along with its index\nfor idx, name in enumerate(feature_names):\n    print(f\"Feature {idx}: {name}\")\n\n# Generate the summary plot HTML\ntry:\n    shap_html = shap.summary_plot(shap_values, features=x_test_poly, feature_names=feature_names, show=False)\n\n    # Save the HTML content to a temporary file\n    with tempfile.NamedTemporaryFile(suffix='.html', delete=False) as f:\n        f.write(shap_html.data.encode('utf-8'))\n        temp_html_file = f.name\n\n    # Open the temporary HTML file in a web browser\n    webbrowser.open('file://' + temp_html_file)\nexcept Exception as e:\n    print(\"Error generating SHAP summary plot:\", str(e))\n\n# Print a subset of feature names\nprint(\"Printing a subset of feature names:\")\nfor idx, name in enumerate(feature_names[:10]):  # Print first 10 feature names\n    print(f\"Feature {idx}: {name}\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read the CSV file and select desired columns\nfile_path = \"/kaggle/input/air-damage/DAMAGE.csv\"\ndata = pd.read_csv(file_path,encoding='latin1')\ndata = data[['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']]\n\ndata = data[['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']]\n\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n# Convert non-numerical values to numerical using LabelEncoder\n# Encode categorical variables\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nimport numpy as np\n# Read the CSV file\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Display the transformed dataframe\nprint(data.head())\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\nlabel_encoder = LabelEncoder()\ndata[\"Record ID\"] = label_encoder.fit_transform(data[\"Record ID\"])\ndata[\"Incident Year\"] = label_encoder.fit_transform(data[\"Incident Year\"])\ndata[\"Incident Month\"] = label_encoder.fit_transform(data[\"Incident Month\"])\ndata[\"Incident Day\"] = label_encoder.fit_transform(data[\"Incident Day\"])\ndata[\"Operator ID\"] = label_encoder.fit_transform(data[\"Operator ID\"])\ndata[\"Operator\"] = label_encoder.fit_transform(data[\"Operator\"])\n\n# ... (continue encoding other columns)\n# Define features and target variable\nfeatures = data_encoded.drop(\"Aircraft Damage\", axis=1)\ntarget = data_encoded[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Predict 'G1' values using the trained model\ny_train_pred = regressor.predict(x_train_poly)\ny_test_pred = regressor.predict(x_test_poly)\n\n# Retrieve the column names\ncolumn_names = features.columns\n\n# Plot predicted 'G1' against each column\nfor column_index, column in enumerate(column_names):\n    # Scatter plot for training data\n    plt.scatter(x_train[:, column_index], y_train_pred, color='blue', label='Training Data')\n\n    # Scatter plot for testing data\n    plt.scatter(x_test[:, column_index], y_test_pred, color='red', label='Testing Data')\n\n    plt.xlabel(column)\n    plt.ylabel('Predicted Aircraft Damage')\n    plt.title(f'Predicted Aircraft Damage vs {column}')\n    plt.legend()\n    plt.show()\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom gplearn.genetic import SymbolicRegressor\nfrom gplearn.functions import make_function\nfrom sklearn.metrics import mean_squared_error\n\n# Read the CSV file and select desired columns\nfile_path = \"/kaggle/input/air-damage/DAMAGE.csv\"\ndata = pd.read_csv(file_path, encoding='latin1')\ndata = data[['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft',\n             'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model',\n             'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position',\n             'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility',\n             'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity',\n             'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage',\n             'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage',\n             'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage',\n             'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage',\n             'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike',\n             'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']]\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1, random_state=42)\n\n# Define improved custom functions for genetic programming\ndef safe_log(x):\n    return np.log(np.maximum(x, 1e-10))\n\ndef safe_exp(x):\n    return np.where(x < 0, np.expm1(x) + 1, np.minimum(np.exp(x), 1e100))\n\ndef safe_pow(x, y):\n    y_int = np.round(y).astype(int)\n    is_int = np.isclose(y, y_int)\n    int_result = np.where((x < 0) & is_int, np.sign(x)**y_int * np.abs(x)**y_int, np.power(np.abs(x), y))\n    float_result = np.exp(safe_log(np.abs(x)) * y)\n    return np.where(is_int, int_result, float_result)\n\ndef safe_sqrt(x):\n    return np.sqrt(np.abs(x))\n\ndef safe_abs(x):\n    return np.abs(x)\n\nsafe_log = make_function(function=safe_log, name='log', arity=1)\nsafe_exp = make_function(function=safe_exp, name='exp', arity=1)\nsafe_pow = make_function(function=safe_pow, name='pow', arity=2)\nsafe_sqrt = make_function(function=safe_sqrt, name='sqrt', arity=1)\nsafe_abs = make_function(function=safe_abs, name='abs', arity=1)\n\n# Use genetic programming to evolve a symbolic expression\ngp = SymbolicRegressor(population_size=1000,\n                       generations=20,\n                       stopping_criteria=0.01,\n                       p_crossover=0.7,\n                       p_subtree_mutation=0.1,\n                       p_hoist_mutation=0.05,\n                       p_point_mutation=0.1,\n                       max_samples=0.9,\n                       verbose=1,\n                       parsimony_coefficient=0.01,\n                       random_state=42,\n                       const_range=(-1, 1),\n                       function_set=['add', 'sub', 'mul', 'div', 'sin', 'cos', 'tan',\n                                     safe_log, safe_exp, safe_pow, safe_sqrt, safe_abs])\n\n# Fit the symbolic regression model\ngp.fit(x_train, y_train)\n\n# Extract and print the symbolic equation\nsymbolic_eq = gp._program\n\nprint(\"\\nSymbolic Equation:\")\nprint(symbolic_eq)\n\n# Predict 'Aircraft Damage' values using the trained model\ny_train_pred = gp.predict(x_train)\ny_test_pred = gp.predict(x_test)\n\n# Retrieve the column names\ncolumn_names = features.columns\n\n# Plot predicted 'Aircraft Damage' against each column\nfor column_index, column in enumerate(column_names):\n    plt.figure(figsize=(10, 6))\n    # Scatter plot for training data\n    plt.scatter(x_train[:, column_index], y_train_pred, color='blue', label='Training Data', alpha=0.5)\n\n    # Scatter plot for testing data\n    plt.scatter(x_test[:, column_index], y_test_pred, color='red', label='Testing Data', alpha=0.5)\n\n    plt.xlabel(column)\n    plt.ylabel('Predicted Aircraft Damage')\n    plt.title(f'Predicted Aircraft Damage vs {column}')\n    plt.legend()\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ML-PLOT-SHAP","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\nfile_path = \"/kaggle/input/air-damage/DAMAGE.csv\"\ndata = pd.read_csv(file_path, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Display the transformed dataframe\nprint(data.head())\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.fit_transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = regressor.coef_\nintercept = regressor.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"Aircraft Damage = \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:.9f}\"\n    else:\n        equation += f\" + {coefficient:.9f} * {feature_names[i]}\"\nprint('Coefficients:', coefficients)\nprint('Intercept:', intercept)\nprint(\"Equation:\", equation)\n\n# Predict on test data\ny_pred = regressor.predict(x_test_poly)\n\n# Binarize the predicted and actual values\ny_pred_binary = (y_pred > 0.5).astype(int)\ny_test_binary = (y_test > 0.5).astype(int)\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(y_test_binary, y_pred_binary, labels=[0, 1])  # Provide labels for clarity\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_binary, y_pred_binary)\nprint(\"Accuracy:\", accuracy)\n\n# Plot normalized confusion matrix\nconf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_norm, annot=True, cmap='Blues', fmt=\".2f\")\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Normalized Confusion Matrix')\nplt.show()\n\n# Plot correlation matrix heatmap for selected features\nselected_features = features.columns[:10]  # Example: Select first 10 features for simplicity\nplt.figure(figsize=(12, 10))\nsns.heatmap(features[selected_features].corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('Correlation Matrix Heatmap of Selected Features')\nplt.show()\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Display the transformed dataframe\nprint(data.head())\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.fit_transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = regressor.coef_\nintercept = regressor.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"Aircraft Damage = \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:.9f}\"\n    else:\n        equation += f\" + {coefficient:.9f} * {feature_names[i]}\"\nprint('Coefficients:', coefficients)\nprint('Intercept:', intercept)\nprint(\"Equation:\", equation)\n\n# Predict on test data\ny_pred = regressor.predict(x_test_poly)\n\n# Binarize the predicted and actual values\ny_pred_binary = (y_pred > 0.5).astype(int)\ny_test_binary = (y_test > 0.5).astype(int)\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(y_test_binary, y_pred_binary, labels=[0, 1])  # Provide labels for clarity\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_binary, y_pred_binary)\nprint(\"Accuracy:\", accuracy)\n\n# Plot normalized confusion matrix\nconf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_norm, annot=True, cmap='Blues', fmt=\".2f\")\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Normalized Confusion Matrix')\n\n# Plot correlation matrix heatmap for all features\nplt.figure(figsize=(12, 10))\nsns.heatmap(features.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('Correlation Matrix Heatmap of All Features')\n\n# Print the equation\nprint(\"\\nEquation for Aircraft Damage Prediction:\")\nprint(equation)\n\nplt.show()\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(data.drop(\"Aircraft Damage\", axis=1))\n\n# Combine features and target into a DataFrame for plotting\ndf_plot = pd.DataFrame(features_imputed, columns=data.columns[:-1])  # Exclude target variable\n\n# Add the target variable to the DataFrame\ndf_plot['Aircraft Damage'] = data['Aircraft Damage']\n\n# Select a subset of features for pair plot (adjust based on your preference)\nselected_features = df_plot.columns[:10]  # Example: Select first 10 features\n\n# Create pair plot using Seaborn\nplt.figure(figsize=(20, 15))\nsns.pairplot(df_plot[selected_features], kind='scatter')\nplt.suptitle('Pair Plot of Selected Features', y=1.02, fontsize=18)\nplt.tight_layout()\nplt.show()\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree, export_text\n\n\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft',\n                     'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model',\n                     'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position',\n                     'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility',\n                     'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity',\n                     'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage',\n                     'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage',\n                     'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage',\n                     'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage',\n                     'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike',\n                     'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize OrdinalEncoder\nencoder = OrdinalEncoder()\n\n# Encode categorical variables\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\nprint(\"Categorical columns encoded.\")\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features)\nprint(\"Missing values in features handled.\")\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\nprint(\"Missing values in target handled.\")\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\nprint(\"Data split into training and testing sets.\")\n\n# Create an instance of the DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(random_state=42)\n\n# Fit the model on the training data\ndecision_tree.fit(x_train, y_train)\nprint(\"Model training completed.\")\n\n# Make predictions on the test data\ny_pred = decision_tree.predict(x_test)\n\n# Evaluate the model's accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy:\", accuracy)\n\n# Print the decision tree in text format for better readability\ntree_rules = export_text(decision_tree, feature_names=features.columns.tolist())\nprint(tree_rules)\n\n# Optionally, visualize the tree using plot_tree with adjusted plot parameters\nplt.figure(figsize=(40,20))  # Adjust the size to make the text more readable\nplot_tree(decision_tree, filled=True, feature_names=features.columns, class_names=True, fontsize=10, proportion=True, rounded=True)\nplt.show()\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree, export_text\n\n\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft',\n                     'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model',\n                     'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position',\n                     'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility',\n                     'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity',\n                     'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage',\n                     'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage',\n                     'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage',\n                     'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage',\n                     'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike',\n                     'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize OrdinalEncoder\nencoder = OrdinalEncoder()\n\n# Encode categorical variables\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\nprint(\"Categorical columns encoded.\")\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features)\nprint(\"Missing values in features handled.\")\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\nprint(\"Missing values in target handled.\")\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\nprint(\"Data split into training and testing sets.\")\n\n# Create an instance of the DecisionTreeClassifier with limited depth\ndecision_tree = DecisionTreeClassifier(random_state=42, max_depth=5)  # Adjust max_depth to control tree complexity\n\n# Fit the model on the training data\ndecision_tree.fit(x_train, y_train)\nprint(\"Model training completed.\")\n\n# Make predictions on the test data\ny_pred = decision_tree.predict(x_test)\n\n# Evaluate the model's accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy:\", accuracy)\n\n# Print the decision tree in text format for better readability\ntree_rules = export_text(decision_tree, feature_names=features.columns.tolist())\nprint(tree_rules)\n\n# Optionally, visualize the tree using plot_tree with adjusted plot parameters\nplt.figure(figsize=(20,10))  # Adjust the size to make the text more readable\nplot_tree(decision_tree, filled=True, feature_names=features.columns, class_names=True, fontsize=10, proportion=True, rounded=True)\nplt.show()\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\ncolumns_to_encode = ['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Read the CSV file\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID','Incident Year','Incident Month','Incident Day','Operator ID','Operator','Aircraft','Aircraft Type','Aircraft Make','Aircraft Model','Aircraft Mass','Engine Make','Engine Model','Engines','Engine Type','Engine1 Position','Engine2 Position','Engine3 Position','Engine4 Position','Airport ID','Airport','State','FAA Region','Warning Issued','Flight Phase','Visibility','Precipitation','Height','Speed','Distance','Species ID','Species Name','Species Quantity','Flight Impact','Fatalities','Injuries','Aircraft Damage','Radome Strike','Radome Damage','Windshield Strike','Windshield Damage','Nose Strike','Nose Damage','Engine1 Strike','Engine1 Damage','Engine2 Strike','Engine2 Damage','Engine3 Strike','Engine3 Damage','Engine4 Strike','Engine4 Damage','Engine Ingested','Propeller Strike','Propeller Damage','Wing or Rotor Strike','Wing or Rotor Damage','Fuselage Strike','Fuselage Damage','Landing Gear Strike','Landing Gear Damage','Tail Strike','Tail Damage','Lights Strike','Lights Damage','Other Strike','Other Damage']\n\n# Initialize OrdinalEncoder\nencoder = OrdinalEncoder()\n\n# Encode categorical variables\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Display the transformed dataframe\nprint(data.head())\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LogisticRegression model\nlogistic_reg = LogisticRegression()\n\n# Fit the model on the training data\nlogistic_reg.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\naccuracy = logistic_reg.score(x_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Retrieve the coefficients and intercept\ncoefficients = logistic_reg.coef_[0].tolist()\nintercept = logistic_reg.intercept_[0]\n\n# Print the coefficients and intercept\nprint(\"Coefficients:\", coefficients)\nprint(\"Intercept:\", intercept)\nimport numpy as np\n\n# Define the logistic equation function\ndef logistic_equation(features, coefficients, intercept):\n    z = np.dot(features, coefficients) + intercept\n    return 1 / (1 + np.exp(-z))\n\n# Create an instance of the LogisticRegression model\nlogistic_reg = LogisticRegression()\n\n# Fit the model on the training data\nlogistic_reg.fit(x_train, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = logistic_reg.coef_[0]\nintercept = logistic_reg.intercept_[0]\n\n# Extract column names of the features\nfeature_names = data.drop(\"Aircraft Damage\", axis=1).columns\n\n# Construct the logistic equation string\nlogistic_eq_str = f\"P(Aircraft Damage=1) = 1 / (1 + e^(-({intercept}\"\nfor feature, coef in zip(feature_names, coefficients):\n    logistic_eq_str += f\" + {coef:.9f}*{feature}\"\nlogistic_eq_str += \"))\"\n\nprint(\"Logistic Equation:\")\nprint(logistic_eq_str)\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Display the transformed dataframe\nprint(data.head())\n\n# Select a specific feature for analysis (change 'Aircraft Type' to your desired feature)\nselected_feature = 'Aircraft Type'\n\n# Create a temporary dataframe with the selected feature and target\ntemp_df = pd.DataFrame(features[selected_feature])\ntemp_df['Aircraft Damage'] = target\n\n# Handle missing values in the selected feature\nimputer = SimpleImputer(strategy='mean')\nselected_feature_imputed = imputer.fit_transform(temp_df[[selected_feature]])\n\n# Split the data into training and testing sets for the selected feature\nx_feat_train, x_feat_test, y_feat_train, y_feat_test = train_test_split(selected_feature_imputed, target, test_size=0.1, random_state=42)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_feat_train_poly = poly.fit_transform(x_feat_train)\nx_feat_test_poly = poly.fit_transform(x_feat_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features for the selected feature\nregressor.fit(x_feat_train_poly, y_feat_train)\n\n# Predict on test data for the selected feature\ny_feat_pred = regressor.predict(x_feat_test_poly)\n\n# Binarize the predicted and actual values for the selected feature\ny_feat_pred_binary = (y_feat_pred > 0.5).astype(int)\ny_feat_test_binary = (y_feat_test > 0.5).astype(int)\n\n# Compute confusion matrix for the selected feature\nconf_matrix_feat = confusion_matrix(y_feat_test_binary, y_feat_pred_binary, labels=[0, 1])\n\n# Display confusion matrix for the selected feature\nconf_matrix_df = pd.DataFrame(conf_matrix_feat, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\nprint(f\"\\nConfusion Matrix for Feature '{selected_feature}':\")\nprint(conf_matrix_df)\n\n# Calculate accuracy for the selected feature\naccuracy = accuracy_score(y_feat_test_binary, y_feat_pred_binary)\nprint(\"\\nAccuracy:\", accuracy)\n\n# Plot normalized confusion matrix for the selected feature\nconf_matrix_norm = conf_matrix_feat.astype('float') / conf_matrix_feat.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_norm, annot=True, cmap='Blues', fmt=\".2f\")\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Normalized Confusion Matrix for Feature \\'{selected_feature}\\'')\nplt.show()\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Display the transformed dataframe\nprint(data.head())\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.fit_transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Predict on test data\ny_pred = regressor.predict(x_test_poly)\n\n# Binarize the predicted and actual values\ny_pred_binary = (y_pred > 0.5).astype(int)\ny_test_binary = (y_test > 0.5).astype(int)\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(y_test_binary, y_pred_binary, labels=[0, 1])  # Provide labels for clarity\nconf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\nprint(\"Confusion Matrix:\")\nprint(conf_matrix_df)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_binary, y_pred_binary)\nprint(\"\\nAccuracy:\", accuracy)\n\n# Print detailed analysis for each feature\nprint(\"\\nDetailed Analysis of Confusion Matrix by Feature Impact:\")\n\n# Loop through each feature\nfor feature in features.columns:\n    # Create a temporary dataframe with only the feature and target\n    temp_df = pd.DataFrame(features[feature])\n    temp_df['Aircraft Damage'] = target\n\n    # Train-test split for this feature\n    x_feat_train, x_feat_test, y_feat_train, y_feat_test = train_test_split(temp_df[[feature]].values, target, test_size=0.1, random_state=42)\n\n    # Apply polynomial features transformation\n    x_feat_train_poly = poly.fit_transform(x_feat_train)\n    x_feat_test_poly = poly.fit_transform(x_feat_test)\n\n    # Fit the model on the polynomial features\n    regressor.fit(x_feat_train_poly, y_feat_train)\n\n    # Predict on test data\n    y_feat_pred = regressor.predict(x_feat_test_poly)\n\n    # Binarize the predicted and actual values\n    y_feat_pred_binary = (y_feat_pred > 0.5).astype(int)\n    y_feat_test_binary = (y_feat_test > 0.5).astype(int)\n\n    # Compute confusion matrix for this feature\n    conf_matrix_feat = confusion_matrix(y_feat_test_binary, y_feat_pred_binary, labels=[0, 1])\n\n    # Print feature header\n    print(f\"\\nFeature: {feature}\")\n    print(\"Confusion Matrix:\")\n    print(pd.DataFrame(conf_matrix_feat, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n\n    # Print interpretation of confusion matrix for this feature\n    print(\"Interpretation of Confusion Matrix:\")\n    print(\"True Negative (TN): Predicted 0 (No Damage) and Actual 0 (No Damage)\")\n    print(\"False Positive (FP): Predicted 1 (Damage) but Actual 0 (No Damage)\")\n    print(\"False Negative (FN): Predicted 0 (No Damage) but Actual 1 (Damage)\")\n    print(\"True Positive (TP): Predicted 1 (Damage) and Actual 1 (Damage)\")\n\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ML-PATTERN","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\n# Read the CSV file\nfile_path = \"/kaggle/input/pupil-csv/pupil-mat.csv\"\ndata = pd.read_csv(file_path,encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define columns to encode\ncolumns_to_encode = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Display the transformed dataframe\nprint(data.head())\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the Lasso model\nlasso_regressor = Lasso(alpha=0.1)  # You can adjust the alpha parameter for tuning the sparsity\n\n# Fit the model on the polynomial features\nlasso_regressor.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = lasso_regressor.coef_\nintercept = lasso_regressor.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"G1 = \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:.2f}\"\n    else:\n        equation += f\" + {coefficient:.2f} * {feature_names[i]}\"\nprint('Coefficients:', coefficients)\nprint('Intercept:', intercept)\nprint(\"Equation:\", equation)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the Lasso regression model\nlasso = Lasso(alpha=0.1)  # Adjust regularization strength with alpha\n\n# Fit the model on the training data\nlasso.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\nacc = lasso.score(x_test, y_test)\nprint(\"Accuracy:\", acc)\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the Lasso Regression model\nmodel = Lasso(alpha=0.1)  # Adjust regularization strength with alpha\n\n# Fit the model on the training data\nmodel.fit(x_train, y_train)\n\n# Evaluate the model's accuracy on training and testing data\ntrain_accuracy = model.score(x_train, y_train)\ntest_accuracy = model.score(x_test, y_test)\n\nprint(\"Train Accuracy:\", train_accuracy)\nprint(\"Test Accuracy:\", test_accuracy)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read the CSV file and select desired columns\n\n# Label encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.columns[data.dtypes == object]:\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define features and target variable\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the Lasso model\nlasso_regressor = Lasso(alpha=0.1)  # You can adjust the alpha value for desired sparsity\n\n# Fit the model on the polynomial features\nlasso_regressor.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = lasso_regressor.coef_\nintercept = lasso_regressor.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"G1 = \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:.2f}\"\n    else:\n        equation += f\" + {coefficient:.2f} * {feature_names[i]}\"\n\nprint(\"Equation:\", equation)\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n# Handle missing values in the data\nimputer = SimpleImputer(strategy='mean')\ndata_imputed = imputer.fit_transform(data)\n\n# Separate features and target variable\nX = data_imputed[:, :-1]\ny = data_imputed[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the ElasticNet regression model\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Adjust regularization strength with alpha and mix ratio with l1_ratio\n\n# Fit the model on the polynomial features\nelastic_net.fit(x_train_poly, y_train)\n\n# Predict 'G1' values for training and testing sets\ny_train_pred = elastic_net.predict(x_train_poly)\ny_test_pred = elastic_net.predict(x_test_poly)\n\n# Print the predicted 'G1' values\nprint(\"Predicted 'G1' values for training set:\", y_train_pred)\nprint(\"Predicted 'G1' values for test set:\", y_test_pred)\n\n# Plot the actual G1 values and the predicted G1 values\nimport matplotlib.pyplot as plt\n\nplt.scatter(y_test, y_test_pred)\nplt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], color='red', linestyle='--')\nplt.xlabel(\"Actual G1\")\nplt.ylabel(\"Predicted G1\")\nplt.title(\"Lasso Regression: Actual vs Predicted G1\")\nplt.show()\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Separate the features and target variable\nX = data_encoded.drop(\"G1\", axis=1)\ny = data_encoded[\"G1\"]\n\n# Handle missing values in the data\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.1)\n\n# Create an instance of the Lasso regression model\nlasso = Lasso(alpha=0.1)  # Adjust regularization strength with alpha\n\n# Fit the model on the training data\nlasso.fit(x_train, y_train)\n\n# Get feature names\nfeature_names = X.columns\n\n# Print the coefficients of the model\nprint(\"Coefficients:\", lasso.coef_)\n\n# Plot 'G1' against each column\nfor i, column in enumerate(feature_names):\n    plt.scatter(X[column], y)\n    plt.xlabel(column)\n    plt.ylabel('G1')\n    plt.title(f'G1 vs {column}')\n    plt.show()\n\nimport pandas as pd\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file into a pandas DataFrame\n\n\n# Separate the features (input variables) and the target variable\nX = data[['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob','Fjob','reason','guardian','traveltime','studytime','failures','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic','famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2']]\ny = data[\"G1\"]\n\n# Handle missing values in y\nimputer = SimpleImputer(strategy='mean')\ny_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).flatten()\n\n# Preprocess and encode non-numeric columns in X using LabelEncoder\nnon_numeric_cols = ['school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic']\nencoder = LabelEncoder()\nX_encoded = X.copy()\nfor col in non_numeric_cols:\n    X_encoded[col] = encoder.fit_transform(X[col])\n\n# Handle missing values in X\nX_imputed = imputer.fit_transform(X_encoded)\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_imputed)\n\n# Create an instance of the Lasso Regression model\nmodel = Lasso(alpha=0.1)  # You can adjust the regularization strength by changing alpha\n\n# Fit the model to the scaled data\nmodel.fit(X_scaled, y_imputed)\n\n# Make predictions for the existing data\npredictions = model.predict(X_scaled)\nprint(\"PREDICTIONS\", predictions)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the Lasso regression model\nlasso = Lasso(alpha=0.1)  # Adjust regularization strength with alpha\n\n# Fit the model on the training data\nlasso.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\nacc = lasso.score(x_test, y_test)\nprint(\"Accuracy:\", acc)\n\n# Print the coefficients and intercept\nprint(\"Coefficients:\", lasso.coef_)\nprint(\"Intercept:\", lasso.intercept_)\n# Print the linear equation\nfeatures_names = features.columns\nlinear_equation = \"G1 = \"\nfor i, feature in enumerate(features_names):\n    coefficient = lasso.coef_[i]\n    linear_equation += f\"({coefficient:.2f}) * {feature} + \"\nlinear_equation += f\"({lasso.intercept_:.2f})\"\nprint(\"Linear Equation:\", linear_equation)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\ndata_encoded = encoder.fit_transform(data)\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = pd.DataFrame(data_encoded, columns=data.columns).drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Create an instance of the Logistic Regression model\nlogistic = LogisticRegression()\n\n# Fit the model on the training data\nlogistic.fit(x_train, y_train)\n\n# Predict the target variable on the test data\ny_pred = logistic.predict(x_test)\n\n# Evaluate the model's accuracy\naccuracy = logistic.score(x_test, y_test)\nprint(\"Test Accuracy:\", accuracy)\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n# ... (continue encoding other columns)\n\n# Define features and target variable\nfeatures = data.drop(\"sex\", axis=1)\ntarget = data[\"sex\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the ElasticNet regression model\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Adjust regularization strength with alpha and mix ratio with l1_ratio\n\n# Fit the model on the training data\nelastic_net.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\nacc = elastic_net.score(x_test, y_test)\nprint(\"Accuracy:\", acc)\n\n# Print the coefficients and intercept\nprint(\"Coefficients:\", elastic_net.coef_)\nprint(\"Intercept:\", elastic_net.intercept_)\n\n# Print the linear equation\nfeatures_names = features.columns\nlinear_equation = \"sex =\"\nfor i, feature in enumerate(features_names):\n    coefficient = elastic_net.coef_[i]\n    linear_equation += f\"({coefficient:.2f}) * {feature} + \"\nlinear_equation += f\"({elastic_net.intercept_:.2f})\"\nprint(\"Linear Equation:\", linear_equation)\nimport pandas as pd\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file into a pandas DataFrame\n\n# Separate the features (input variables) and the target variable\nX = data[['school', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\ny = data[\"sex\"]\n\n# Preprocess and encode all columns using OrdinalEncoder\nencoder = OrdinalEncoder()\nX_encoded = encoder.fit_transform(X)\n\n# Handle missing values in X\nimputer = SimpleImputer(strategy='most_frequent')\nX_imputed = imputer.fit_transform(X_encoded)\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_imputed)\n\n# Encode the target variable y\nencoder_y = OrdinalEncoder()\ny_encoded = encoder_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n\n# Create an instance of the Elastic Net model\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Adjust regularization strength with alpha and mix ratio with l1_ratio\n\n# Fit the model to the scaled data\nelastic_net.fit(X_scaled, y_encoded)\n\n# Make predictions for the existing data\npredictions = elastic_net.predict(X_scaled)\nprint(\"PREDICTIONS:\", predictions)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import ElasticNet\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor column in data.columns:\n    if data[column].dtype == 'object':\n        data[column] = label_encoder.fit_transform(data[column])\n\n# Define features and target variable\nX = data.drop(columns=['sex'])\ny = data['sex']\n\n# Handle missing values in the data\nimputer = SimpleImputer(strategy='most_frequent')\nX_imputed = imputer.fit_transform(X)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.1)\n\n# Create an instance of the ElasticNet model\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Adjust regularization strength with alpha and mix ratio with l1_ratio\n\n# Fit the model on the training data\nelastic_net.fit(x_train, y_train)\n\n# Evaluate the model's accuracy on the training data\ntrain_acc = elastic_net.score(x_train, y_train)\nprint(\"Training Accuracy:\", train_acc)\n\n# Evaluate the model's accuracy on the test data\ntest_acc = elastic_net.score(x_test, y_test)\nprint(\"Test Accuracy:\", test_acc)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Define features and target variable\nfeatures = data_encoded.drop(\"sex\", axis=1)\ntarget = data_encoded[\"sex\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the ElasticNet model\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n\n# Fit the model on the polynomial features\nelastic_net.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = elastic_net.coef_\nintercept = elastic_net.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"sex = \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:.2f}\"\n    else:\n        equation += f\" + {coefficient:.2f} * {feature_names[i]}\"\n\nprint(\"Equation:\", equation)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import ElasticNet\n\n# Read the CSV file\n\n\n# Select relevant columns\ndata = data[['school', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3', 'sex']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Handle missing values\nimputer = SimpleImputer(strategy='mean')\ndata_imputed = imputer.fit_transform(data_encoded)\n\n# Split features and target variable\nX = data_imputed[:, :-1]\ny = data_imputed[:, -1]\n\n# Split data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of ElasticNet model\n# You can adjust the parameters alpha (regularization strength) and l1_ratio (L1 ratio) as needed\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n\n# Train the model\nelastic_net.fit(x_train, y_train)\n\n# Make predictions for training and testing sets\ny_train_pred = elastic_net.predict(x_train)\ny_test_pred = elastic_net.predict(x_test)\n\n# Print the predicted values for training and testing sets\nprint(\"Predicted 'sex' values for training set:\", y_train_pred)\nprint(\"Predicted 'sex' values for test set:\", y_test_pred)\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import ElasticNet\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\n\n# Select relevant columns\ndata = data[['school', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3', 'sex']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Handle missing values\nimputer = SimpleImputer(strategy='mean')\ndata_imputed = imputer.fit_transform(data_encoded)\n\n# Split features and target variable\nX = data_imputed[:, :-1]\ny = data_imputed[:, -1]\n\n# Split data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an instance of ElasticNet model\n# You can adjust the parameters alpha (regularization strength) and l1_ratio (L1 ratio) as needed\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n\n# Train the model\nelastic_net.fit(x_train, y_train)\n\n# Make predictions for training and testing sets\ny_train_pred = elastic_net.predict(x_train)\ny_test_pred = elastic_net.predict(x_test)\n\n# Plot the actual sex values and the predicted sex values\nplt.scatter(y_test, y_test_pred)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.xlabel(\"Actual sex\")\nplt.ylabel(\"Predicted sex\")\nplt.title(\"Elastic Net Regression: Actual vs Predicted sex\")\nplt.show()\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nimport matplotlib.pyplot as plt\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Define features and target variable\nfeatures = data_encoded.drop(\"sex\", axis=1)\ntarget = data_encoded[\"sex\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the ElasticNet model\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Adjust regularization strength with alpha and mix ratio with l1_ratio\n\n# Fit the model on the polynomial features\nelastic_net.fit(x_train_poly, y_train)\n\n# Predict 'sex' values using the trained model\ny_pred = elastic_net.predict(x_test_poly)\n\n# Create scatter plots between predicted sex and other columns\nfor col_idx, col_name in enumerate(features.columns):\n    plt.scatter(x_test_poly[:, col_idx], y_pred, label=\"Predicted sex\")\n    plt.xlabel(col_name)\n    plt.ylabel(\"Predicted sex\")\n    plt.title(f\"Predicted sex vs {col_name}\")\n    plt.legend()\n    plt.show()\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Define features and target variable\nfeatures = data_encoded.drop(\"sex\", axis=1)\ntarget = data_encoded[\"sex\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Scale the features\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\n# Create an instance of the LogisticRegression model with elastic net penalty\nclassifier = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)  # Adjust the l1_ratio as needed\n\n# Fit the model\nclassifier.fit(x_train_scaled, y_train)\n\n# Predict 'sex' values using the trained model\ny_pred = classifier.predict(features_imputed)\n\n# Retrieve the column names\ncolumn_names = features.columns\n\n# Plot predicted 'school' against each column\nfor column in column_names:\n    unique_values = data[column].unique()\n    num_unique = len(unique_values)\n    plt.figure(figsize=(12, 6))\n    for i, value in enumerate(unique_values):\n        plt.subplot(1, num_unique, i+1)\n        plt.bar([0, 1], [np.sum((features[column] == value) & (y_pred == 0)),\n                         np.sum((features[column] == value) & (y_pred == 1))], color=['blue', 'red'])\n        plt.xlabel(\"sex\")\n        plt.ylabel(\"Count\")\n        plt.title(f\"{column} = {value}\")\n        plt.xticks([0, 1], label_encoder.inverse_transform([0, 1]))  # Use inverse_transform to get the original labels\n    plt.tight_layout()\n    plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SPARSE-REG","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nimport numpy as np\n# Read the CSV file\nfile_path = \"/kaggle/input/pupil-csv/pupil-mat.csv\"\ndata = pd.read_csv(file_path,encoding='latin1')\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define columns to encode\ncolumns_to_encode = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Display the transformed dataframe\nprint(data.head())\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = regressor.coef_\nintercept = regressor.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"G1= \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:f}\"\n    else:\n        equation += f\" + {coefficient:f} * {feature_names[i]}\"\nprint('coefficient',coefficients)\nprint('intercept',intercept)\nprint(\"Equation:\", equation)\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n# Read the CSV file and select desired columns\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n# Define features and target variable\n\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LinearRegression model\nlinear = linear_model.LinearRegression()\n\n# Fit the model on the training data\nlinear.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\nacc = linear.score(x_test, y_test)\nprint(\"Accuracy:\", acc)\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the Linear Regression model\nmodel = LinearRegression()\n\n# Fit the model on the training data\nmodel.fit(x_train, y_train)\n\n# Evaluate the model's accuracy on training and testing data\ntrain_accuracy = model.score(x_train, y_train)\ntest_accuracy = model.score(x_test, y_test)\n\nprint(\"Train Accuracy:\", train_accuracy)\nprint(\"Test Accuracy:\", test_accuracy)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n# Define features and target variable\nfeatures = data_encoded.drop(\"G1\", axis=1)\ntarget = data_encoded[\"G1\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = regressor.coef_\nintercept = regressor.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"G1= \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:.2f}\"\n    else:\n        equation += f\" + {coefficient:.2f} * {feature_names[i]}\"\n\nprint(\"Equation:\", equation)\n\n\n# Predict 'G1' values using the trained model\ny_train_pred = regressor.predict(x_train_poly)\ny_test_pred = regressor.predict(x_test_poly)\n\n# Print the predicted 'G1' values\nprint(\"Predicted 'G1' values for training set:\", y_train_pred)\nprint(\"Predicted 'G1' values for test set:\", y_test_pred)\n# Predict on the test data\nimport matplotlib.pyplot as plt\ny_pred = regressor.predict(x_test_poly)\n\n# Plot the actual G1 values and the predicted G1 values\nplt.scatter(y_test, y_pred)\nplt.plot([np.min(x_test), np.max(x_test)], [np.min(y_test), np.max(y_test)], color='red', linestyle='--')\nplt.xlabel(\"Actual G1\")\nplt.ylabel(\"Predicted G1\")\nplt.title(\"Polynomial Regression: Actual vs Predicted G1\")\nplt.show()\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Retrieve the column names\ncolumn_names = data_encoded.columns\n\n# Plot 'G1' against each column\nfor column in column_names:\n    if column != 'G1':\n        plt.scatter(data_encoded[column], data_encoded['G1'])\n        plt.xlabel(column)\n        plt.ylabel('G1')\n        plt.title(f'G1 vs {column}')\n        plt.show()\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file into a pandas DataFrame\n\n\n# Separate the features (input variables) and the target variable\nX = data[['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob','Fjob','reason','guardian','traveltime','studytime','failures','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic','famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2']]\ny = data[\"G1\"]\n\n# Handle missing values in y\nimputer = SimpleImputer(strategy='mean')\ny_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).flatten()\n\n# Preprocess and encode non-numeric columns in X using LabelEncoder\nnon_numeric_cols = ['school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic']\nencoder = LabelEncoder()\nX_encoded = X.copy()\nfor col in non_numeric_cols:\n    X_encoded[col] = encoder.fit_transform(X[col])\n\n# Handle missing values in X\nX_imputed = imputer.fit_transform(X_encoded)\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_imputed)\n\n# Create an instance of the Linear Regression model\nmodel = LinearRegression()\n\n# Fit the model to the scaled data\nmodel.fit(X_scaled, y_imputed)\n\n# Make predictions for the existing data\npredictions = model.predict(X_scaled)\nprint(\"PREDICTIONS\", predictions)\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n# ... (continue encoding other columns)\n\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LinearRegression model\nlinear = linear_model.LinearRegression()\n\n# Fit the model on the training data\nlinear.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\nacc = linear.score(x_test, y_test)\nprint(\"Accuracy:\", acc)\n\n# Print the coefficients and intercept\nprint(\"Coefficients:\", linear.coef_)\nprint(\"Intercept:\", linear.intercept_)\n\n# Print the linear equation\nfeatures_names = features.columns\nlinear_equation = \"G1 = \"\nfor i, feature in enumerate(features_names):\n    coefficient = linear.coef_[i]\n    linear_equation += f\"({coefficient:.2f}) * {feature} + \"\nlinear_equation += f\"({linear.intercept_:.2f})\"\nprint(\"Linear Equation:\", linear_equation)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n\n# Define features and target variable\nfeatures = data.drop(\"G1\", axis=1)\ntarget = data[\"G1\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Predict 'G1' values using the trained model\ny_train_pred = regressor.predict(x_train_poly)\ny_test_pred = regressor.predict(x_test_poly)\n\n# Retrieve the column names\ncolumn_names = features.columns\n\n# Plot predicted 'G1' against each column\nfor column_index, column in enumerate(column_names):\n    # Scatter plot for training data\n    plt.scatter(x_train[:, column_index], y_train_pred, color='blue', label='Training Data')\n\n    # Scatter plot for testing data\n    plt.scatter(x_test[:, column_index], y_test_pred, color='red', label='Testing Data')\n\n    plt.xlabel(column)\n    plt.ylabel('Predicted G1')\n    plt.title(f'Predicted G1 vs {column}')\n    plt.legend()\n    plt.show()\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\n\n\n# Specify the column name for comparison\ncolumn_name = \"G1\"\n\n# Get the values from the specified column\ncolumn_values = data[column_name]\n\n# Calculate the percentage values based on the maximum value in the column\nmax_value = column_values.max()\npercentage_values = (column_values / max_value) * 100\n\n# Create a line plot to compare the percentage values across rows\nplt.figure(figsize=(10, 6))\nplt.plot(percentage_values, marker='o')\nplt.xlabel(\"Row Index\")\nplt.ylabel(f\"{column_name} Percentage\")\nplt.title(f\"Comparison of {column_name} Percentage Across Rows\")\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n# ... (continue encoding other columns)\n\n# Define features and target variable\nfeatures = data.drop(\"sex\", axis=1)\ntarget = data[\"sex\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LinearRegression model\nlinear = linear_model.LinearRegression()\n\n# Fit the model on the training data\nlinear.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\nacc = linear.score(x_test, y_test)\nprint(\"Accuracy:\", acc)\n\n# Print the coefficients and intercept\nprint(\"Coefficients:\", linear.coef_)\nprint(\"Intercept:\", linear.intercept_)\n\n# Print the linear equation\nfeatures_names = features.columns\nlinear_equation = \"sex =\"\nfor i, feature in enumerate(features_names):\n    coefficient = linear.coef_[i]\n    linear_equation += f\"({coefficient:.2f}) * {feature} + \"\nlinear_equation += f\"({linear.intercept_:.2f})\"\nprint(\"Linear Equation:\", linear_equation)\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file into a pandas DataFrame\n\n\n# Separate the features (input variables) and the target variable\nX = data[['school', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\ny = data[\"sex\"]\n\n# Preprocess and encode all columns using OrdinalEncoder\nencoder = OrdinalEncoder()\nX_encoded = encoder.fit_transform(X)\n\n# Handle missing values in X\nimputer = SimpleImputer(strategy='most_frequent')\nX_imputed = imputer.fit_transform(X_encoded)\n\n# Scale the features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_imputed)\n\n# Encode the target variable y\nencoder_y = OrdinalEncoder()\ny_encoded = encoder_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n\n# Create an instance of the Linear Regression model\nmodel = LinearRegression()\n\n# Fit the model to the scaled data\nmodel.fit(X_scaled, y_encoded)\n\n# Make predictions for the existing data\npredictions = model.predict(X_scaled)\nprint(\"PREDICTIONS:\", predictions)\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Convert non-numerical values to numerical using LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n\n# Define features and target variable\nfeatures = data.drop(\"sex\", axis=1)\ntarget = data[\"sex\"]\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\n\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LinearRegression model\nlinear = linear_model.LinearRegression()\n\n# Fit the model on the training data\nlinear.fit(x_train, y_train)\n\n# Evaluate the model's accuracy on the training data\ntrain_acc = linear.score(x_train, y_train)\nprint(\"Training Accuracy:\", train_acc)\n\n# Evaluate the model's accuracy on the test data\ntest_acc = linear.score(x_test, y_test)\nprint(\"Test Accuracy:\", test_acc)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\nlabel_encoder = LabelEncoder()\ndata[\"school\"] = label_encoder.fit_transform(data[\"school\"])\ndata[\"sex\"] = label_encoder.fit_transform(data[\"sex\"])\ndata[\"address\"] = label_encoder.fit_transform(data[\"address\"])\ndata[\"famsize\"] = label_encoder.fit_transform(data[\"famsize\"])\ndata[\"Pstatus\"] = label_encoder.fit_transform(data[\"Pstatus\"])\ndata[\"Mjob\"] = label_encoder.fit_transform(data[\"Mjob\"])\ndata[\"Fjob\"] = label_encoder.fit_transform(data[\"Fjob\"])\ndata[\"reason\"] = label_encoder.fit_transform(data[\"reason\"])\ndata[\"guardian\"] = label_encoder.fit_transform(data[\"guardian\"])\ndata[\"schoolsup\"] = label_encoder.fit_transform(data[\"schoolsup\"])\ndata[\"famsup\"] = label_encoder.fit_transform(data[\"famsup\"])\ndata[\"paid\"] = label_encoder.fit_transform(data[\"paid\"])\ndata[\"activities\"] = label_encoder.fit_transform(data[\"activities\"])\ndata[\"nursery\"] = label_encoder.fit_transform(data[\"nursery\"])\ndata[\"higher\"] = label_encoder.fit_transform(data[\"higher\"])\ndata[\"internet\"] = label_encoder.fit_transform(data[\"internet\"])\ndata[\"romantic\"] = label_encoder.fit_transform(data[\"romantic\"])\n# Define features and target variable\nfeatures = data_encoded.drop(\"sex\", axis=1)\ntarget = data_encoded[\"sex\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Apply polynomial features transformation\npoly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n\n# Create an instance of the LinearRegression model\nregressor = LinearRegression()\n\n# Fit the model on the polynomial features\nregressor.fit(x_train_poly, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = regressor.coef_\nintercept = regressor.intercept_\n\n# Retrieve the original feature names\noriginal_feature_names = features.columns\n\n# Generate the polynomial feature names\nfeature_names = list(original_feature_names)\nfor feature_idx in poly.powers_:\n    if np.sum(feature_idx) > 1:\n        feature_name = \"*\".join(\n            [\n                f\"{name}^{power}\"\n                for name, power in zip(original_feature_names, feature_idx)\n                if power > 0\n            ]\n        )\n        feature_names.append(feature_name)\n\n# Create the equation\nequation = \"sex = \"\nfor i, coefficient in enumerate(coefficients):\n    if i == 0:\n        equation += f\"{intercept:.2f}\"\n    else:\n        equation += f\" + {coefficient:.2f} * {feature_names[i]}\"\n\nprint(\"Equation:\", equation)\n# Predict 'sex' values using the trained model\ny_train_pred = regressor.predict(x_train_poly)\ny_test_pred = regressor.predict(x_test_poly)\n\n# Print the predicted 'G1' values\nprint(\"Predicted 'sex' values for training set:\", y_train_pred)\nprint(\"Predicted 'sex' values for test set:\", y_test_pred)\n# Predict on the test data\ny_pred = regressor.predict(x_test_poly)\n\n# Plot the actual sex values and the predicted sex values\nplt.scatter(y_test, y_pred)\nplt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], color='red', linestyle='--')\nplt.xlabel(\"Actualsex\")\nplt.ylabel(\"Predicted sex\")\nplt.title(\"Polynomial Regression: Actual vs Predicted sex\")\nplt.show()\n# Create scatter plots between predicted sex and other columns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nimport matplotlib.pyplot as plt\n\n# ... (previous code remains the same)\n\n# Predict on the test data\ny_pred = regressor.predict(x_test_poly)\n\n# Create scatter plots between predicted sex and other columns\nfor col_idx, col_name in enumerate(features.columns):\n    plt.scatter(x_test_poly[:, col_idx], y_pred, label=\"Predicted sex\")\n    plt.xlabel(col_name)\n    plt.ylabel(\"Predicted sex\")\n    plt.title(f\"Predicted sex vs {col_name}\")\n    plt.legend()\n    plt.show()\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\n\ndata = data[['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata_encoded = data.apply(label_encoder.fit_transform)\n\n# Define features and target variable\nfeatures = data_encoded.drop(\"sex\", axis=1)\ntarget = data_encoded[\"sex\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='mean')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.1)\n\n# Create an instance of the LogisticRegression model\nclassifier = LogisticRegression()\n\n# Fit the model\nclassifier.fit(x_train, y_train)\n\n# Predict 'sex' values using the trained model\ny_train_pred = classifier.predict(features_imputed)\n\n# Retrieve the column names\ncolumn_names = features.columns\n\n# Plot predicted 'school' against each column\nfor column in column_names:\n    unique_values = data[column].unique()\n    num_unique = len(unique_values)\n    plt.figure(figsize=(12, 6))\n    for i, value in enumerate(unique_values):\n        plt.subplot(1, num_unique, i+1)\n        plt.bar([0, 1], [np.sum((features[column] == value) & (y_train_pred == 0)),\n                         np.sum((features[column] == value) & (y_train_pred == 1))], color=['blue', 'red'])\n        plt.xlabel(\"sex\")\n        plt.ylabel(\"Count\")\n        plt.title(f\"{column} = {value}\")\n        plt.xticks([0, 1], label_encoder.inverse_transform([0, 1]))  # Use inverse_transform to get the original labels\n    plt.tight_layout()\n    plt.show()\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\n\n# Specify the column name for comparison\ncolumn_name = \"sex\"\n\n# Get the value counts from the specified column\nvalue_counts = data[column_name].value_counts()\n\n# Create a bar plot to compare the distribution of values across rows\nplt.figure(figsize=(10, 6))\nvalue_counts.plot(kind='bar')\nplt.xlabel(\"sex\")\nplt.ylabel(\"Count\")\nplt.title(f\"Distribution of {column_name} Across Rows\")\nplt.tight_layout()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LINEAR-REG","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Read the CSV file and select desired columns\nfile_path = \"/kaggle/input/landing-aircraft/Landing.csv\"\ndata = pd.read_csv(file_path, encoding='latin1')\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']):\n    data[column] = label_encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"JOINT_USE\", axis=1)\ntarget = data[\"JOINT_USE\"]\n\n\n# Apply OrdinalEncoder to encode categorical variables\nencoder = OrdinalEncoder()\nfeatures_encoded = encoder.fit_transform(features)\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Read the CSV file\n\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['index','X','Y','FID','SITE_NO','LAN_FA_TY','LOCID','EFF_DATE','FAA_REGION','FAA_DISTRI','ST_POSTAL','STFIPS','FAA_ST','STATE_NAME','COUNTY_NAM','COUNTY_ST','CITY_NAME','FULLNAME','OWNER_TYPE','FAC_USE','FAC_CYSTZP','LATITUDE','LONGITUDE','ELEV','AERO_CHART','CBD_DIST','CBD_DIR','ACT_DATE','CERT_TYPE','FED_AGREE','INTERNATIO','CUST_LNDG','JOINT_USE','MIL_LNDG_R','CNTL_TWR','S_ENG_GA','M_ENG_GA','JET_EN_GA','HELICOPTER','OPER_GLIDE','OPER_MIL','ULTRALIGHT','COMM_SERV','AIR_TAXI','LOCAL_OPS','ITIN_OPS','MIL_OPS','Arrivals','Departures','Enplanemen','Passengers']\n\n# Initialize OrdinalEncoder\nencoder = OrdinalEncoder()\n\n# Encode categorical variables\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\n\n# Define features and target variable\nfeatures = data.drop(\"JOINT_USE\", axis=1)\ntarget = data[\"JOINT_USE\"]\n\n# Display the transformed dataframe\nprint(data.head())\n# Handle missing values in the encoded features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features_encoded)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LogisticRegression model\nlogistic_reg = LogisticRegression()\n\n# Fit the model on the training data\nlogistic_reg.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\naccuracy = logistic_reg.score(x_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Retrieve the coefficients and intercept\ncoefficients = logistic_reg.coef_[0].tolist()\nintercept = logistic_reg.intercept_[0]\n\n# Print the coefficients and intercept\nprint(\"Coefficients:\", coefficients)\nprint(\"Intercept:\", intercept)\nimport numpy as np\n\n# Define the logistic equation function\ndef logistic_equation(features, coefficients, intercept):\n    z = np.dot(features, coefficients) + intercept\n    return 1 / (1 + np.exp(-z))\n\n# Create an instance of the LogisticRegression model\nlogistic_reg = LogisticRegression()\n\n# Fit the model on the training data\nlogistic_reg.fit(x_train, y_train)\n\n# Retrieve the coefficients and intercept\ncoefficients = logistic_reg.coef_[0]\nintercept = logistic_reg.intercept_[0]\n\n# Extract column names of the features\nfeature_names = data.drop(\"JOINT_USE\", axis=1).columns\n\n# Construct the logistic equation string\nlogistic_eq_str = f\"P(JOINT_USE=1) = 1 / (1 + e^(-({intercept}\"\nfor feature, coef in zip(feature_names, coefficients):\n    logistic_eq_str += f\" + {coef:.4f}*{feature}\"\nlogistic_eq_str += \"))\"\n\nprint(\"Logistic Equation:\")\nprint(logistic_eq_str)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LOGISTIC","metadata":{}},{"cell_type":"code","source":"# @title Default title text ROLLING WINDOWS\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\nfile_name='/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Standardize the data\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features_imputed)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_scaled, target_imputed, test_size=0.1, random_state=42)\n\n# Build the ANN model\nmodel = keras.Sequential([\n    layers.Input(shape=(x_train.shape[1],)),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1)\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n\n# Train the model\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=20, batch_size=64, verbose=1)\n\n# Predict the entire target variable\ny_pred = model.predict(features_scaled).flatten()\n\n# Rolling window setup\nwindow_size = 100  # Define your rolling window size\nstep_size = 10  # Define the step size for the rolling window\nnum_windows = (len(features_scaled) - window_size) // step_size + 1\n\n# Prepare a list to store rolling window predictions\nrolling_predictions = []\n\n# Apply rolling window on the predictions\nfor start in range(0, num_windows * step_size, step_size):\n    end = start + window_size\n    if end > len(y_pred):\n        break\n    window_pred = y_pred[start:end]\n    rolling_predictions.append(window_pred.mean())\n\n# Convert rolling predictions to numpy array for evaluation\nrolling_predictions = np.array(rolling_predictions)\ntrue_values = target_imputed[window_size // 2: len(rolling_predictions) + window_size // 2]\n\n# Evaluate the overall predictions\nmse = mean_squared_error(true_values, rolling_predictions)\nr2 = r2_score(true_values, rolling_predictions)\n\nprint(f\"Rolling Window ANN Mean Squared Error: {mse}\")\nprint(f\"Rolling Window ANN R-squared Score: {r2}\")\n\n# Visualize the true values and rolling predictions\nplt.figure(figsize=(12, 6))\nplt.plot(true_values, label='True Values')\nplt.plot(rolling_predictions, label='Rolling Predictions', alpha=0.7)\nplt.legend()\nplt.title('Rolling Window Predictions')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ROLLING-WINDOWS","metadata":{}},{"cell_type":"code","source":"# @title  RANDOM-FOREST\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\nfile_name='/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Standardize the data\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features_imputed)\n\n# Convert target to binary (0 and 1)\ntarget_binary = (target > 0).astype(int)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_scaled, target_binary, test_size=0.1, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(x_train, y_train)\n\n# Get feature importances\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = features.columns\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\nfor f in range(x_train.shape[1]):\n    print(f\"{f + 1}. feature {indices[f]} ({importances[indices[f]]}) - {feature_names[indices[f]]}\")\n\n# Plot the feature importances\nplt.figure(figsize=(15, 10))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), feature_names[indices], rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()\n\n# Visualize the relationship between the most important features and the target\nimportant_features = feature_names[indices[:5]]  # Top 5 important features\n\nfor feature in important_features:\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=data[feature], y=target, alpha=0.5)\n    plt.title(f'{feature} vs Aircraft Damage')\n    plt.xlabel(feature)\n    plt.ylabel('Aircraft Damage')\n    plt.show()\n\n# Calculate the correlation matrix\ncorr_matrix = data.corr()\n\n# Plot the correlation matrix\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n# Read the CSV file\nfile_name='/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Standardize the data\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features_imputed)\n\n# Convert target to binary (0 and 1)\ntarget_binary = (target > 0).astype(int)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_scaled, target_binary, test_size=0.1, random_state=42)\n\n# Define the flag and pennant detection function\ndef detect_patterns(data, window_size=20):\n    patterns = []\n    for i in range(len(data) - window_size):\n        window = data[i:i+window_size]\n        # Simple heuristic for flag and pennant pattern detection\n        if max(window) - min(window) > np.std(window) * 2:  # Example condition for strong movement\n            patterns.append(1)  # Flag or pennant detected\n        else:\n            patterns.append(0)  # No pattern detected\n    patterns += [0] * window_size  # Append zeros for the remaining part\n    return np.array(patterns)\n\n# Apply the pattern detection to each feature\npattern_features = np.apply_along_axis(detect_patterns, 0, features_scaled)\n\n# Train a classifier to predict patterns\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(x_train, y_train)\n\n# Predict patterns on the test set\ny_pred = clf.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Pattern Prediction Accuracy: {accuracy}\")\n\n# Visualize the detected patterns for one of the features\nplt.figure(figsize=(12, 6))\nplt.plot(range(len(features_scaled)), features_scaled[:, 0], label='Feature 0')\nplt.plot(range(len(features_scaled)), pattern_features[:, 0], label='Detected Patterns', linestyle='--', color='red')\nplt.legend()\nplt.title('Detected Flag and Pennant Patterns for Feature 0')\nplt.show()\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the CSV file\nfile_name = '/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass', 'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position', 'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport', 'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation', 'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity', 'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike', 'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike', 'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage', 'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage', 'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike', 'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike', 'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike', 'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Standardize the data\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features_imputed)\n\n# Convert target to binary (0 and 1)\ntarget_binary = (target > 0).astype(int)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_scaled, target_binary, test_size=0.1, random_state=42)\n\n# Train a Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(x_train, y_train)\n\n# Get feature importances\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = features.columns\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\nfor f in range(x_train.shape[1]):\n    print(f\"{f + 1}. feature {indices[f]} ({importances[indices[f]]}) - {feature_names[indices[f]]}\")\n\n# Plot the feature importances\nplt.figure(figsize=(15, 10))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), feature_names[indices], rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()\n\n# Visualize the relationship between the most important features and the target\nimportant_features = feature_names[indices[:5]]  # Top 5 important features\n\nfor feature in important_features:\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=data[feature], y=target, alpha=0.5)\n    plt.title(f'{feature} vs Aircraft Damage')\n    plt.xlabel(feature)\n    plt.ylabel('Aircraft Damage')\n    plt.show()\n\n# Calculate the correlation matrix\ncorr_matrix = data.corr()\n\n# Print the correlation matrix as text\nprint(\"Correlation Matrix:\")\nprint(corr_matrix)\n\n# Plot the correlation matrix\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"RANDOM-FOREST","metadata":{}},{"cell_type":"code","source":"# @title  POWER-LAW\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\n\n# Read the CSV file\nfile_path = \"/kaggle/input/air-damage/DAMAGE.csv\"\ndata = pd.read_csv(file_path, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day',\n                     'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type',\n                     'Aircraft Make', 'Aircraft Model', 'Aircraft Mass',\n                     'Engine Make', 'Engine Model', 'Engines', 'Engine Type',\n                     'Engine1 Position', 'Engine2 Position', 'Engine3 Position',\n                     'Engine4 Position', 'Airport ID', 'Airport', 'State',\n                     'FAA Region', 'Warning Issued', 'Flight Phase',\n                     'Visibility', 'Precipitation', 'Height', 'Speed',\n                     'Distance', 'Species ID', 'Species Name',\n                     'Species Quantity', 'Flight Impact', 'Fatalities',\n                     'Injuries', 'Radome Strike', 'Windshield Strike',\n                     'Nose Strike', 'Engine1 Strike', 'Engine2 Strike',\n                     'Engine3 Strike', 'Engine4 Strike', 'Engine Ingested',\n                     'Propeller Strike', 'Wing or Rotor Strike',\n                     'Fuselage Strike', 'Landing Gear Strike',\n                     'Tail Strike', 'Lights Strike', 'Other Strike']\n\n# Initialize OrdinalEncoder and encode categorical variables\nencoder = OrdinalEncoder()\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LogisticRegression model\nlogistic_reg = LogisticRegression()\n\n# Fit the model on the training data\nlogistic_reg.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\naccuracy = logistic_reg.score(x_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Retrieve the coefficients and intercept\ncoefficients = logistic_reg.coef_[0]\nintercept = logistic_reg.intercept_[0]\n\n# Extract column names of the features\nfeature_names = data.drop(\"Aircraft Damage\", axis=1).columns\n\n# Construct the logistic equation string\nlogistic_eq_str = f\"P(Aircraft Damage=1) = 1 / (1 + e^(-({intercept:.4f}\"\nfor feature, coef in zip(feature_names, coefficients):\n    logistic_eq_str += f\" + {coef:.4f}*{feature}\"\nlogistic_eq_str += \"))\"\n\nprint(\"Logistic Equation:\")\nprint(logistic_eq_str)\n\n# Visualizing Distribution of the target variable\nplt.figure(figsize=(10, 6))\nsns.histplot(target_imputed, bins=50, kde=True, stat=\"density\")\nplt.title('Distribution of Aircraft Damage')\nplt.xlabel('Aircraft Damage')\nplt.ylabel('Density')\nplt.show()\n\n# Checking for power law distribution\ndef plot_power_law(data, title='Power Law Distribution'):\n    # Drop NaN values\n    data = data[~np.isnan(data)]\n\n    if len(data) == 0:  # Check if there's still data after dropping NaNs\n        print(\"No valid data available for plotting.\")\n        return\n\n    # Calculate the empirical frequency distribution\n    counts, bin_edges = np.histogram(data, bins=50, density=True)\n\n    # Calculate the cumulative distribution\n    cumulative_counts = np.cumsum(counts[::-1])[::-1]\n\n    # Plot the log-log plot\n    plt.figure(figsize=(10, 6))\n    plt.loglog(bin_edges[1:], cumulative_counts, marker='o', linestyle='None')\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Cumulative Frequency')\n    plt.grid(True)\n    plt.show()\n\n# Check the distribution of Aircraft Damage for power law behavior\nplot_power_law(target_imputed, title='Cumulative Distribution of Aircraft Damage')\n\n# Optionally, perform additional analysis on features for power law behavior\nfor column in features.columns:\n    feature_data = features[column]\n    plot_power_law(feature_data, title=f'Cumulative Distribution of {column}')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\n\n# Load CSV\ndata = pd.read_csv('E:/URP-SHIT/transportation/flights_sample_3m.csv/flights_sample_3m_modified-2.csv', encoding='latin1')\n\n# Convert non-numeric columns to strings, then encode\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n    data[column] = LabelEncoder().fit_transform(data[column])\n\n# Select numeric columns that may follow power-law and transform them\npowerlaw_candidates = ['DISTANCE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_LATE_AIRCRAFT',\n                       'DELAY_DUE_NAS', 'DELAY_DUE_WEATHER', 'AIR_TIME']\n\nfor col in powerlaw_candidates:\n    if col in data.columns:\n        # Apply log1p to handle 0s safely\n        data[f'log_{col}'] = np.log1p(data[col])\n        # Optionally drop original if desired:\n        data.drop(columns=col, inplace=True)\n\n# Define features and target\nfeatures = data.drop(\"CANCELLED\", axis=1)\ntarget = data[\"CANCELLED\"]\n\n# Impute missing values\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features)\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split dataset\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1, random_state=42)\n\n# Train logistic regression\nlogistic_reg = LogisticRegression(max_iter=1000)\nlogistic_reg.fit(x_train, y_train)\n\n# Accuracy\naccuracy = logistic_reg.score(x_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Coefficients and equation\ncoefficients = logistic_reg.coef_[0]\nintercept = logistic_reg.intercept_[0]\nfeature_names = features.columns\n\nlogistic_eq_str = f\"P(CANCELLED=1) = 1 / (1 + e^(-({intercept:.4f}\"\nfor feature, coef in zip(feature_names, coefficients):\n    logistic_eq_str += f\" + {coef:.4f}*{feature}\"\nlogistic_eq_str += \")))\"\n\nprint(\"\\nLogistic Equation:\")\nprint(logistic_eq_str)\n\n# 🔍 Distribution Plot\nplt.figure(figsize=(10, 6))\nsns.histplot(target_imputed, bins=50, kde=True, stat=\"density\")\nplt.title('Distribution of CANCELLED')\nplt.xlabel('CANCELLED')\nplt.ylabel('Density')\nplt.show()\n\n# 🔍 Power Law Visualizations\ndef plot_power_law(data, title='Power Law Distribution'):\n    data = data[~np.isnan(data)]\n    if len(data) == 0:\n        print(\"No valid data available for plotting.\")\n        return\n    counts, bin_edges = np.histogram(data, bins=50, density=True)\n    cumulative_counts = np.cumsum(counts[::-1])[::-1]\n    plt.figure(figsize=(10, 6))\n    plt.loglog(bin_edges[1:], cumulative_counts, marker='o', linestyle='None')\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Cumulative Frequency')\n    plt.grid(True)\n    plt.show()\n\n# Plot transformed and original variables\nfor col in powerlaw_candidates:\n    if f'log_{col}' in data.columns:\n        plot_power_law(data[f'log_{col}'], f'Log-Transformed Power Law: {col}')\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"POWER-LAW","metadata":{}},{"cell_type":"code","source":"# @title  LONG-TAIL\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\n\n# Read the CSV file\nfile_path = \"/kaggle/input/air-damage/DAMAGE.csv\"\ndata = pd.read_csv(file_path, encoding='latin1')\n\n# Convert non-numeric columns to categorical type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day',\n                     'Operator ID', 'Operator', 'Aircraft', 'Aircraft Type',\n                     'Aircraft Make', 'Aircraft Model', 'Aircraft Mass',\n                     'Engine Make', 'Engine Model', 'Engines', 'Engine Type',\n                     'Engine1 Position', 'Engine2 Position', 'Engine3 Position',\n                     'Engine4 Position', 'Airport ID', 'Airport', 'State',\n                     'FAA Region', 'Warning Issued', 'Flight Phase',\n                     'Visibility', 'Precipitation', 'Height', 'Speed',\n                     'Distance', 'Species ID', 'Species Name',\n                     'Species Quantity', 'Flight Impact', 'Fatalities',\n                     'Injuries', 'Radome Strike', 'Windshield Strike',\n                     'Nose Strike', 'Engine1 Strike', 'Engine2 Strike',\n                     'Engine3 Strike', 'Engine4 Strike', 'Engine Ingested',\n                     'Propeller Strike', 'Wing or Rotor Strike',\n                     'Fuselage Strike', 'Landing Gear Strike',\n                     'Tail Strike', 'Lights Strike', 'Other Strike']\n\n# Initialize OrdinalEncoder and encode categorical variables\nencoder = OrdinalEncoder()\ndata[columns_to_encode] = encoder.fit_transform(data[columns_to_encode])\n\n# Define features and target variable\nfeatures = data.drop(\"Aircraft Damage\", axis=1)\ntarget = data[\"Aircraft Damage\"]\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='most_frequent')\nfeatures_imputed = imputer.fit_transform(features)\n\n# Handle missing values in the target variable\ntarget_imputed = imputer.fit_transform(target.values.reshape(-1, 1)).flatten()\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_imputed, test_size=0.1)\n\n# Create an instance of the LogisticRegression model\nlogistic_reg = LogisticRegression()\n\n# Fit the model on the training data\nlogistic_reg.fit(x_train, y_train)\n\n# Evaluate the model's accuracy\naccuracy = logistic_reg.score(x_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Retrieve the coefficients and intercept\ncoefficients = logistic_reg.coef_[0]\nintercept = logistic_reg.intercept_[0]\n\n# Extract column names of the features\nfeature_names = data.drop(\"Aircraft Damage\", axis=1).columns\n\n# Construct the logistic equation string\nlogistic_eq_str = f\"P(Aircraft Damage=1) = 1 / (1 + e^(-({intercept:.4f}\"\nfor feature, coef in zip(feature_names, coefficients):\n    logistic_eq_str += f\" + {coef:.4f}*{feature}\"\nlogistic_eq_str += \"))\"\n\nprint(\"Logistic Equation:\")\nprint(logistic_eq_str)\n\n# Visualizing Distribution of the target variable\nplt.figure(figsize=(10, 6))\nsns.histplot(target_imputed, bins=50, kde=True, stat=\"density\")\nplt.title('Distribution of Aircraft Damage')\nplt.xlabel('Aircraft Damage')\nplt.ylabel('Density')\nplt.show()\n\n# Checking for power law distribution\ndef plot_power_law(data, title='Power Law Distribution'):\n    # Drop NaN values\n    data = data[~np.isnan(data)]\n\n    if len(data) == 0:  # Check if there's still data after dropping NaNs\n        print(\"No valid data available for plotting.\")\n        return\n\n    # Calculate the empirical frequency distribution\n    counts, bin_edges = np.histogram(data, bins=50, density=True)\n\n    # Calculate the cumulative distribution\n    cumulative_counts = np.cumsum(counts[::-1])[::-1]\n\n    # Plot the log-log plot\n    plt.figure(figsize=(10, 6))\n    plt.loglog(bin_edges[1:], cumulative_counts, marker='o', linestyle='None')\n    plt.title(title)\n    plt.xlabel('Value')\n    plt.ylabel('Cumulative Frequency')\n    plt.grid(True)\n    plt.show()\n\n# Check the distribution of Aircraft Damage for power law behavior\nplot_power_law(target_imputed, title='Cumulative Distribution of Aircraft Damage')\n\n# Optionally, perform additional analysis on features for power law behavior\nfor column in features.columns:\n    feature_data = features[column]\n    plot_power_law(feature_data, title=f'Cumulative Distribution of {column}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LONG-TAIL","metadata":{}},{"cell_type":"code","source":"# @title HIERARCHICAL\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\nfile_name = '/kaggle/input/air-damage/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator',\n                     'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass',\n                     'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position',\n                     'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport',\n                     'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation',\n                     'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity',\n                     'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike',\n                     'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike',\n                     'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage',\n                     'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage',\n                     'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike',\n                     'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike',\n                     'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike',\n                     'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define levels based on thresholds\ndata['Damage Level'] = pd.cut(data['Aircraft Damage'],\n                              bins=[-1, 0, 2, 4, 6, 8, 10],\n                              labels=[0, 1, 2, 3, 4, 5])\n\n# Define features and target variable\nfeatures = data.drop([\"Aircraft Damage\", \"Damage Level\"], axis=1)\ntarget = data[\"Damage Level\"].astype(int)\n\n# Sample a subset of the data to reduce memory usage\nsampled_data = data.sample(frac=0.1, random_state=42)\nfeatures_sampled = sampled_data.drop([\"Aircraft Damage\", \"Damage Level\"], axis=1)\ntarget_sampled = sampled_data[\"Damage Level\"].astype(int)\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features_sampled)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_sampled, test_size=0.1, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n# Build the ANN model\nmodel = keras.Sequential([\n    layers.Input(shape=(x_train.shape[1],)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(6, activation='softmax')  # Predicting hierarchical classes\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=20, batch_size=64, verbose=1)\n\n# Evaluate the model\ny_pred_probs = model.predict(x_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"ANN Accuracy: {accuracy}\")\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# Plot training history\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\n# Print hierarchical structure\nprint(\"\\nHierarchical Structure of Damage Levels:\")\nprint(\"Level 0: No Damage\")\nprint(\"Level 1: Minor Damage\")\nprint(\"Level 2: Moderate Damage\")\nprint(\"Level 3: Significant Damage\")\nprint(\"Level 4: Severe Damage\")\nprint(\"Level 5: Catastrophic Damage\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"HIERARCHICAL","metadata":{}},{"cell_type":"code","source":"# @title  BIOFURCATIONS\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\n\n# Read the CSV file\nfile_name = '/content/DAMAGE.csv'\ndata = pd.read_csv(file_name, encoding='latin1')\n\n# Convert non-numeric columns to string type\nfor column in data.select_dtypes(include=['object']):\n    data[column] = data[column].astype(str)\n\n# Define columns to encode\ncolumns_to_encode = ['Record ID', 'Incident Year', 'Incident Month', 'Incident Day', 'Operator ID', 'Operator',\n                     'Aircraft', 'Aircraft Type', 'Aircraft Make', 'Aircraft Model', 'Aircraft Mass',\n                     'Engine Make', 'Engine Model', 'Engines', 'Engine Type', 'Engine1 Position',\n                     'Engine2 Position', 'Engine3 Position', 'Engine4 Position', 'Airport ID', 'Airport',\n                     'State', 'FAA Region', 'Warning Issued', 'Flight Phase', 'Visibility', 'Precipitation',\n                     'Height', 'Speed', 'Distance', 'Species ID', 'Species Name', 'Species Quantity',\n                     'Flight Impact', 'Fatalities', 'Injuries', 'Aircraft Damage', 'Radome Strike',\n                     'Radome Damage', 'Windshield Strike', 'Windshield Damage', 'Nose Strike',\n                     'Nose Damage', 'Engine1 Strike', 'Engine1 Damage', 'Engine2 Strike', 'Engine2 Damage',\n                     'Engine3 Strike', 'Engine3 Damage', 'Engine4 Strike', 'Engine4 Damage',\n                     'Engine Ingested', 'Propeller Strike', 'Propeller Damage', 'Wing or Rotor Strike',\n                     'Wing or Rotor Damage', 'Fuselage Strike', 'Fuselage Damage', 'Landing Gear Strike',\n                     'Landing Gear Damage', 'Tail Strike', 'Tail Damage', 'Lights Strike',\n                     'Lights Damage', 'Other Strike', 'Other Damage']\n\n# Initialize LabelEncoder\nencoder = LabelEncoder()\n\n# Encode categorical variables\nfor column in columns_to_encode:\n    data[column] = encoder.fit_transform(data[column])\n\n# Define levels based on thresholds\ndata['Damage Level'] = pd.cut(data['Aircraft Damage'],\n                              bins=[-1, 0, 2, 4, 6, 8, 10],\n                              labels=[0, 1, 2, 3, 4, 5])\n\n# Define features and target variable\nfeatures = data.drop([\"Aircraft Damage\", \"Damage Level\"], axis=1)\ntarget = data[\"Damage Level\"].astype(int)\n\n# Sample a subset of the data to reduce memory usage\nsampled_data = data.sample(frac=0.1, random_state=42)\nfeatures_sampled = sampled_data.drop([\"Aircraft Damage\", \"Damage Level\"], axis=1)\ntarget_sampled = sampled_data[\"Damage Level\"].astype(int)\n\n# Handle missing values in the features\nimputer = SimpleImputer(strategy='median')\nfeatures_imputed = imputer.fit_transform(features_sampled)\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(features_imputed, target_sampled, test_size=0.1, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n# Build the ANN model\nmodel = keras.Sequential([\n    layers.Input(shape=(x_train.shape[1],)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(6, activation='softmax')  # Predicting hierarchical classes\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=20, batch_size=64, verbose=1)\n\n# Evaluate the model\ny_pred_probs = model.predict(x_test)\ny_pred = np.argmax(y_pred_probs, axis=1)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"ANN Accuracy: {accuracy}\")\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# Identify bifurcations by analyzing prediction probabilities\nbifurcations = []\nfor i, probs in enumerate(y_pred_probs):\n    sorted_probs = np.sort(probs)\n    # Detect if there's a significant drop between the two highest probabilities\n    if sorted_probs[-1] - sorted_probs[-2] < 0.2:  # Adjust threshold as needed\n        bifurcations.append((i, y_test.iloc[i], y_pred[i]))\n\nprint(f\"\\nIdentified Bifurcations (based on prediction uncertainty): {len(bifurcations)}\")\nfor idx, true_val, pred_val in bifurcations:\n    print(f\"Index: {idx}, True: {true_val}, Predicted: {pred_val}, Probabilities: {y_pred_probs[idx]}\")\n\n# Plot training history\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\n# Print hierarchical structure\nprint(\"\\nHierarchical Structure of Damage Levels:\")\nprint(\"Level 0: No Damage\")\nprint(\"Level 1: Minor Damage\")\nprint(\"Level 2: Moderate Damage\")\nprint(\"Level 3: Significant Damage\")\nprint(\"Level 4: Severe Damage\")\nprint(\"Level 5: Catastrophic Damage\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"BIOFURCATIONS","metadata":{}}]}