{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src='https://github.com/Deci-AI/super-gradients/blob/master/documentation/assets/SG_img/SG%20-%20Horizontal%20Glow.png?raw=true'>\n\n## If you want to learn more about the ResNet family of model architectures, be sure to check out [my FREE course on Udemy](https://www.udemy.com/course/supergradients-resnet/).\n\nIn this notebook, you'll use the SuperGradients training library to classify the color and type for apparel images.\n\nBefore you dive into the code, it's worth talking about something: The difference between <span style=\"color:red\">multi-class</span>, <span style=\"color:green\">multi-task</span>, and <span style=\"color:orange\">multi-label learning</span>,.\n\n<span style=\"color:red\">\n\nIn multi-class learning, you're predicting a single label for each input, but each label is a single element from a set of possible labels. \n\nFor example, imagine you're working with a dataset of clothing images, and you want to predict whether each item is a shirt, a pair of pants, or a pair of shoes. To add another dimension, you should also predict the colour of each item, such as whether it's red, blue, or green. With multi-class learning, you can predict both the clothing item and its colour at the same time. For example, \"red shirt\" vs \"blue shirt\" vs \"brown show\" vs \"black shoe,\" etc. etc. \n\nYou typically use the Cross-entropy loss function to train a neural network on this problem.\n</span>\n\n<span style=\"color:green\">\n\nIn multi-task learning, you have multiple problems that need to be solved simultaneously. \n\nFor instance, you could predict the clothing item and its colour as separate tasks. The idea here is that solving one task could help solve the other task - for example, certain colours might be more common for certain types of clothing. In this case, you can assign each output (clothing item and colour) loss function to train a neural network. \n\nYou can then combine the loss functions by summing them up (or averaging) and using weights to balance the importance of each task.\n</span>\n\n<span style=\"color:orange\">\n\nMulti-label learning is a special case of multi-task learning. \n\nIn this scenario, you should label an image with multiple clothing items and their colours. You can break down the task into multiple binary classification problems to solve this. If the possible labels are \"shirt ðŸ‘”\", \"pants ðŸ‘–\", and \"shoes ðŸ‘Ÿ\", and the possible colours are <span style=\"color:red\"> \"red\",</span> <span style=\"color:lightblue\"> \"blue\",</span> and <span style=\"color:green\">\"green\",</span> you would need to train the network to answer questions like \"is there a shirt in the image?\" and \"is the shirt in the image red?\" You want to use multi-label learning in the scenario where your labels are not mutually exclusive. \n\nYou would use the `BCEWithLogisLoss` for multi-label learning since it combines a sigmoid activation function and binary cross-entropy loss into a single function, making it efficient and numerically stable.\n</span>\n\n## What type of learning are we going to perfom here?\n\nThe answer will be revealed, but I want you to think about as you read through the notebook. We need to predict the *color* and *type* of an image of clothing, what would make sense here?\n\n#### What is SuperGradients?\n\n[SuperGradients](https://github.com/Deci-AI/super-gradients) is an open-source PyTorch based training library that has a number of pre-trained models for you to use, training recipies that will get you amazing accuracy, and many [training tricks](https://www.deeplearningdaily.community/t/tips-for-training-your-neural-networks/307) that you can use with just the \"flip of a switch\". For this example you'll use an EfficientNetB0 to perfom the classification. You can check out our [model zoo](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/Computer_Vision_Models_Pretrained_Checkpoints.md) and use any of the pretrained models we have available.\n\nFeel free to reach out to me on my community forum, [Deep Learning Daily (free and open to all)](https://www.deeplearningdaily.community/), should you have any questions.\n","metadata":{}},{"cell_type":"code","source":"%%capture \n#  NOTE: You MUST restart the notebook after installation is complete, else you will get an import error\n!pip install super-gradients==3.0.7 \n!pip install imutils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\n# import torchvision.models as models\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom torchvision import transforms\nimport imutils\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import ImageFolder\nfrom imutils import paths\nimport os\nimport cv2 as cv\nimport re\nimport requests\n\nimport pathlib\nfrom pathlib import Path\n\nimport torch.nn.functional as F\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torchmetrics import Metric\n\nimport super_gradients\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import Trainer\nfrom super_gradients.training import training_hyperparams\nfrom super_gradients.training.metrics.classification_metrics import Accuracy, Top5\nfrom super_gradients.training.utils.early_stopping import EarlyStop\nfrom super_gradients.training import models\nfrom super_gradients.training.utils.callbacks import Phase\nfrom super_gradients.common.registry import register_metric, register_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-02T20:25:56.469206Z","iopub.execute_input":"2023-03-02T20:25:56.469583Z","iopub.status.idle":"2023-03-02T20:26:06.218516Z","shell.execute_reply.started":"2023-03-02T20:25:56.469544Z","shell.execute_reply":"2023-03-02T20:26:06.217776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config class\n\nHere we'll have variable information saved.\n","metadata":{}},{"cell_type":"code","source":"class config:\n    # specify the paths to datasets\n    ROOT_DIR = Path(\"../input/apparel-images-dataset\")\n\n    # set the input height and width\n    INPUT_HEIGHT = 224\n    INPUT_WIDTH = 224\n\n    # set the input heig/ht and width\n    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n    IMAGENET_STD = [0.229, 0.224, 0.225]\n    \n    IMAGE_TYPE = '.jpg'\n    BATCH_SIZE = 128\n    MODEL_NAME = 'resnet50'\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    TRAINING_PARAMS = 'training_hyperparams/imagenet_resnet50_train_params'\n    \n    COLOUR_LABELS = ['black', 'blue', 'brown', 'green', 'red', 'white']\n    N_COLOUR_LABELS = len(COLOUR_LABELS)\n    ARTICLE_LABELS = ['dress', 'pants', 'shirt', 'shoes', 'shorts']\n    N_ARTICLE_LABELS = len(ARTICLE_LABELS)\n    NUM_CLASSES = N_COLOUR_LABELS + N_ARTICLE_LABELS\n    \n    CHECKPOINT_DIR = 'checkpoints'","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:06.221410Z","iopub.execute_input":"2023-03-02T20:26:06.223112Z","iopub.status.idle":"2023-03-02T20:26:06.229248Z","shell.execute_reply.started":"2023-03-02T20:26:06.223058Z","shell.execute_reply":"2023-03-02T20:26:06.228865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data into train, validation, and testing dataset","metadata":{}},{"cell_type":"code","source":"all_images = list(paths.list_images(config.ROOT_DIR))\ntrain_images, dummy_list = train_test_split(all_images, test_size=.20, shuffle=True, random_state=42)\nval_images, test_images = train_test_split(dummy_list, test_size=.50, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:11.601773Z","iopub.execute_input":"2023-03-02T20:26:11.602510Z","iopub.status.idle":"2023-03-02T20:26:12.093114Z","shell.execute_reply.started":"2023-03-02T20:26:11.602472Z","shell.execute_reply":"2023-03-02T20:26:12.092688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to encode the labels","metadata":{}},{"cell_type":"code","source":"def encode_label(label, class_list):\n    \"\"\"Encode a list of labels using one-hot encoding.\n\n    Args:\n        label: Label to encode.\n        class_list: A list of all possible labels. Defaults to DEFAULT_LABELS.\n\n    Returns:\n        A tensor representing the one-hot encoding of the input labels.\n    \"\"\"\n    # Create a tensor of zeros with the same length as the class list\n    target = torch.zeros(len(class_list))\n    for _ in class_list:\n        # Find the index of the current label in the class list\n        idx = class_list.index(label)\n        # Set the corresponding index in the target tensor to 1\n        target[idx] = 1\n    return target","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:17.946279Z","iopub.execute_input":"2023-03-02T20:26:17.946901Z","iopub.status.idle":"2023-03-02T20:26:17.950950Z","shell.execute_reply.started":"2023-03-02T20:26:17.946850Z","shell.execute_reply":"2023-03-02T20:26:17.950560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining a custom dataset\n\n\nThis line of code: `labels = re.findall(r'\\w+\\_\\w+', img_path)[0].split('_')`\n\nThis line extracts labels from an image path by searching for a pattern of one or more word characters followed by an underscore and one or more word characters, and then splitting the extracted match into a list of strings using underscores as the delimiter.\n\nYou'll unpack the result of this and define `colour_label` and `article_label`, which are one-hot encoded tensors that are then concatenated. \n\nThis will be your target tensor.\n\nTake a look at the naming convention of the image folders and it will make sense","metadata":{}},{"cell_type":"code","source":"!ls ../input/apparel-images-dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:23.323239Z","iopub.execute_input":"2023-03-02T20:26:23.323885Z","iopub.status.idle":"2023-03-02T20:26:24.292892Z","shell.execute_reply.started":"2023-03-02T20:26:23.323834Z","shell.execute_reply":"2023-03-02T20:26:24.292064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ApparelDataset(Dataset):\n    def __init__(self, image_list, transform=None):\n        self.transform=transform\n        self.image_list=image_list\n        \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_list[idx]\n        img = Image.open(img_path)\n        \n        if self.transform:\n            img = self.transform(img)\n        # The following line of code finds the pattern aaa_bbb in the filepath string\n        labels = re.findall(r'\\w+\\_\\w+', img_path)[0].split('_')\n        colour_label = encode_label(labels[0], config.COLOUR_LABELS)\n        article_label = encode_label(labels[1], config.ARTICLE_LABELS)\n        return img, np.concatenate([colour_label, article_label])","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:26.874210Z","iopub.execute_input":"2023-03-02T20:26:26.874740Z","iopub.status.idle":"2023-03-02T20:26:26.887632Z","shell.execute_reply.started":"2023-03-02T20:26:26.874684Z","shell.execute_reply":"2023-03-02T20:26:26.887229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data augmentations\n\nI've instantiated a number of augmentations that you're free to play around with.\n\nFor illustrative purposes I've only included RandomAugment in the pipeline. I encourage you to try some other ones.","metadata":{}},{"cell_type":"code","source":"# initialize our data augmentation functions\nresize = transforms.Resize(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH))\nmake_tensor = transforms.ToTensor()\nnormalize = transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\ncenter_cropper = transforms.CenterCrop((config.INPUT_HEIGHT,config.INPUT_WIDTH))\nrandom_horizontal_flip = transforms.RandomHorizontalFlip(p=0.75)\nrandom_vertical_flip = transforms.RandomVerticalFlip(p=0.75)\nrandom_rotation = transforms.RandomRotation(degrees=90)\nrandom_crop = transforms.RandomCrop(size=(200,200))\naugmix = transforms.AugMix(severity = 3, mixture_width=3, alpha=0.2)\nauto_augment = transforms.AutoAugment()\nrandom_augment = transforms.RandAugment()\n\n# initialize our training and validation set data augmentation pipeline\ntrain_transforms = transforms.Compose([\n  resize, \n  random_augment,\n  make_tensor,\n  normalize\n])\n\nval_transforms = transforms.Compose([resize, make_tensor, normalize])","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:34.103699Z","iopub.execute_input":"2023-03-02T20:26:34.104480Z","iopub.status.idle":"2023-03-02T20:26:34.114036Z","shell.execute_reply.started":"2023-03-02T20:26:34.104437Z","shell.execute_reply":"2023-03-02T20:26:34.113530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiate datasets","metadata":{}},{"cell_type":"code","source":"train_dataset = ApparelDataset(train_images, transform = train_transforms)\nval_dataset = ApparelDataset(val_images, transform = val_transforms)\ntest_dataset = ApparelDataset(test_images, transform = val_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:38.451250Z","iopub.execute_input":"2023-03-02T20:26:38.451919Z","iopub.status.idle":"2023-03-02T20:26:38.454761Z","shell.execute_reply.started":"2023-03-02T20:26:38.451876Z","shell.execute_reply":"2023-03-02T20:26:38.454452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiate dataloaders","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, config.BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, config.BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, config.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:40.511896Z","iopub.execute_input":"2023-03-02T20:26:40.512513Z","iopub.status.idle":"2023-03-02T20:26:40.516485Z","shell.execute_reply.started":"2023-03-02T20:26:40.512473Z","shell.execute_reply":"2023-03-02T20:26:40.516101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model definition\n\nThis defines a PyTorch module named MultilabelClassifier that inherits from nn.Module. The @register_model decorator is used to register the module in the SuperGradients training library.\n\nIn the constructor (`__init__()`), the ResNet model is loaded using `models.get()`, which is a utility function that retrieves a pre-trained model by name (`config.MODEL_NAME`) from the [SuperGradients model zoo](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/Computer_Vision_Models_Pretrained_Checkpoints.md). \n\nThe last two layers of the ResNet model are removed to obtain a feature extractor, which is stored in `self.model_wo_fc`. \n\n`self.avgpool` is an instance of `nn.AdaptiveAvgPool2d`, which computes the spatial average of each feature map. `self.colour` and `self.article` are instances of `nn.Sequential` that define the classification heads for colour and article labels, respectively. \n\nEach head consists of a dropout layer, a fully connected layer, and a softmax activation function.\n\nThe `forward()` method is where the actual computation takes place. \n\nThe input tensor x is passed through the ResNet feature extractor (`self.model_wo_fc`) and the average pooling layer (`self.avgpool`). The resulting feature tensor is then fed into the color and article classification heads, and the predicted probabilities are concatenated along the second dimension (`dim=1`) to form a single tensor pred_tensor, which is returned as the output of the module.\n\nNote that the `nn.Softmax() `function is applied to the output of each classification head, which normalizes the predicted probabilities across all possible labels for each head. This ensures that the predicted probabilities for each head sum to 1 and can be interpreted as probabilities. \n\nHowever, this is not strictly necessary.","metadata":{}},{"cell_type":"code","source":"@register_model(\"multilabelmodel\")\nclass MultilabelClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet = models.get(config.MODEL_NAME, pretrained_weights='imagenet')\n        self.model_wo_fc = nn.Sequential(*(list(self.resnet.children())[:-2]))\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.colour = nn.Sequential(nn.Dropout(0.5),\n                                     nn.Linear(2048, config.N_COLOUR_LABELS),\n                                     nn.Softmax(dim=1))\n        self.article = nn.Sequential(nn.Dropout(0.5),\n                                      nn.Linear(2048, config.N_ARTICLE_LABELS),\n                                      nn.Softmax(dim=1))\n    \n    def forward(self, x):\n        x = self.model_wo_fc(x)\n        x = self.avgpool(x)\n        predicted_colour = self.colour(x.view(x.size(0), -1))\n        predicted_article = self.article(x.view(x.size(0), -1))\n        pred_tensor = torch.cat([predicted_colour, predicted_article], dim=1)\n        return pred_tensor","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:48.616018Z","iopub.execute_input":"2023-03-02T20:26:48.616407Z","iopub.status.idle":"2023-03-02T20:26:48.625323Z","shell.execute_reply.started":"2023-03-02T20:26:48.616371Z","shell.execute_reply":"2023-03-02T20:26:48.624867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function definition\n\nHere you define a custom loss function called `CustomLoss`, which extends the PyTorch `nn.Module` class. This custom loss function calculates the average of the cross-entropy loss between the predicted outputs and the ground truth targets.\n\nIn the `__init__` method, the `super()` function is called to initialize the parent class `nn.Module`.\n\nThe cross-entropy loss is calculated using the PyTorch `nn.CrossEntropyLoss()` function in the forward method. The predicted outputs and target are passed as arguments to this function. The `preds` tensor contains the predicted outputs from the model and has shape (batch_size, num_classes), where `num_classes` is the total number of classes.\n\nNext, the colour loss and article loss are calculated separately using the cross-entropy loss function. The colour loss is calculated by taking the first `N_COLOUR_LABELS` columns of the preds tensor, while the article loss is calculated by taking the remaining columns.\n\nFinally, the average loss is calculated as the average of the colour and article loss. This is returned as the output of the forward method.","metadata":{}},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n\n    def forward(self, preds, target):\n        criterion = nn.CrossEntropyLoss()\n        colour_loss = criterion(preds[:, :config.N_COLOUR_LABELS], target[:,:config.N_COLOUR_LABELS])\n        article_loss = criterion(preds[:,config.N_COLOUR_LABELS:], target[:,config.N_COLOUR_LABELS:])\n        average_loss = (colour_loss + article_loss)/2\n        return average_loss","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:53.701267Z","iopub.execute_input":"2023-03-02T20:26:53.702136Z","iopub.status.idle":"2023-03-02T20:26:53.707007Z","shell.execute_reply.started":"2023-03-02T20:26:53.702098Z","shell.execute_reply":"2023-03-02T20:26:53.706227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy metric definition\n\nThis code defines a custom metric called `MyAccuracy` that inherits from the PyTorch Metric class. This metric measures the accuracy of a multi-task classification model that predicts an image's colour and article labels.\n\nIn the constructor of MyAccuracy, four state variables are created using the `add_state` method. These state variables keep track of the number of correct predictions and the total number of predictions made for each task (colour and article). The `dist_reduce_fx` argument specifies how to aggregate the state variables across different devices in a distributed setting. \n\nHere, the sum is used to add the values of the state variables across devices.\n\nThe update method of `MyAccuracy` is called after each batch of data is processed. It takes two arguments: `preds`, a tensor of shape (batch_size, num_labels) representing the model's predictions, and `target`, a tensor of the same shape representing the ground truth labels.\n\nThe `preds` tensor is split into colour and article predictions using slicing in the update method. The `argmax` function is used to get the indices of the predicted labels for each task. The same is done for the target tensor to get the true indices of the labels.\n\nThe number of correct predictions and the total number of predictions are then updated for each task using the `self.color_correct`, `self.color_total`, `self.article_correct`, and `self.article_total` state variables.\n\nThe compute method of MyAccuracy is called after all the batches have been processed. It calculates the total accuracy of the model as the average of the accuracies for each task. The accuracies are computed by dividing the number of correct predictions by the total number of predictions for each task and then taking the average of the two accuracies. \n\nThe total accuracy is then returned.","metadata":{}},{"cell_type":"code","source":"@register_metric(\"my_accuracy\")\nclass MyAccuracy(Metric):\n    def __init__(self):\n        super().__init__()\n        self.add_state(\"color_correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"color_total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"article_correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"article_total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds, target):\n        # Split the output into color and article predictions\n        color_output, article_output = preds[:, :config.N_COLOUR_LABELS], preds[:, config.N_COLOUR_LABELS:]\n\n        # Get the predicted indices for each task\n        color_pred = torch.argmax(color_output, dim=1)\n        article_pred = torch.argmax(article_output, dim=1)\n\n        # Get the true indices for each task\n        color_true, article_true = target[:, :config.N_COLOUR_LABELS], target[:, config.N_COLOUR_LABELS:]\n        color_true = torch.argmax(color_true, dim=1)\n        article_true = torch.argmax(article_true, dim=1)\n\n        # Calculate the accuracy for each task\n        self.color_correct += torch.sum(color_pred == color_true)\n        self.color_total += color_true.numel()\n        self.article_correct += torch.sum(article_pred == article_true)\n        self.article_total += article_true.numel()\n\n    def compute(self):\n        # Calculate the total accuracy as the average of the two task accuracies\n        color_acc = self.color_correct.float() / self.color_total\n        article_acc = self.article_correct.float() / self.article_total\n        total_acc = (color_acc + article_acc) / 2\n        return total_acc","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:26:57.417695Z","iopub.execute_input":"2023-03-02T20:26:57.418279Z","iopub.status.idle":"2023-03-02T20:26:57.437457Z","shell.execute_reply.started":"2023-03-02T20:26:57.418229Z","shell.execute_reply":"2023-03-02T20:26:57.436746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training hyperparameters\n\n","metadata":{}},{"cell_type":"code","source":"training_params =  training_hyperparams.get(config.TRAINING_PARAMS)\n\ntraining_params['loss'] = CustomLoss()\n\ntraining_params[\"train_metrics_list\"] = [\"my_accuracy\"]\ntraining_params[\"valid_metrics_list\"] = [\"my_accuracy\"]\ntraining_params[\"metric_to_watch\"] = \"my_accuracy\"\n\n# Set the silent mode to True to reduce clutter in the notebook, you can turn it on to see the full output\ntraining_params[\"silent_mode\"] = True\ntraining_params[\"optimizer\"] = 'AdamW'\ntraining_params[\"ema\"] = True\ntraining_params[\"criterion_params\"] = {'smooth_eps': 0.10}\ntraining_params[\"average_best_models\"] = True\ntraining_params[\"max_epochs\"] = 5\ntraining_params[\"initial_lr\"] = 0.001","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:27:13.339864Z","iopub.execute_input":"2023-03-02T20:27:13.340265Z","iopub.status.idle":"2023-03-02T20:27:13.506237Z","shell.execute_reply.started":"2023-03-02T20:27:13.340229Z","shell.execute_reply":"2023-03-02T20:27:13.505818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model!","metadata":{"execution":{"iopub.status.busy":"2023-03-01T18:53:10.784965Z","iopub.execute_input":"2023-03-01T18:53:10.785560Z","iopub.status.idle":"2023-03-01T18:53:10.790872Z","shell.execute_reply.started":"2023-03-01T18:53:10.785519Z","shell.execute_reply":"2023-03-01T18:53:10.790509Z"}}},{"cell_type":"code","source":"multi_label_model = MultilabelClassifier()\nmodel_trainer = Trainer(experiment_name='0_Baseline_Experiment', ckpt_root_dir=config.CHECKPOINT_DIR)\nmodel_trainer.train(model= multi_label_model, \n              training_params=training_params, \n              train_loader=train_loader,\n              valid_loader=val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:27:18.680296Z","iopub.execute_input":"2023-03-02T20:27:18.680960Z","iopub.status.idle":"2023-03-02T20:39:47.767839Z","shell.execute_reply.started":"2023-03-02T20:27:18.680919Z","shell.execute_reply":"2023-03-02T20:39:47.767367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to leverage weight averaging, which is  post-training method that takes the best model weights across the training and averages them into a single model. \n\nBy employing this technique you help your model overcome the optimization tendency to alternate between adjacent local minimas in the later stages of the training.\n\nIt turns out that more often than not, you can average these model weights and obtain a model that perfoms better than the individual models.\n\nYou can compare the averaged model with model trained during the last epoch and see which one performs better - if it turns out the last epoch does better, you can keep that one!\n\nWeight averaging doesnâ€™t cost you anything other than keeping a few additional weights on the disk, and can yield a substantial boost in performance and stability.\n\nIf you want to see the \"best\" model checkpoint as opposed to the averaged weights, swap `average_model.pth` with `ckpt_best.pth` below\n","metadata":{}},{"cell_type":"code","source":"best_full_model = models.get('multilabelmodel',\n                        num_classes=config.NUM_CLASSES,\n                        checkpoint_path=os.path.join(model_trainer.checkpoints_dir_path, \"average_model.pth\"))","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:41:13.329312Z","iopub.execute_input":"2023-03-02T20:41:13.330375Z","iopub.status.idle":"2023-03-02T20:41:14.154410Z","shell.execute_reply.started":"2023-03-02T20:41:13.330329Z","shell.execute_reply":"2023-03-02T20:41:14.154016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model on unseen data\n\n","metadata":{}},{"cell_type":"code","source":"model_trainer.test(model=best_full_model,\n            test_loader=test_loader,\n            test_metrics_list=['my_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:41:15.065213Z","iopub.execute_input":"2023-03-02T20:41:15.066044Z","iopub.status.idle":"2023-03-02T20:41:32.235584Z","shell.execute_reply.started":"2023-03-02T20:41:15.066005Z","shell.execute_reply":"2023-03-02T20:41:32.235124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple\nimport requests\nimport torchvision\nimport random\nimport textwrap\n\ndef pred_and_plot_image(image_path: str, \n                        subplot: Tuple[int, int, int],  # subplot tuple for `subplot()` function\n                        model: torch.nn.Module = best_full_model,\n                        image_size: Tuple[int, int] = (config.INPUT_HEIGHT, config.INPUT_WIDTH),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=config.DEVICE):\n\n    if isinstance(image_path, pathlib.PosixPath):\n        img = Image.open(image_path)\n    else: \n        img = Image.open(requests.get(image_path, stream=True).raw)\n\n    # create transformation for image (if one doesn't exist)\n    if transform is None:\n        transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=config.IMAGENET_MEAN,\n                                 std=config.IMAGENET_STD),\n        ])\n    transformed_image = transform(img)\n\n    # make sure the model is on the target device\n    model.to(device)\n\n    # turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n        transformed_image = transformed_image.unsqueeze(dim=0)\n\n        # make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(transformed_image.to(device))\n\n    # convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n    color = torch.argmax(target_image_pred[:, :config.N_COLOUR_LABELS], dim=1).item()\n    article = torch.argmax(target_image_pred[:, config.N_COLOUR_LABELS:], dim=1).item()\n    \n    predicted_color = config.COLOUR_LABELS[color]\n    predicted_article = config.ARTICLE_LABELS[article]\n    \n    target_image_pred_label = predicted_color + '_' +  predicted_article\n\n    # plot image with predicted label and probability \n    plt.subplot(*subplot)\n    plt.imshow(img)\n    if isinstance(image_path, pathlib.PosixPath):\n        # actual label\n        ground_truth = re.findall(r'\\w+\\_\\w+', str(image_path))[0]\n        title = f\"Ground Truth: {ground_truth} | Pred: {target_image_pred_label}\"\n    else:\n        title = f\"Pred: {target_image_pred_label}\"\n    plt.title(\"\\n\".join(textwrap.wrap(title, width=20)))  # wrap text using textwrap.wrap() function\n    plt.axis(False)\n    \n\ndef plot_random_test_images(model, test_images):\n    num_images_to_plot = 30\n    test_image_path_list = [pathlib.PosixPath(p) for p in test_images] \n    test_image_path_sample = random.sample(population=test_image_path_list,  # randomly select 'k' image paths to pred and plot\n                                           k=num_images_to_plot)\n\n    # set up subplots\n    num_rows = int(np.ceil(num_images_to_plot / 5))\n    fig, ax = plt.subplots(num_rows, 5, figsize=(15, num_rows * 3))\n    ax = ax.flatten()\n\n    # Make predictions on and plot the images\n    for i, image_path in enumerate(test_image_path_sample):\n        pred_and_plot_image(model=model, \n                            image_path=image_path,\n                            subplot=(num_rows, 5, i+1),  # subplot tuple for `subplot()` function\n                            image_size=(config.INPUT_HEIGHT, config.INPUT_WIDTH))\n\n    # adjust spacing between subplots\n    plt.subplots_adjust(wspace=1)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:55:33.089964Z","iopub.execute_input":"2023-03-02T20:55:33.090407Z","iopub.status.idle":"2023-03-02T20:55:33.104983Z","shell.execute_reply.started":"2023-03-02T20:55:33.090371Z","shell.execute_reply":"2023-03-02T20:55:33.104463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_random_test_images(best_full_model, test_images)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:52:10.381491Z","iopub.execute_input":"2023-03-02T20:52:10.382708Z","iopub.status.idle":"2023-03-02T20:52:12.936856Z","shell.execute_reply.started":"2023-03-02T20:52:10.382663Z","shell.execute_reply":"2023-03-02T20:52:12.936098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_random_test_images(best_full_model, test_images)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_and_plot_image(image_path='https://sneakernews.com/wp-content/uploads/2020/11/Air-Jordan-13-GS-DC9443-007-01.jpg', subplot=(1, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:55:36.585096Z","iopub.execute_input":"2023-03-02T20:55:36.586071Z","iopub.status.idle":"2023-03-02T20:55:37.212652Z","shell.execute_reply.started":"2023-03-02T20:55:36.586014Z","shell.execute_reply":"2023-03-02T20:55:37.212183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_and_plot_image(image_path='https://images.solecollector.com/complex/image/upload/c_fill,dpr_auto,f_auto,fl_lossy,g_face,q_auto,w_1280/cjapz33ntap9njssipiw_beyuom.jpg', subplot=(1, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:56:23.003955Z","iopub.execute_input":"2023-03-02T20:56:23.004434Z","iopub.status.idle":"2023-03-02T20:56:23.757868Z","shell.execute_reply.started":"2023-03-02T20:56:23.004391Z","shell.execute_reply":"2023-03-02T20:56:23.757381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_and_plot_image(image_path='https://assets.adidas.com/images/w_600,f_auto,q_auto/0e8981a897f446468189af0000b7e69e_9366/Adicolor_SST_Track_Suit_Blue_IB8636_01_laydown.jpg', subplot=(1, 1, 1))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-02T20:57:25.845196Z","iopub.execute_input":"2023-03-02T20:57:25.845858Z","iopub.status.idle":"2023-03-02T20:57:26.439951Z","shell.execute_reply.started":"2023-03-02T20:57:25.845813Z","shell.execute_reply":"2023-03-02T20:57:26.439448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Your homework\n\nCopy/fork this notebook and try some different architectures.\n\nIf you have a question you can leave a comment on this notebook, or visit the community and post it in the [Q&A section](https://www.deeplearningdaily.community/c/qanda/8).\n\n## Use a different pretrained model\n\nYou can change the model you use. Take a look at the [SG model zoo](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/Computer_Vision_Models_Pretrained_Checkpoints.md)\n\nFor example, if you wanted to use RegNet you would do the following:\n\n```\nresnet_imagenet_model = models.get(model_name='regnetY800', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\nresnet_params =  training_hyperparams.get('training_hyperparams/imagenet_regnetY_train_params')\n```\n\nNote you can also pass 'model_name=regnetY200', 'model_name=regnetY400', 'model_name=regnetY600' to try a variety of the architecture\n\nFor ResNet50, you would do:\n\n```\nresnet_imagenet_model = models.get(model_name='resnet50', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\nresnet_params =  training_hyperparams.get('training_hyperparams/imagenet_resnet50_train_params')\n```\n\nNote you can also pass 'model_name=resnet18' or 'model_name=resnet34' to try a variety of the architecture\n\nFor MobileNetV2, you would do:\n\n```\nmobilenet_imagenet_model = models.get(model_name='mobilenet_v2', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\nresnet_params =  training_hyperparams.get('training_hyperparams/imagenet_mobilenetv2_train_params')\n```\n\nFor MobileNetV3, you would do:\n\n```\nmobilenet_imagenet_model = models.get(model_name='mobilenet_v3_large', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\nresnet_params =  training_hyperparams.get('training_hyperparams/imagenet_mobilenetv3_train_params')\n```\n\nNote you can also pass 'model_name=mobilenet_v3_small' to try a variety of the architecture\n\n\nFor ViT, you would do:\n\n\n```\nvit_imagenet_model = models.get(model_name='vit_base', num_classes=NUM_CLASSES, pretrained_weights='imagenet')\nvit_params =  training_hyperparams.get(\"training_hyperparams/imagenet_vit_train_params\")\n```\n\nNote you can also pass 'model_name=vit_large' to try a variety of the architecture\n\n\nI encourage you play around with different optimizers, all you have to do is change the value of `training_params[\"optimizer\"]`. You can use one of ['Adam','SGD','RMSProp'] out of the box. You can play around with the optimizer params as well.\n\nIn general, play and tweak around the training recipies...\n\n## Training recipes\n\nSuperGradients has a number of [training recipes](https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/recipes) you can use. [See here](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/recipes/training_hyperparams/default_train_params.yaml) for more information about the training params.\n\nIf you're using Weights and Biases to track your experiments, you would do the following\n\n```\nsg_logger: wandb_sg_logger\nsg_logger_params:\nproject_name: <YOUR PROJECT NAME>\nentity: algo\napi_server: https://wandb.research.deci.ai\nsave_checkpoints_remote: True\nsave_tensorboard_remote: True\nsave_logs_remote: True\n```","metadata":{}}]}