{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31eb832c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-27T09:51:43.803935Z",
     "iopub.status.busy": "2025-03-27T09:51:43.803556Z",
     "iopub.status.idle": "2025-03-27T09:51:55.377234Z",
     "shell.execute_reply": "2025-03-27T09:51:55.376085Z"
    },
    "papermill": {
     "duration": 11.580357,
     "end_time": "2025-03-27T09:51:55.378911",
     "exception": true,
     "start_time": "2025-03-27T09:51:43.798554",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vit_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2b32cffdea2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobile_vit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMobileViT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvit_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossFormer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vit_pytorch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from vit_pytorch import ViT\n",
    "from vit_pytorch.mobile_vit import MobileViT\n",
    "from vit_pytorch.crossformer import CrossFormer\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99edfee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:01:04.995237Z",
     "iopub.status.busy": "2025-03-27T09:01:04.994939Z",
     "iopub.status.idle": "2025-03-27T09:01:10.808685Z",
     "shell.execute_reply": "2025-03-27T09:01:10.807895Z",
     "shell.execute_reply.started": "2025-03-27T09:01:04.995211Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install vit-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e17199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:01:52.981031Z",
     "iopub.status.busy": "2025-03-27T09:01:52.980358Z",
     "iopub.status.idle": "2025-03-27T09:01:52.985566Z",
     "shell.execute_reply": "2025-03-27T09:01:52.984725Z",
     "shell.execute_reply.started": "2025-03-27T09:01:52.980998Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_size = 32  # CIFAR-10 image size\n",
    "patch_size = 4   # Patch size for ViT\n",
    "num_classes = 10\n",
    "dim = 128        # Embedding dimension\n",
    "depth = 6        # Number of transformer layers\n",
    "heads = 8        # Number of attention heads\n",
    "mlp_dim = 256    # Dimension of MLP layer\n",
    "dropout = 0.1\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba830f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:05:39.190981Z",
     "iopub.status.busy": "2025-03-27T09:05:39.190516Z",
     "iopub.status.idle": "2025-03-27T09:05:53.939889Z",
     "shell.execute_reply": "2025-03-27T09:05:53.938964Z",
     "shell.execute_reply.started": "2025-03-27T09:05:39.190930Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define dataset paths (assuming folders are inside `/kaggle/input/`)\n",
    "train_dir = \"/kaggle/input/training-flower\"\n",
    "test_dir = \"/kaggle/input/testing-flower\"\n",
    "\n",
    "# Check if directories exist\n",
    "assert os.path.exists(train_dir), f\"Training folder not found: {train_dir}\"\n",
    "assert os.path.exists(test_dir), f\"Testing folder not found: {test_dir}\"\n",
    "\n",
    "# Define transformations (resize, convert to tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1,1]\n",
    "])\n",
    "\n",
    "# Load datasets (expects subfolders for each class inside \"train\" and \"test\")\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print dataset details\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "print(\"Classes:\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0ba48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:06:25.243087Z",
     "iopub.status.busy": "2025-03-27T09:06:25.242789Z",
     "iopub.status.idle": "2025-03-27T09:06:25.248469Z",
     "shell.execute_reply": "2025-03-27T09:06:25.247661Z",
     "shell.execute_reply.started": "2025-03-27T09:06:25.243066Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only the first 5000 examples for training and first 1000 for testing\n",
    "train_dataset.samples = train_dataset.samples[:5000]\n",
    "train_dataset.targets = train_dataset.targets[:5000]\n",
    "\n",
    "test_dataset.samples = test_dataset.samples[:1000]\n",
    "test_dataset.targets = test_dataset.targets[:1000]\n",
    "\n",
    "# Update the length of datasets\n",
    "train_dataset.imgs = train_dataset.samples\n",
    "test_dataset.imgs = test_dataset.samples\n",
    "\n",
    "# Print the number of samples\n",
    "print(f\"Number of training examples: {len(train_dataset)} | Number of testing examples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e240b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:07:25.727989Z",
     "iopub.status.busy": "2025-03-27T09:07:25.727681Z",
     "iopub.status.idle": "2025-03-27T09:07:26.216455Z",
     "shell.execute_reply": "2025-03-27T09:07:26.215683Z",
     "shell.execute_reply.started": "2025-03-27T09:07:25.727967Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT  # Install with: pip install vit-pytorch\n",
    "\n",
    "# Define device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Vision Transformer (ViT) parameters\n",
    "image_size = 128          # Input image size\n",
    "patch_size = 16           # Patch size\n",
    "num_classes = 10          # Number of output classes\n",
    "dim = 512                 # Model dimension\n",
    "depth = 6                 # Number of transformer layers\n",
    "heads = 8                 # Number of attention heads\n",
    "mlp_dim = 1024            # Feed-forward hidden dimension\n",
    "dropout = 0.1             # Dropout rate\n",
    "\n",
    "# Initialize the ViT model\n",
    "model = ViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=dim,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    dropout=dropout,\n",
    "    emb_dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dd3c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:07:40.884290Z",
     "iopub.status.busy": "2025-03-27T09:07:40.883989Z",
     "iopub.status.idle": "2025-03-27T09:07:40.889227Z",
     "shell.execute_reply": "2025-03-27T09:07:40.888247Z",
     "shell.execute_reply.started": "2025-03-27T09:07:40.884267Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36784b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:09:08.388175Z",
     "iopub.status.busy": "2025-03-27T09:09:08.387878Z",
     "iopub.status.idle": "2025-03-27T09:12:13.548802Z",
     "shell.execute_reply": "2025-03-27T09:12:13.547779Z",
     "shell.execute_reply.started": "2025-03-27T09:09:08.388154Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard\n",
    "\n",
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(\"runs/ViT_training\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "train_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Log training loss to TensorBoard\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch + 1)\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    test_accuracy_history.append(accuracy)\n",
    "\n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar(\"Accuracy/Test\", accuracy, epoch + 1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ad9c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:12:18.228500Z",
     "iopub.status.busy": "2025-03-27T09:12:18.228164Z",
     "iopub.status.idle": "2025-03-27T09:12:24.113064Z",
     "shell.execute_reply": "2025-03-27T09:12:24.112248Z",
     "shell.execute_reply.started": "2025-03-27T09:12:18.228471Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating on Test Data...\")\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "# Plot confusion matrix as an image for TensorBoard\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "\n",
    "# Save confusion matrix plot to TensorBoard\n",
    "plt.tight_layout()\n",
    "writer.add_figure(\"Confusion erfgerfgweryMatrix\", plt.gcf())\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8795d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:12:54.952075Z",
     "iopub.status.busy": "2025-03-27T09:12:54.951778Z",
     "iopub.status.idle": "2025-03-27T09:12:56.686722Z",
     "shell.execute_reply": "2025-03-27T09:12:56.685842Z",
     "shell.execute_reply.started": "2025-03-27T09:12:54.952053Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize test images with predictions and ground truth\n",
    "import random\n",
    "\n",
    "def visualize_test_images(model, test_loader, class_names, num_images=16):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    images, labels = next(iter(test_loader))  # Get a batch of test images\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Select random indices\n",
    "    indices = random.sample(range(len(images)), num_images)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, idx in enumerate(indices):\n",
    "        image = images[idx].cpu().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
    "        image = (image * 0.5) + 0.5  # Unnormalize\n",
    "        label = labels[idx].item()\n",
    "        prediction = predicted[idx].item()\n",
    "\n",
    "        plt.subplot(4, 4, i + 1)  # Arrange in a 4x4 grid\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"True: {class_names[label]}\\nPred: {class_names[prediction]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally log the visualization to TensorBoard\n",
    "    writer.add_figure(\"Test Images\", plt.gcf())\n",
    "\n",
    "# Call the function to visualize\n",
    "visualize_test_images(model, test_loader, class_names, num_images=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a02457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:13:41.212146Z",
     "iopub.status.busy": "2025-03-27T09:13:41.211834Z",
     "iopub.status.idle": "2025-03-27T09:13:41.429340Z",
     "shell.execute_reply": "2025-03-27T09:13:41.428463Z",
     "shell.execute_reply.started": "2025-03-27T09:13:41.212126Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_attention(model, data_loader, num_layers=6, patch_size=4):\n",
    "    \"\"\"\n",
    "    Visualizes attention maps for a random image after each transformer layer.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Vision Transformer model.\n",
    "        data_loader: DataLoader for test data.\n",
    "        num_layers: Number of transformer layers in the model.\n",
    "        patch_size: Patch size used in ViT.\n",
    "    \"\"\"\n",
    "    # Hook to store attention maps\n",
    "    attention_maps = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        # The attention weights are typically in `module.attention`\n",
    "        attention_maps.append(module.attention_weights)\n",
    "\n",
    "    # Register hooks for each transformer block\n",
    "    hooks = []\n",
    "    for i in range(num_layers):\n",
    "        hooks.append(\n",
    "            model.transformer.layers[i].register_forward_hook(hook)\n",
    "        )\n",
    "\n",
    "    # Select a random image from the test loader\n",
    "    images, labels = next(iter(data_loader))\n",
    "    random_idx = random.randint(0, images.size(0) - 1)\n",
    "    image = images[random_idx:random_idx + 1].to(device)\n",
    "    label = labels[random_idx].item()\n",
    "\n",
    "    # Pass the image through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(image)\n",
    "\n",
    "    # Plot the original image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, num_layers + 1, 1)\n",
    "    plt.imshow((image[0].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "\n",
    "    # Plot attention maps\n",
    "    for i, attention in enumerate(attention_maps):\n",
    "        attention = attention[0].mean(dim=0).cpu().numpy()  # Take mean attention over heads\n",
    "        num_patches = int(attention.shape[0] ** 0.5)\n",
    "\n",
    "        # Reshape attention to match patches\n",
    "        attention = attention.reshape(num_patches, num_patches)\n",
    "        attention = torch.nn.functional.interpolate(\n",
    "            torch.tensor(attention).unsqueeze(0).unsqueeze(0),\n",
    "            size=(image.size(2), image.size(3)),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze().cpu().numpy()\n",
    "\n",
    "        plt.subplot(1, num_layers + 1, i + 2)\n",
    "        plt.imshow(attention, cmap=\"viridis\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Layer {i + 1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_attention(model, test_loader, num_layers=depth, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457f643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:13:57.829449Z",
     "iopub.status.busy": "2025-03-27T09:13:57.829125Z",
     "iopub.status.idle": "2025-03-27T09:13:57.834335Z",
     "shell.execute_reply": "2025-03-27T09:13:57.833498Z",
     "shell.execute_reply.started": "2025-03-27T09:13:57.829421Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_attention_maps(model, images):\n",
    "    \"\"\"\n",
    "    Captures attention maps from the Vision Transformer model.\n",
    "    Args:\n",
    "        model: Vision Transformer model.\n",
    "        images: Input batch of images.\n",
    "    Returns:\n",
    "        List of attention maps for each transformer layer.\n",
    "    \"\"\"\n",
    "    attention_maps = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        attention_maps.append(output)\n",
    "\n",
    "    # Register hooks for each transformer block\n",
    "    hooks = []\n",
    "    for transformer_block in model.transformer.blocks:\n",
    "        hooks.append(transformer_block.attn.attn_drop.register_forward_hook(hook_fn))\n",
    "\n",
    "    # Perform a forward pass to capture attention maps\n",
    "    with torch.no_grad():\n",
    "        _ = model(images)\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd20d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:14:18.126842Z",
     "iopub.status.busy": "2025-03-27T09:14:18.126499Z",
     "iopub.status.idle": "2025-03-27T09:14:18.134378Z",
     "shell.execute_reply": "2025-03-27T09:14:18.133333Z",
     "shell.execute_reply.started": "2025-03-27T09:14:18.126813Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_attention(images, attention_maps, patch_size, image_size, num_heads=8):\n",
    "    \"\"\"\n",
    "    Visualizes the attention maps for a batch of images.\n",
    "    Args:\n",
    "        images: Batch of input images (torch.Tensor).\n",
    "        attention_maps: List of attention maps from ViT.\n",
    "        patch_size: Patch size used in the ViT model.\n",
    "        image_size: Image size of the input images.\n",
    "        num_heads: Number of attention heads.\n",
    "    \"\"\"\n",
    "    batch_size = images.size(0)\n",
    "    num_layers = len(attention_maps)\n",
    "\n",
    "    # Rescale images to [0, 1] for visualization\n",
    "    images = images.permute(0, 2, 3, 1).cpu().numpy() * 0.5 + 0.5\n",
    "\n",
    "    for idx in range(batch_size):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.suptitle(f\"Image {idx + 1}: Attention Maps\", fontsize=16)\n",
    "\n",
    "        # Plot the original image\n",
    "        plt.subplot(2, num_layers + 1, 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        # Loop through each layer\n",
    "        for layer_idx, layer_attentions in enumerate(attention_maps):\n",
    "            # Extract the attention map for this image and layer\n",
    "            attention = layer_attentions[idx]  # Shape: (num_heads, num_patches, num_patches)\n",
    "            attention = attention.mean(dim=0).reshape(patch_size, patch_size)  # Average over heads\n",
    "\n",
    "            # Resize attention map to match the image size\n",
    "            attention_resized = cv2.resize(attention.cpu().numpy(), (image_size, image_size))\n",
    "            attention_resized = (attention_resized - attention_resized.min()) / (attention_resized.max() - attention_resized.min())\n",
    "\n",
    "            # Overlay attention on the original image\n",
    "            overlay = images[idx].copy()\n",
    "            heatmap = cv2.applyColorMap((attention_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "            heatmap = heatmap[..., ::-1] / 255.0\n",
    "            overlay = cv2.addWeighted(overlay, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "            plt.subplot(2, num_layers + 1, layer_idx + 2)\n",
    "            plt.imshow(overlay)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Layer {layer_idx + 1}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8fc0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:16:09.053911Z",
     "iopub.status.busy": "2025-03-27T09:16:09.053539Z",
     "iopub.status.idle": "2025-03-27T09:16:09.136469Z",
     "shell.execute_reply": "2025-03-27T09:16:09.135754Z",
     "shell.execute_reply.started": "2025-03-27T09:16:09.053873Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vit_model.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa946b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:16:28.875815Z",
     "iopub.status.busy": "2025-03-27T09:16:28.875455Z",
     "iopub.status.idle": "2025-03-27T09:16:28.926742Z",
     "shell.execute_reply": "2025-03-27T09:16:28.925960Z",
     "shell.execute_reply.started": "2025-03-27T09:16:28.875785Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"vit_model.pth\"))\n",
    "model.to(device)  # Move to GPU if available\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdace05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T09:17:59.535063Z",
     "iopub.status.busy": "2025-03-27T09:17:59.534764Z",
     "iopub.status.idle": "2025-03-27T09:17:59.594496Z",
     "shell.execute_reply": "2025-03-27T09:17:59.593558Z",
     "shell.execute_reply.started": "2025-03-27T09:17:59.535040Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define image transformations (same as used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to match model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load(\"vit_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to predict an image\n",
    "def predict_image(image_path, model, class_names):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Apply transformations & add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Forward pass\n",
    "        _, predicted = torch.max(output, 1)  # Get class index\n",
    "    \n",
    "    return class_names[predicted.item()]  # Return class label\n",
    "\n",
    "# Define class names (Update this according to your dataset)\n",
    "class_names = [\"daisy\", \"dandelion\", \"rose\"]  # Example class names\n",
    "\n",
    "# Test the model on an image\n",
    "image_path = \"/kaggle/input/testing-flower/dandelion/1128626197_3f52424215_n.jpg\"  # Replace with your image file path\n",
    "predicted_class = predict_image(image_path, model, class_names)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6967254,
     "sourceId": 11164966,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6967261,
     "sourceId": 11164974,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.278863,
   "end_time": "2025-03-27T09:51:57.305470",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-27T09:51:40.026607",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
